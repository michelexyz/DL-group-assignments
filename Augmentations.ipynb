{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 and 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the mnist data with the data loader and put in the following tensors: (x_train_data, y_train_data), (x_test_data, y_test_data)\n",
    "\n",
    "def load_mnist_data():\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=len(train_data))\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "\n",
    "    x_train_data, y_train_data = next(iter(train_loader))\n",
    "    x_test_data, y_test_data = next(iter(test_loader))\n",
    "\n",
    "    return (x_train_data, y_train_data), (x_test_data, y_test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def load_mnist_data_with_augmentations():\n",
    "    # Define augmentations for the training set\n",
    "    train_transform = transforms.Compose([\n",
    "        # transforms.RandomRotation(20),  # Randomly rotate images by Â±10 degrees \n",
    "        # transforms.ToTensor(),  # Convert images to tensors\n",
    "        # transforms.Normalize((0.5,), (0.5,))  # Normalize images to [-1, 1]\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Rotation, translation, scaling\n",
    "        transforms.RandomPerspective(distortion_scale=0.1, p=0.5),  # Perspective distortion\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),  # Add Gaussian blur\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "\n",
    "    ])\n",
    "    \n",
    "    # Define transformations for the test set (no augmentations, just normalization)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Load datasets with the respective transformations\n",
    "    train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.MNIST(root='data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    # Split training data into training and validation sets\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Example Usage\n",
    "train_loader, val_loader, test_loader = load_mnist_data_with_augmentations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = load_mnist_data_with_augmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data into 50 000 training instances and 10 000 validation instances\n",
    "\n",
    "def split_data(x_train_data, y_train_data):\n",
    "\n",
    "    x_train_data, x_val_data = x_train_data[:50000], x_train_data[50000:]\n",
    "    y_train_data, y_val_data = y_train_data[:50000], y_train_data[50000:]\n",
    "\n",
    "    return (x_train_data, y_train_data), (x_val_data, y_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_val, y_val) = split_data(x_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementig the network and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss_and_accuracy(model, x_data, y_data, criterion, batch):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(x_data), batch):\n",
    "            to = min(i + batch, len(x_data))\n",
    "            x_batch = x_data[i:to]\n",
    "            y_batch = y_data[i:to]\n",
    "            output = model(x_batch)\n",
    "            loss += criterion(output, y_batch).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y_batch.view_as(pred)).sum().item()\n",
    "    return loss / len(x_data), correct / len(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model and training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the neural network\n",
    "class MNISTConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)  # 1 input channel, 16 output channels\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # 16 input channels, 32 output channels\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 32 input channels, 64 output channels\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 10)  # Flattened to a fully connected layer with 10 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # First convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = F.relu(self.conv2(x))  # Second convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = F.relu(self.conv3(x))  # Third convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)            # Fully connected layer\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Training loop that computes the running loss per epoch and validation loss and accuracy per epoch\n",
    "def train(model, train_loader,val_loader, optimizer, criterion, epochs=10, batch_size=64):\n",
    "\n",
    "    first_epoch_running_loss = []\n",
    "\n",
    "    train_evaluations = np.zeros((epochs, 2))\n",
    "    val_evaluations = np.zeros((epochs, 2))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        x_train, y_train = next(iter(train_loader))\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        x_val, y_val = next(iter(val_loader))\n",
    "        x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "        model.train()\n",
    "        #desc=f'Batches for epoch {epoch + 1}/{epochs}'\n",
    "        for i in tqdm(range(0, len(x_train), batch_size),disable=True):\n",
    "\n",
    "            to = min(i + batch_size, len(x_train))\n",
    "            x_batch = x_train[i:to]\n",
    "            y_batch = y_train[i:to]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch == 0:\n",
    "                # get average loss and append to list\n",
    "                first_epoch_running_loss.append(loss.item())\n",
    "\n",
    "        \n",
    "        train_loss, train_acc = calculate_loss_and_accuracy(model, x_train, y_train, criterion, batch_size)\n",
    "        val_loss, val_acc = calculate_loss_and_accuracy(model, x_val, y_val, criterion, batch_size)\n",
    "\n",
    "        train_evaluations[epoch] = [train_loss, train_acc]\n",
    "        val_evaluations[epoch] = [val_loss, val_acc]\n",
    "        if (epoch+1)%100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return first_epoch_running_loss, train_evaluations, val_evaluations\n",
    "\n",
    "\n",
    "# Training on CPU (use \"cuda\" for GPU training if available)\n",
    "\n",
    "\n",
    "# put the data on the device\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/3000, Train Loss: 0.0088, Train Acc: 0.7812, Val Loss: 0.0135, Val Acc: 0.7188\n",
      "Epoch 200/3000, Train Loss: 0.0063, Train Acc: 0.8594, Val Loss: 0.0036, Val Acc: 0.9375\n",
      "Epoch 300/3000, Train Loss: 0.0031, Train Acc: 0.9375, Val Loss: 0.0034, Val Acc: 0.9062\n",
      "Epoch 400/3000, Train Loss: 0.0027, Train Acc: 0.9531, Val Loss: 0.0013, Val Acc: 1.0000\n",
      "Epoch 500/3000, Train Loss: 0.0035, Train Acc: 0.9531, Val Loss: 0.0018, Val Acc: 0.9844\n",
      "Epoch 600/3000, Train Loss: 0.0023, Train Acc: 0.9219, Val Loss: 0.0032, Val Acc: 0.9062\n",
      "Epoch 700/3000, Train Loss: 0.0028, Train Acc: 0.9375, Val Loss: 0.0025, Val Acc: 0.9531\n",
      "Epoch 800/3000, Train Loss: 0.0021, Train Acc: 0.9531, Val Loss: 0.0029, Val Acc: 0.9531\n",
      "Epoch 900/3000, Train Loss: 0.0032, Train Acc: 0.9062, Val Loss: 0.0008, Val Acc: 0.9844\n",
      "Epoch 1000/3000, Train Loss: 0.0017, Train Acc: 0.9531, Val Loss: 0.0004, Val Acc: 1.0000\n",
      "Epoch 1100/3000, Train Loss: 0.0026, Train Acc: 0.9531, Val Loss: 0.0010, Val Acc: 0.9844\n",
      "Epoch 1200/3000, Train Loss: 0.0013, Train Acc: 0.9844, Val Loss: 0.0008, Val Acc: 1.0000\n",
      "Epoch 1300/3000, Train Loss: 0.0011, Train Acc: 0.9844, Val Loss: 0.0019, Val Acc: 0.9531\n",
      "Epoch 1400/3000, Train Loss: 0.0008, Train Acc: 0.9844, Val Loss: 0.0017, Val Acc: 0.9844\n",
      "Epoch 1500/3000, Train Loss: 0.0022, Train Acc: 0.9531, Val Loss: 0.0011, Val Acc: 0.9688\n",
      "Epoch 1600/3000, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0012, Val Acc: 0.9844\n",
      "Epoch 1700/3000, Train Loss: 0.0023, Train Acc: 0.9688, Val Loss: 0.0004, Val Acc: 1.0000\n",
      "Epoch 1800/3000, Train Loss: 0.0021, Train Acc: 0.9531, Val Loss: 0.0005, Val Acc: 0.9844\n",
      "Epoch 1900/3000, Train Loss: 0.0011, Train Acc: 0.9844, Val Loss: 0.0005, Val Acc: 1.0000\n",
      "Epoch 2000/3000, Train Loss: 0.0007, Train Acc: 0.9844, Val Loss: 0.0015, Val Acc: 0.9688\n",
      "Epoch 2100/3000, Train Loss: 0.0022, Train Acc: 0.9688, Val Loss: 0.0016, Val Acc: 0.9688\n",
      "Epoch 2200/3000, Train Loss: 0.0007, Train Acc: 0.9844, Val Loss: 0.0010, Val Acc: 0.9844\n",
      "Epoch 2300/3000, Train Loss: 0.0010, Train Acc: 0.9688, Val Loss: 0.0009, Val Acc: 0.9844\n",
      "Epoch 2400/3000, Train Loss: 0.0011, Train Acc: 0.9688, Val Loss: 0.0009, Val Acc: 0.9844\n",
      "Epoch 2500/3000, Train Loss: 0.0007, Train Acc: 0.9688, Val Loss: 0.0011, Val Acc: 0.9844\n",
      "Epoch 2600/3000, Train Loss: 0.0008, Train Acc: 0.9688, Val Loss: 0.0006, Val Acc: 0.9844\n",
      "Epoch 2700/3000, Train Loss: 0.0011, Train Acc: 0.9688, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 2800/3000, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9688\n",
      "Epoch 2900/3000, Train Loss: 0.0006, Train Acc: 0.9844, Val Loss: 0.0006, Val Acc: 0.9844\n",
      "Epoch 3000/3000, Train Loss: 0.0010, Train Acc: 0.9844, Val Loss: 0.0020, Val Acc: 0.9844\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model = MNISTConvNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "first_epoch_running_loss, train_evaluations, val_evaluations = train(model, train_loader, val_loader, optimizer, criterion, epochs=3000, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Test the model on the whole test set.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch model.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        device: The device to run the evaluation on (default is 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        accuracy: The accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.85%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy = test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO here we plot the results stored in *first_epoch_running_loss, train_evaluations, val_evaluations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuckyou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
