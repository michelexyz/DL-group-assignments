{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 and 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the mnist data with the data loader and put in the following tensors: (x_train_data, y_train_data), (x_test_data, y_test_data)\n",
    "\n",
    "def load_mnist_data():\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=len(train_data))\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "\n",
    "    x_train_data, y_train_data = next(iter(train_loader))\n",
    "    x_test_data, y_test_data = next(iter(test_loader))\n",
    "\n",
    "    return x_train_data, y_train_data, x_test_data, y_test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def load_mnist_data_with_augmentations():\n",
    "    # Define augmentations for the training set\n",
    "    train_transform = transforms.Compose([\n",
    "        # transforms.RandomRotation(20),  # Randomly rotate images by Â±10 degrees \n",
    "        # transforms.ToTensor(),  # Convert images to tensors\n",
    "        # transforms.Normalize((0.5,), (0.5,))  # Normalize images to [-1, 1]\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Rotation, translation, scaling\n",
    "        transforms.RandomPerspective(distortion_scale=0.1, p=0.5),  # Perspective distortion\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),  # Add Gaussian blur\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "\n",
    "    ])\n",
    "    \n",
    "    # Define transformations for the test set (no augmentations, just normalization)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Load datasets with the respective transformations\n",
    "    train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.MNIST(root='data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    # Split training data into training and validation sets\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "x_train_full,y_train_full,_,_ = load_mnist_data()\n",
    "    \n",
    "x_train_full = x_train_full.to(\"cuda\")\n",
    "y_train_full = y_train_full.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = load_mnist_data_with_augmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data into 50 000 training instances and 10 000 validation instances\n",
    "\n",
    "def split_data(x_train_data, y_train_data):\n",
    "\n",
    "    x_train_data, x_val_data = x_train_data[:50000], x_train_data[50000:]\n",
    "    y_train_data, y_val_data = y_train_data[:50000], y_train_data[50000:]\n",
    "\n",
    "    return (x_train_data, y_train_data), (x_val_data, y_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_val, y_val) = split_data(x_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementig the network and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss_and_accuracy(model, x_data, y_data, criterion, batch):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        output = model(x_data)\n",
    "        loss += criterion(output, y_data).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(y_data.view_as(pred)).sum().item()\n",
    "    return loss / len(x_data), correct / len(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model and training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the neural network\n",
    "class MNISTConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)  # 1 input channel, 16 output channels\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # 16 input channels, 32 output channels\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 32 input channels, 64 output channels\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 10)  # Flattened to a fully connected layer with 10 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # First convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = F.relu(self.conv2(x))  # Second convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = F.relu(self.conv3(x))  # Third convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)            # Fully connected layer\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Training loop that computes the running loss per epoch and validation loss and accuracy per epoch\n",
    "def train(model, train_loader,val_loader, optimizer, criterion, epochs=10, batch_size=16):\n",
    "\n",
    "    first_epoch_running_loss = []\n",
    "\n",
    "    train_evaluations = np.zeros((epochs, 2))\n",
    "    val_evaluations = np.zeros((epochs, 2))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x_val, y_val = next(iter(val_loader))\n",
    "    x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x_train,y_train in train_loader:\n",
    "\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            model.train()\n",
    "            #desc=f'Batches for epoch {epoch + 1}/{epochs}'\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_train)\n",
    "            loss = criterion(output, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch == 0:\n",
    "                # get average loss and append to list\n",
    "                first_epoch_running_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        train_loss, train_acc = calculate_loss_and_accuracy(model, x_train_full, y_train_full, criterion, batch_size)\n",
    "        val_loss, val_acc = calculate_loss_and_accuracy(model, x_val, y_val, criterion, batch_size)\n",
    "\n",
    "        train_evaluations[epoch] = [train_loss, train_acc]\n",
    "        val_evaluations[epoch] = [val_loss, val_acc]\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return first_epoch_running_loss, train_evaluations, val_evaluations\n",
    "\n",
    "\n",
    "# Training on CPU (use \"cuda\" for GPU training if available)\n",
    "\n",
    "\n",
    "# put the data on the device\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'x_train_full' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m first_epoch_running_loss, train_evaluations, val_evaluations \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs, batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m val_evaluations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((epochs, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     37\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m x_train_full \u001b[38;5;241m=\u001b[39m \u001b[43mx_train_full\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m y_train_full \u001b[38;5;241m=\u001b[39m y_train_full\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m x_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_loader))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'x_train_full' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model = MNISTConvNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "first_epoch_running_loss, train_evaluations, val_evaluations = train(model, train_loader, val_loader, optimizer, criterion, epochs=3000, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Test the model on the whole test set.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch model.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        device: The device to run the evaluation on (default is 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        accuracy: The accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.85%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy = test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO here we plot the results stored in *first_epoch_running_loss, train_evaluations, val_evaluations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_results(first_epoch_running_loss, train_evaluations, val_evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuckyou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
