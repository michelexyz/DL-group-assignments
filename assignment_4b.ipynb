{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ_pmgxvGur9"
      },
      "source": [
        "# Assignment 4b - Graph Convolutional Networks\n",
        "## Deep Learning Course - Vrije Universiteit Amsterdam, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEneMITS2agU"
      },
      "source": [
        "#### Instructions on how to use this notebook:\n",
        "\n",
        "This notebook is hosted on Google Colab. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
        "\n",
        "You can also avoid using Colab entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
        "\n",
        "The advantage of using Colab is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is very valuable because for many interesting models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
        "\n",
        "The default Colab runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
        "\n",
        "```sh\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "Note that despite the name, Google Colab does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
        "\n",
        "**Submission:** Upload your notebook in .ipynb format to Canvas. The code and answers to the questions in the notebook are sufficient, no separate report is expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBgoJIpdLI2Y"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsdc7fDp40rQ"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Graphs are very useful data structures that allow us to represent sets of entities and the way they are related among each other. In a graph, entities are also known as *nodes*, and any link between entities is also called an *edge*.\n",
        "\n",
        "Examples of real world objects that can be modeled as graphs are social networks, where entities are people and relations denote friendship; and molecules, where entities are atoms and relations indicate a bond between them.\n",
        "\n",
        "There has been increased interest in the recent years in the application of deep learning architectures to graph-structured data, for tasks like predicting missing relations between entities, classifying entities, and classifying graphs. This interest has been spurred by the introduction of Graph Convolutional Networks (GCNs).\n",
        "\n",
        "In this assignment, you will implement and experiment with one of the first versions of the GCN, proposed by Thomas Kipf and Max Welling in their 2017 paper, [Semi-supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). In particular, the goals of this assignment are to\n",
        "\n",
        "- Understand how GCNs are formulated\n",
        "- Implement the GCN using PyTorch\n",
        "- Train and evaluate a model for semi-supervised node classification in citation networks\n",
        "- Train and evaluate a model for binary classification of molecules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvsuVNczG6pP"
      },
      "source": [
        "### Representing graphs\n",
        "\n",
        "Suppose we have the following graph:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/01-graph.png\" width=\"200\">\n",
        "\n",
        "This is an undirected graph (since the edges have no specified direction) with 4 nodes. One way to represent the connectivity structure of the graph is by means of the **adjacency matrix**. The $i$-th row of the matrix contains a 1 in the $j$-th column, if nodes $i$ and $j$ are connected. For an undirected graph like the one above, this means that the adjacency matrix\n",
        "\n",
        "- Is symmetric (e.g. an edge between 0 and 2 is equivalent as an edge between 2 and 0)\n",
        "- Is square, of size $n\\times n$ where $n$ is the number of nodes\n",
        "\n",
        "The adjacency matrix for the graph above is then the following:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 1 & 0 \\\\\n",
        "0 & 0 & 1 & 0 \\\\\n",
        "1 & 1 & 0 & 1 \\\\\n",
        "0 & 0 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "A second matrix of interest is the **degree matrix**. This is a diagonal matrix where the $i$-th element of the diagonal indicates the number of edges connected to node $i$. Note that these can be obtained from $A$ by summing across the columns, or the rows. For our example, the degree matrix is\n",
        "\n",
        "$$\n",
        "D = \\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 3 & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "For specific applications, each node in the graph will have an associated vector of features $x\\in\\mathbb{R}^c$. If our graph is a social network, then the vector of features can contain information like age, location, and musical tastes, in a specific numeric format. In the case of a molecule, the node could represent an atom and have features like the atomic mass, etc. We can lay out the features in a matrix $X\\in\\mathbb{R}^{n\\times c}$, so that the feature vector for node $i$ is in the $i$-th row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCEQ2ffzHCf2"
      },
      "source": [
        "### Loading a citation network\n",
        "\n",
        "To move to a real world example, we will start with the Cora dataset. This dataset represents a citation network, where nodes are scientific publications, edges denote citations between them, and features are a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) extracted from their contents.\n",
        "\n",
        "This graph contains labels for nodes, that represent a specific topic. We will use these for a node classification task.\n",
        "\n",
        "To easily load it, we will use [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) (PyG), a deep learning library for graph-structured data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yd2bTEBADt-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6aff57e-6b43-4d59-bd5a-59b8d42e0e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_scatter-2.1.2%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_sparse-0.6.18%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch Geometric\n",
        "import torch\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    # Installing torch geometric packages with specific CUDA+PyTorch version.\n",
        "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details\n",
        "    TORCH = torch.__version__.split('+')[0]\n",
        "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "    %pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    %pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    %pip install torch-geometric\n",
        "    import torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Nvh_-qEo1q"
      },
      "source": [
        "We can now use the library to download and import the dataset. Initializing the `Planetoid` class returns a `Dataset` object that can contain multiple graphs. In this task we will only use the `Cora` dataset (the citation network) and hence, we will select only the first element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vuOvwhsHD2YK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bbcba9-2129-46da-e8c4-7fbb599c155f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
        "data = dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4WZkoiHFyZm"
      },
      "source": [
        "\n",
        "#### Question 1 (0.25 pt)\n",
        "\n",
        "The `data` object is an instance of the `Data` class in PyG. Check the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html) and report the following properties of the graph:\n",
        "\n",
        "- Number of nodes\n",
        "- Number of edges\n",
        "- The dimension $c$ of the feature vectors $x\\in\\mathbb{R}^c$\n",
        "- The number of targets for the classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sjVuGJhlJC_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ed5dea-9cb5-4dd2-b13a-1d82a34cdf06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of nodes: 2708\n",
            "Number of features: 1433\n",
            "Number of edges: 10556\n",
            "Number of targets: 7\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nnodes = data.num_nodes\n",
        "in_features = data.num_node_features\n",
        "# Also possible\n",
        "assert (nnodes, in_features) == data.x.size()\n",
        "\n",
        "nedges = data.num_edges\n",
        "\n",
        "ntargets = len(set(map(int, data.y)))\n",
        "# Also possible\n",
        "assert ntargets == dataset.num_classes\n",
        "\n",
        "print(f\"\"\"\n",
        "Number of nodes: {nnodes}\n",
        "Number of features: {in_features}\n",
        "Number of edges: {nedges}\n",
        "Number of targets: {ntargets}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4DrGDAuJ2YO"
      },
      "source": [
        "#### Question 2 (0.25 pt)\n",
        "\n",
        "In PyG, edges are provided in a tensor of shape (2, number of edges). You can access it via `data.edge_index`. Each column in this tensor contains the IDs for two nodes that are connected in the graph.\n",
        "\n",
        "We saw that in an undirected graph, an edge between nodes $i$ and $j$ adds a value of 1 to positions $(i, j)$ and $(j, i)$ of the adjacency matrix. Is this also true for the edge index? That is, if there is an edge $(i, j)$ in `data.edge_index`, is there also an edge for $(j, i)$? This is important to know for the next steps of the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jTRfNxibarRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5686cc52-2a85-48fa-b0a3-73b04849b1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer is: YES\n"
          ]
        }
      ],
      "source": [
        "indices = data.edge_index.t()\n",
        "is_true = all([torch.tensor([j, i]) in indices for i, j in indices])\n",
        "print(f\"The answer is: {'YES' if is_true else 'NO'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOpS3QTYiOqp"
      },
      "source": [
        "#### Question 3 (0.5 pt)\n",
        "\n",
        "In graphs, especially large ones, the adjacency matrix is **sparse**: most entries are zero. Sparse matrices allow for efficient storage and computation.\n",
        "\n",
        "To prepare and pre-process sparse matrices, we will use [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html). Once the matrices are ready, we will convert them to PyTorch tensors.\n",
        "\n",
        "We will use the [Sparse COO format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)). We encourage you to first get familiar with how it works after continuing with the assignment.\n",
        "\n",
        "- Use the [`scipy.sparse.coo_matrix()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) function to build the adjacency matrix. Think of what arguments are needed, and how you can obtain them from the graph data loaded above.\n",
        "- Use the `sum()` method of sparse matrices, together with `scipy.sparse.diags()`, to compute the degree matrix using the definition above.\n",
        "\n",
        "Both resulting matrices must be sparse of type `float32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QC01OjbJs92-"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import scipy.sparse\n",
        "\n",
        "row = data.edge_index[0, :]\n",
        "col = data.edge_index[1, :]\n",
        "assert len(row) == len(col) == nedges\n",
        "\n",
        "# We just put ones for neighbours\n",
        "values = torch.ones(nedges)\n",
        "\n",
        "shape = (nnodes, nnodes)\n",
        "\n",
        "adj_mat = scipy.sparse.coo_matrix((values, (row, col)), shape=shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIEJyQi2TzyY"
      },
      "source": [
        "You might wonder why we suggest to use a scipy sparse matrix, while also PyTorch supports them. The reason is that in the next step, we will be multiplying two sparse matrices, an operation not supported in PyTorch. PyTorch only allows multiplying a sparse matrix with a dense one, something which we will be doing at a later stage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlmzSb0up4LB"
      },
      "source": [
        "### The Graph Convolutional Network\n",
        "\n",
        "The goal of the graph convolution is to take the feature vectors of all nodes $X\\in\\mathbb{R}^{n\\times c}$, and propagate them along the existing edges, to obtain updated representations $Z\\in\\mathbb{R}^{n\\times d}$.\n",
        "\n",
        "\n",
        "The GCN is initially motivated as performing a convolution, similarly as it is done in CNNs for images, for graph-structured data. In Kipf and Welling (2017), a theoretical derivation leads to the following formula:\n",
        "\n",
        "$$\n",
        "Z = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}XW\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $W\\in\\mathbb{R}^{c\\times d}$ is a matrix of parameters to be learned via gradient descent\n",
        "- $\\tilde{A} = A + I_n$, where $I_n$ is an $n\\times n$ identity matrix\n",
        "- $\\tilde{D}$ is the degree matrix computed with $\\tilde{A}$ as the adjacency matrix\n",
        "\n",
        "If we define $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$, the graph convolution can be written as $Z = \\hat{A}XW$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL4b-MTvysBp"
      },
      "source": [
        "#### Question 4 (0.25 pt)\n",
        "\n",
        "Given the formula for the GCN, explain why it operates by propagating feature vectors across the graph. To answer this, it might be useful to recall the definitions of the adjacency and degree matrices, and how they are involved in the formula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgx2SkTTyiSN"
      },
      "source": [
        "The multiplication of the **adjacency matrix** (or some function of it) with the matrix $X$ **has the effect of summing up feature vectors for all neighbouring nodes**. This is effectively propagating feature vectors across the graph. The **degree matrix** provides a normalization constant for each feature vector, just to keep the scale of the features.\n",
        "\n",
        "To give a more precise intuiton, it is helpful to write the formula above in vector form ([source](https://tkipf.github.io/graph-convolutional-networks/)):\n",
        "$$\n",
        "    z_{i} = \\sum_{j \\in \\mathcal{N}_i} \\frac{1}{c_{ij}}x_{j}W\n",
        "$$\n",
        "where we identify $x_j$ as the feature vectors (rows of $X$), $\\mathcal{N}_i$ is the set of neighbours of node $i$ and $c_{ij}$ is the normalization constant. The neighbours of the node $i$ are encoded as ones in the $i$-th row of the adjacency matrix $A$. $\\tilde{A}$ encodes the same information as $A$ plus adding self-loops. $\\tilde{D}$ is the degree matrix (taking into account self-loops), constructed as:\n",
        "$$\n",
        "    \\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}\n",
        "$$\n",
        "which effectively normalizes the summed-up feature vectors according to the number of neighbors of each node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_5SKaLIKa3v"
      },
      "source": [
        "More generally we have ,for each layer $l$, the activations (with non-linearity $\\sigma$):\n",
        "$$\n",
        "H^{(l+1)} = σ(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})\n",
        "$$\n",
        "where $H^{(0)} = X$. From this it's clear how for each layer $l$, after we transform linearly the node features with $H^{(l)}W^{(l)} = M^{(l)}$, we propagate the transformed node features $M$ by one step over through the edges of the graph by multipling $M$ with the normalized adjacency matrix $\\hat{A}$. In fact, $\\hat{A}$ has on each row(representing a single node) non-zero values only for the adjacent nodes. Therefore it selects exclusively the values of the latter for each column of $M$ through the matrix multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUGABEqxylsd"
      },
      "source": [
        "#### Question 5 (0.5 pt)\n",
        "\n",
        "Compute the **normalized adjacency matrix** $\\hat{A}$. The result should be a sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GPZbnSaSyDzO"
      },
      "outputs": [],
      "source": [
        "# Add self-loops. This will turn A into \\tilde{A}\n",
        "adj_mat.setdiag(1)\n",
        "\n",
        "# Sum along columns to get the diagonal\n",
        "diag = adj_mat.sum(axis=1).A1\n",
        "\n",
        "# Compute degree matrix\n",
        "d = scipy.sparse.diags_array(diag, shape=(nnodes, nnodes))\n",
        "\n",
        "# Compute \\hat{A}\n",
        "adj_mat_norm = ((d ** -(1/2)) @ adj_mat @ (d ** -(1/2))).tocoo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLLdGdZoMEy-"
      },
      "source": [
        "#### Question 6 (0.5 pt)\n",
        "\n",
        "So far we have used scipy to build and compute sparse matrices. Since we want to train a GCN with PyTorch, we need to convert $\\hat{A}$ into a sparse PyTorch tensor. You can do this with the [`torch.sparse_coo_tensor()`](https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html) function, making sure to specify `torch.float` as the type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dgDsVHzEM32F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "assert adj_mat_norm.shape == (nnodes, nnodes)\n",
        "\n",
        "coords = np.array(adj_mat_norm.coords)\n",
        "values = adj_mat_norm.data\n",
        "\n",
        "adj_mat_norm_torch = torch.sparse_coo_tensor(indices=coords, values=values, size=(nnodes, nnodes), dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAlRVT5aODkX"
      },
      "source": [
        "#### Question 7 (0.5 pt)\n",
        "\n",
        "We now have all the ingredients to build a GCN layer. Implement a class (inheriting from `torch.nn.Module`) with a learnable matrix of weights $W\\in\\mathbb{R}^{c\\times d}$. Make sure to\n",
        "\n",
        "- Call this class `GCNLayer`\n",
        "- The `__init__()` constructor should take as argument the number of input and output features.\n",
        "- Use `torch.nn.init.kaiming_uniform_` to initialize $W$.\n",
        "- Define the `forward` method, which takes as input $X$ and $\\hat{A}$ and returns $Z$. Note that multiplications involving the sparse matrix $\\hat{A}$ have to be done with `torch.spmm`.\n",
        "\n",
        "Once you have implemented the class, instantiate a layer with the correct number of input features for the Cora dataset, and a number of output features of your choice. Do a forward pass and report the shape of the output tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JFCohhhwPpTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c6a69f-2d73-42c6-e7ac-e07db7b08345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the output is: torch.Size([2708, 200])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class GCNLayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weights = Parameter(torch.empty(in_features, out_features))\n",
        "        self.bias = Parameter(torch.zeros(out_features))\n",
        "        torch.nn.init.kaiming_uniform_(self.weights.T, nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, input_m, adjacency_m):\n",
        "        assert adjacency_m.size(1) == input_m.size(0)\n",
        "        return torch.spmm(adjacency_m, F.linear(input_m, self.weights.T)) + self.bias\n",
        "\n",
        "out_features = 200\n",
        "gcn_layer = GCNLayer(in_features=in_features, out_features=out_features)\n",
        "\n",
        "# Forward pass\n",
        "output = gcn_layer(data.x, adj_mat_norm_torch)\n",
        "print(f\"The shape of the output is: {output.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ptAiizZUKaM"
      },
      "source": [
        "#### Question 8 (0.5 pt)\n",
        "\n",
        "As we have seen so far, the GCN layer implements a special type of linear transformation of the inputs. However, it is often beneficial in deep learning to stack multiple, non-linear transformations of the input features. Implement a second module class for a model with two GCN layers (use the module you implemented in the previous question).\n",
        "\n",
        "- Call this class `GCN`\n",
        "- The constructor must now take as input the number of input features, the output dimension of the first layer (this is the hidden layer), and the output dimension of the output layer.\n",
        "- In the forward pass, add a ReLU activation function after the first layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2zhyu3S9Vj3b"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_features, n_hidden, n_output):\n",
        "        super().__init__()\n",
        "        self.gcn_layer_1 = GCNLayer(in_features=in_features, out_features=n_hidden)\n",
        "        self.gcn_layer_2 = GCNLayer(in_features=n_hidden, out_features=n_output)\n",
        "\n",
        "    def forward(self, x, adjacency):\n",
        "        x = self.gcn_layer_1(x, adjacency)\n",
        "        x = F.relu(x)\n",
        "        x = self.gcn_layer_2(x, adjacency)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NVB-3I5Wfkf"
      },
      "source": [
        "### GCNs for semi-supervised node classification\n",
        "\n",
        "Now that we have a GCN with two layers, we can test its performance in a node classification task. We will pass the input node features $X$ through the GCN layers, and the output will be of size $n\\times k$ where $k$ is the number of classes (which you found in question 1). The label denotes the topic an article in the citation network belongs to (e.g. physics, computer science, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trc4dSa7cuQj"
      },
      "source": [
        "#### Question 9 (1.5 pt)\n",
        "\n",
        "Note that the `data` object contains all labels (for all splits) in `data.y`, and binary masks for the train, validation, and test splits in `data.train_mask`, `data.val_mask`, and `data.test_mask`, respectively. These masks are the same size as `data.y`, and indicate which labels belong to which split.\n",
        "\n",
        "- Create a GCN with two layers (using the class from the previous question), with 32 as the hidden dimension, and the number of output features equal to the number of classes in the Cora dataset.\n",
        "\n",
        "- Use the Adam optimizer with a learning rate of 0.01.\n",
        "\n",
        "- Implement a training loop for the GCN. At each step, pass $X$ and $\\hat{A}$ to the GCN to obtain the logits. Compute the mean cross-entropy loss **only for the training instances**, using the binary masks.\n",
        "\n",
        "- After each training step, evaluate the accuracy for the validation instances.\n",
        "\n",
        "- Train for 100 epochs. Once training is finished, plot the training loss and validation accuracy (in a graph in function of the epoch number), and report the accuracy in the test set.\n",
        "\n",
        "You should obtain an accuracy over 75% on both the validation and test sets. You can also compare your results with the original paper, which also contains results for the Cora dataset. Give a brief discussion on the results of your experiments.\n",
        "\n",
        "Note that in contrast with other tasks, like image classification on some datasets, we don't use mini-batches here. The whole matrix of features and the adjacency is passed to the GCN in one step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5Z2OP_ZRWlmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7056a5-fee1-4e63-ff86-28642a230ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loss: 1.945315 \n",
            " Training accuracy: 0.114286\n",
            " Validation accuracy: 0.5700\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loss: 1.727428 \n",
            " Training accuracy: 0.928571\n",
            " Validation accuracy: 0.6740\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loss: 1.472354 \n",
            " Training accuracy: 0.942857\n",
            " Validation accuracy: 0.6900\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loss: 1.214849 \n",
            " Training accuracy: 0.942857\n",
            " Validation accuracy: 0.7040\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loss: 0.981157 \n",
            " Training accuracy: 0.942857\n",
            " Validation accuracy: 0.7220\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loss: 0.777605 \n",
            " Training accuracy: 0.971429\n",
            " Validation accuracy: 0.7360\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loss: 0.605047 \n",
            " Training accuracy: 0.978571\n",
            " Validation accuracy: 0.7600\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loss: 0.464262 \n",
            " Training accuracy: 0.978571\n",
            " Validation accuracy: 0.7800\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loss: 0.352536 \n",
            " Training accuracy: 0.985714\n",
            " Validation accuracy: 0.7900\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loss: 0.266103 \n",
            " Training accuracy: 0.985714\n",
            " Validation accuracy: 0.7880\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loss: 0.200553 \n",
            " Training accuracy: 0.985714\n",
            " Validation accuracy: 0.7820\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loss: 0.151327 \n",
            " Training accuracy: 0.992857\n",
            " Validation accuracy: 0.7800\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loss: 0.114486 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7760\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loss: 0.086968 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7720\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loss: 0.066522 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7720\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loss: 0.051344 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loss: 0.039974 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loss: 0.031371 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loss: 0.024833 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loss: 0.019851 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loss: 0.016037 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7600\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loss: 0.013092 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loss: 0.010804 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loss: 0.009006 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loss: 0.007583 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loss: 0.006452 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loss: 0.005550 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loss: 0.004826 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7680\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loss: 0.004240 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7700\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loss: 0.003759 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7700\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loss: 0.003358 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7700\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loss: 0.003020 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7700\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loss: 0.002730 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7700\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loss: 0.002481 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loss: 0.002266 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loss: 0.002081 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loss: 0.001920 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loss: 0.001782 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loss: 0.001662 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loss: 0.001557 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loss: 0.001464 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loss: 0.001383 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loss: 0.001311 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loss: 0.001247 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loss: 0.001189 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loss: 0.001138 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loss: 0.001092 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7680\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loss: 0.001050 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7680\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loss: 0.001013 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7680\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loss: 0.000979 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7680\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loss: 0.000948 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loss: 0.000920 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7660\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loss: 0.000895 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loss: 0.000872 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loss: 0.000850 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loss: 0.000831 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loss: 0.000813 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7640\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loss: 0.000796 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loss: 0.000781 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loss: 0.000766 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loss: 0.000752 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loss: 0.000740 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loss: 0.000728 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loss: 0.000716 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loss: 0.000706 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loss: 0.000695 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loss: 0.000686 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7620\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loss: 0.000677 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7600\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loss: 0.000668 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7600\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loss: 0.000660 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7600\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loss: 0.000652 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loss: 0.000644 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loss: 0.000637 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loss: 0.000630 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loss: 0.000623 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loss: 0.000616 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loss: 0.000610 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loss: 0.000604 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loss: 0.000598 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loss: 0.000592 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loss: 0.000587 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loss: 0.000581 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loss: 0.000576 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loss: 0.000571 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loss: 0.000566 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loss: 0.000561 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loss: 0.000556 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loss: 0.000551 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loss: 0.000547 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loss: 0.000542 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loss: 0.000538 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loss: 0.000533 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loss: 0.000529 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loss: 0.000525 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loss: 0.000521 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loss: 0.000517 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loss: 0.000512 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loss: 0.000509 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loss: 0.000505 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loss: 0.000501 \n",
            " Training accuracy: 1.000000\n",
            " Validation accuracy: 0.7580\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# We are taking inspiration from the PyG tutorial: https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "gcn = GCN(in_features=in_features, n_hidden=32, n_output=ntargets).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01)  #, weight_decay=5e-4) (?)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Store data\n",
        "epochs = 100\n",
        "loss_train = torch.zeros(epochs)\n",
        "loss_val = torch.zeros_like(loss_train)\n",
        "acc_train = torch.zeros_like(loss_train)\n",
        "acc_val = torch.zeros_like(loss_train)\n",
        "\n",
        "# Move data to gpu\n",
        "X, t = data.x.to(device), data.y.to(device)\n",
        "adj = adj_mat_norm_torch.to(device)\n",
        "\n",
        "size_train = int(data.train_mask.sum())\n",
        "size_val = int(data.val_mask.sum())\n",
        "\n",
        "for e in range(epochs):\n",
        "    gcn.train()\n",
        "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ### Training set\n",
        "    # Forward\n",
        "    out = gcn(X, adj)[data.train_mask]\n",
        "    target = t[data.train_mask]\n",
        "\n",
        "    # Backpropagate\n",
        "    loss = criterion(out, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Metrics\n",
        "    loss_train[e] = loss.item() # / size_train\n",
        "    ncorrect = (out.argmax(1) == target).type(torch.float).sum().item()\n",
        "    acc_train[e] = ncorrect / size_train\n",
        "\n",
        "    ### Validation set\n",
        "    gcn.eval()\n",
        "    with torch.no_grad():\n",
        "        # Forward\n",
        "        out_val = gcn(X, adj)[data.val_mask]\n",
        "        target_val = t[data.val_mask]\n",
        "\n",
        "        # Metrics\n",
        "        loss_val[e] = criterion(out_val, target_val).item()#  / size_val\n",
        "        ncorrect = (out_val.argmax(1) == target_val).type(torch.float).sum().item()\n",
        "        acc_val[e] = ncorrect / size_val\n",
        "\n",
        "    print(f\"Training loss: {loss_train[e]:>5f} \\n Training accuracy: {acc_train[e]:>5f}\\n Validation accuracy: {acc_val[e]:.4f}\")\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eMLJ13pKa3w",
        "outputId": "be3a9f8a-c5f7-4aa6-e712-14995d843366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7880\n"
          ]
        }
      ],
      "source": [
        "### test set\n",
        "with torch.no_grad():\n",
        "    test_out = gcn(X, adj)[data.test_mask]\n",
        "    test_target = t[data.test_mask]\n",
        "    test_accuracy = (test_out.argmax(1) == test_target).type(torch.float).mean().item()\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "KCpbqAS5Ka3w",
        "outputId": "43a8d760-b9b9-4da7-92ff-1aa007623c16"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "ax[0].plot(range(epochs), loss_train, label=\"Training loss\")\n",
        "ax[0].plot(range(epochs), loss_val, label=\"Validation loss\")\n",
        "ax[0].set_title(\"Loss per epoch\")\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(range(epochs), acc_train, label=\"Acc. train\")\n",
        "ax[1].plot(range(epochs), acc_val, label=\"Acc. val\")\n",
        "ax[1].set_title(\"Accuracy per epoch\")\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Accuracy\")\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb5YjHVClCqo"
      },
      "source": [
        "#### Question 10 (0.5 pt)\n",
        "\n",
        "The paper introduces GCNs as a way to solve a *semi-supervised* classification problem.\n",
        "\n",
        "- What makes this problem semi-supervised?\n",
        "- What is the proportion of labeled data used for training with respect to labeled data in the validation and test sets? What is difference in this context with other benchmark tasks in machine learning, like image classification with MNIST?\n",
        "- Why do you think the GCN performs well in this semi-supervised scenario?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7qw58r1MmCUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8793089-9e23-4178-e7ae-7c5c80d02c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels: tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "Counts per label: tensor([351, 217, 418, 818, 426, 298, 180])\n"
          ]
        }
      ],
      "source": [
        "# Your answer here\n",
        "unique_labels, counts = torch.unique(data.y, return_counts=True)\n",
        "print(\"Unique labels:\", unique_labels)\n",
        "print(\"Counts per label:\", counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqY-mKb2Ka3w",
        "outputId": "bfb954da-106b-4ceb-fc1e-ad7ff2e12eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training nodes: 140\n",
            "Number of validation nodes: 500\n",
            "Number of test nodes: 1000\n"
          ]
        }
      ],
      "source": [
        "# count number of train values\n",
        "num_train_nodes = torch.sum(data.train_mask).item()\n",
        "print(\"Number of training nodes:\", num_train_nodes)\n",
        "\n",
        "num_val_nodes = torch.sum(data.val_mask).item()\n",
        "print(\"Number of validation nodes:\", num_val_nodes)\n",
        "\n",
        "num_test_nodes = torch.sum(data.test_mask).item()\n",
        "print(\"Number of test nodes:\", num_test_nodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPx3QzYIKa3w",
        "outputId": "efbb2cfb-784f-4605-e858-16ca59eb9ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training nodes per class:\n",
            "Class 0: 20\n",
            "Class 1: 20\n",
            "Class 2: 20\n",
            "Class 3: 20\n",
            "Class 4: 20\n",
            "Class 5: 20\n",
            "Class 6: 20\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training nodes per class:\")\n",
        "for label in unique_labels:\n",
        "    num_train_nodes_per_class = torch.sum((data.y == label) & data.train_mask).item()\n",
        "    print(f\"Class {label}: {num_train_nodes_per_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uwna7DyKa3x",
        "outputId": "f0d1a3c0-8e43-4b47-8b59-43b215bce2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion of training data: 0.0854\n",
            "Proportion of validation data: 0.3049\n",
            "Proportion of test data: 0.6098\n",
            "Proportion of training data considering all nodes: 0.0517\n",
            "Proportion of validation data considering all nodes: 0.1846\n",
            "Proportion of test data considering all nodes: 0.3693\n"
          ]
        }
      ],
      "source": [
        "num_train_nodes = torch.sum(data.train_mask).item()\n",
        "num_val_nodes = torch.sum(data.val_mask).item()\n",
        "num_test_nodes = torch.sum(data.test_mask).item()\n",
        "total_nodes = num_train_nodes + num_val_nodes + num_test_nodes\n",
        "total_graph_nodes = data.val_mask.shape[0]\n",
        "\n",
        "\n",
        "print(f\"Proportion of training data: {num_train_nodes / total_nodes:.4f}\")\n",
        "print(f\"Proportion of validation data: {num_val_nodes / total_nodes:.4f}\")\n",
        "print(f\"Proportion of test data: {num_test_nodes / total_nodes:.4f}\")\n",
        "\n",
        "\n",
        "print(f\"Proportion of training data considering all nodes: {num_train_nodes / total_graph_nodes:.4f}\")\n",
        "print(f\"Proportion of validation data considering all nodes: {num_val_nodes / total_graph_nodes:.4f}\")\n",
        "print(f\"Proportion of test data considering all nodes: {num_test_nodes / total_graph_nodes:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gEDDkuwKa3x"
      },
      "source": [
        "\n",
        "**1)**\n",
        "\n",
        "What makes this problem semi-supervised is the fact that we use only 20 labeled examples per class for the training process. I imagine this to be common for ML tasks involving graphs as usually we have only have one large knowledge-base graph for which we want to extract additional data or predict missing links, that we do not already have with the exception of some examples.\n",
        "\n",
        "**2)**\n",
        "\n",
        "The exact proportion of training, validation and test data is 8.5%, 30.5% and 61% respectively, out of a total size of 1640 nodes considered by any of the masks. Intead, taking into account the total number of nodes in the graph, the proportions are 0.05%, 0.19% and 37% respectively. These proportions are far different with the ones that are commonly seen in other ML tasks in which we have tens of thousands of training data points as for the MNIST dataset. The latter has 60,000 images in the training set and 10,000 in the test set.\n",
        "\n",
        "**3)**\n",
        "\n",
        "We report two main reasons that could explain why GCN perform well in this semi-supervised scenario.\n",
        "\n",
        "The first one, as outlined [here](https://tkipf.github.io/graph-convolutional-networks/), revolves round the fact that we could see GCNs as a generalized and differentiable version of the Weisfeiler-Lehman algorithm for graphs. The latter, similarly to GCNs, for every step updates each node based on a summation of the features of all the adjacent nodes. After k steps the nodes can be clusterized based on their newly generated features.\n",
        "\n",
        "The second reason it's centered around the fact that graph data is fundamentally structured. On the other hand, for example, image data can be represented as points in a really high dimentional space for wich complex transformations have to be learned to map these points to the correct classes, because of this a higher amount of data is needed. Graph data, instead , already contains much of the information needed within their morphology, defined by the edges, and little transformations are needed, which can be effectively learned with few examples.\n",
        "\n",
        "Threfore, we could say that the main strenght of GCN is that they take advantage of this structure at every step of the computation through the adjacency matrix.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ihrjZddvz5d"
      },
      "source": [
        "### Loading a dataset of proteins\n",
        "\n",
        "In the previous sections you learned how to pass the adjacency matrix of a graph with a couple of thousand of nodes, to classify each node with a particular label. A different and useful application of GCNs is graph classification.\n",
        "\n",
        "In contrast with the previous part, where there was a single, big graph, in graph classification we have multiple graphs, and each graph can be assigned a label. In this part of the assignment you will implement a classifier for proteins.\n",
        "\n",
        "[Proteins](https://en.wikipedia.org/wiki/Protein_(nutrient)) are parts of the buildings block of life. They consist of chains of amino acids, and can take many shapes. In the PROTEINS dataset, proteins are represented as graphs, where the nodes are amino acids, and an edge between them indicates that they are 6 [Angstroms](https://en.wikipedia.org/wiki/Angstrom) apart. All graphs have a binary label, where 1 means that the protein is not an enzyme.\n",
        "\n",
        "We will start by loading and examining this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmqweMcvnUH6",
        "outputId": "8cec0488-aeb4-480b-e852-690a78682b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "dataset = TUDataset(root='data/TU', name='PROTEINS', use_node_attr=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oF1gyKPXiz-"
      },
      "source": [
        "#### Question 11 (0.25 pt)\n",
        "\n",
        "Unlike in the previous part, where we selected the first element returned by the loading function, note that here we get all the elements returned by `TUDataset()`. `dataset` is an interable object, that has some similar behaviors as a Python list: you can call `len()` on it, and you can takes slices from it.\n",
        "\n",
        "Each element in `dataset` is a `Data` object containing a graph that represents a protein. This is the same type of object that we used in the previous part to store the Cora citation network.\n",
        "\n",
        "Knowing this, answer the following:\n",
        "\n",
        "- How many proteins (graphs) are there in `dataset`?\n",
        "- Take any protein from `dataset`. How many nodes and edges does it contain? What is its label? How many features does each node have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZNPsnXXbbHHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ba237c-b0b2-415e-a2f9-0686e7d66a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of proteins: 1113\n",
            "\n",
            "## Protein with index 18:\n",
            "\tNumber of nodes: 56\n",
            "\tNumber of edges: 158\n",
            "\tNumber of features per node: 4\n",
            "\tThe label of protein 18 is: 0, so it IS an enzyme\n",
            "\n"
          ]
        }
      ],
      "source": [
        "n_proteins = len(dataset)\n",
        "prot_i = 18\n",
        "\n",
        "data = dataset[prot_i]\n",
        "nnodes = data.num_nodes\n",
        "in_features = data.num_node_features\n",
        "is_not_enzyme = data.y.item()\n",
        "\n",
        "# Just chekin\n",
        "assert all(d.num_node_features == in_features for d in dataset)\n",
        "\n",
        "nedges = data.num_edges\n",
        "\n",
        "# Also possible\n",
        "assert (nnodes, in_features) == data.x.size()\n",
        "\n",
        "print(f\"\"\"\n",
        "Number of proteins: {n_proteins}\\n\n",
        "## Protein with index {prot_i}:\n",
        "\\tNumber of nodes: {nnodes}\n",
        "\\tNumber of edges: {nedges}\n",
        "\\tNumber of features per node: {in_features}\n",
        "\\tThe label of protein {prot_i} is: {is_not_enzyme}, so it {'IS NOT' if is_not_enzyme else 'IS'} an enzyme\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHSklBZXpKpR"
      },
      "source": [
        "#### Question 12 (0.5 pt)\n",
        "\n",
        "To properly train and evaluate our model, we need training, validation, and test splits.\n",
        "\n",
        "For reproducibility purposes, we generate a random tensor of indices for you. Use it to extract the three splits from `dataset`.\n",
        "\n",
        "For training, take 80% of the indices (starting from the first element in `indices`), then the following 10% for validation, and the remaining 10% for testing. You can use the indices to index `dataset`.\n",
        "\n",
        "Call the resulting splits `train_dataset`, `valid_dataset`, and `test_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ttY4d1GInn08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce6dc5e-5f20-42a6-8c60-dba133eccd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenght of the dataset: 1113\n",
            "Lenght of training set: 890, lenght of validation set: 111, lenght of test set: 112\n"
          ]
        }
      ],
      "source": [
        "# Don't erase the following three lines\n",
        "import torch\n",
        "import math\n",
        "torch.random.manual_seed(0)\n",
        "indices = torch.randperm(len(dataset))\n",
        "\n",
        "length = len(indices)\n",
        "print(f\"Lenght of the dataset: {length}\")\n",
        "\n",
        "#train_dataset = [dataset[i] for i in indices[:math.floor(length * 0.8)]]\n",
        "train_dataset = dataset[indices[:math.floor(length * 0.8)]]\n",
        "\n",
        "#valid_dataset =  [dataset[i] for i in indices[math.floor(length * 0.8):math.floor(length * 0.9)]]\n",
        "valid_dataset = dataset[indices[math.floor(length * 0.8):math.floor(length * 0.9)]]\n",
        "\n",
        "#test_dataset = [dataset[i] for i in indices[math.floor(length * 0.9):]]\n",
        "test_dataset = dataset[indices[math.floor(length * 0.9):]]\n",
        "print(\"Lenght of training set: {}, lenght of validation set: {}, lenght of test set: {}\".\n",
        "format(len(train_dataset),len(valid_dataset),len(test_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDJbB4CQqsfp"
      },
      "source": [
        "### Working with a batch of graphs\n",
        "\n",
        "When working with the Cora dataset, you used the information in `data.edge_index` to build the sparse normalized adjacency matrix $\\hat{A}$ that is required by the GCN. We could do something similar here: for each graph, we build $\\hat{A}$, and pass it to the GCN. However, if the number of graphs is big, this can really slow down training.\n",
        "\n",
        "To avoid this, we will resort to a very useful trick that also allows us to reuse the same GCN you implemented previously. The trick makes it possible to do a forward pass through the GCN for multiple, disconnected graphs at the same time (instead of only one), much like when you train with mini-batches for other kinds of data.\n",
        "\n",
        "Let's first revisit the propagation rule of the GCN, $Z = \\hat{A}XW$, with an illustration (we have omitted the cells of $X$ and $W$ for clarity):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-forward.png\">\n",
        "\n",
        "If we have multiple graphs, we can still use the same propagation rule, if we\n",
        "\n",
        "- Set $\\hat{A}$ to be a block diagonal matrix, where the blocks are the different adjacency matrices of the graphs\n",
        "- Concatenate the feature matrices along the first dimension\n",
        "\n",
        "This is illustrated in the following figure, for a batch of 3 graphs. Note that the elements outside of the blocks are zero.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-batch-forward.png\">\n",
        "\n",
        "The resulting adjacency matrix $\\hat{A}_B$ can also be built as a sparse matrix, and once we have it together with the concatenated matrix of features, the computation of the graph convolution is exactly the same as before. Note how this trick also allows us to process graphs with different sizes and structures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DLPJ62b2mQ6"
      },
      "source": [
        "#### Question 13 (0.5 pt)\n",
        "\n",
        "\n",
        "Just as the citation network, the graphs in each of the datasets you created in Question 12 also have an `edge_index` attribute, which can be used to compute the normalized adjacency matrix $\\hat{A}$, for each graph.\n",
        "\n",
        "Reusing your code for Questions 3 and 5, define a function `get_a_norm()` that takes as input an element of a dataset (e.g. `train_dataset[0]`), and returns a `scipy.sparse` matrix containing $\\hat{A}$.\n",
        "\n",
        "Note that an element of a dataset has properties like `num_edges`, `num_nodes`, etc. which you can use here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4nvPX2GB8oXp"
      },
      "outputs": [],
      "source": [
        "# Your answer here\n",
        "def get_a_norm(data):\n",
        "\n",
        "    row = data.edge_index[0, :]\n",
        "    col = data.edge_index[1, :]\n",
        "    nedges = data.num_edges\n",
        "    nnodes = data.num_nodes\n",
        "    assert len(row) == len(col) == nedges\n",
        "\n",
        "    # We just put ones for neighbours\n",
        "    values = torch.ones(nedges)\n",
        "\n",
        "    shape = (nnodes, nnodes)\n",
        "\n",
        "    adj_mat = scipy.sparse.coo_matrix((values, (row, col)), shape=shape)\n",
        "\n",
        "    # Add self-loops. This will turn A into \\tilde{A}\n",
        "    adj_mat.setdiag(1)\n",
        "\n",
        "    # Sum along columns to get the diagonal\n",
        "    diag = adj_mat.sum(axis=1).A1\n",
        "\n",
        "    # Compute degree matrix\n",
        "    d = scipy.sparse.diags_array(diag, shape=(nnodes, nnodes))\n",
        "\n",
        "    # Compute \\hat{A}\n",
        "    return ((d ** -(1/2)) @ adj_mat @ (d ** -(1/2))).tocoo()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBrmYBY3AfhW"
      },
      "source": [
        "#### Question 14 (1 pt)\n",
        "\n",
        "To prepare the batch of graphs, we need to collect multiple adjacency matrices, feature matrices, and labels.\n",
        "\n",
        "When using the trick described in the last figure, we see that we have to keep track of when a graph starts and when it ends, so that we can later differentiate the outputs due to $X^{(0)}$, $X^{(1)}$, etc. To achieve this, we will additionally collect a 1D array of batch indices, one for each $X^{(i)}$.\n",
        "\n",
        "The 1D array has as many elements as rows in $X^{(i)}$, and it is filled with the value $i$ (the position of $X^{(i)}$ in the batch):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/03-batch-indices.png\">\n",
        "\n",
        "We will later concatenate all the 1D arrays along the first dimension, just as we will do with all the $X^{(i)}$.\n",
        "\n",
        "Define a function `prepare_graphs_batch()` that takes as input a dataset (e.g. `train_dataset`), and does the following\n",
        "\n",
        "- Create four empty lists:\n",
        "  - `adj_matrices`\n",
        "  - `feature_matrices`\n",
        "  - `batch_indices`\n",
        "  - `labels`\n",
        "- Iterate over the input dataset, getting one graph at a time. At each step, use your function from Question 13 to append the adjacency matrix to `adj_matrices`, append the matrix of input features to `feature_matrices`, create the array of batch indices (as explained above) and append it to `batch_indices`, and append the label of the graph to `labels`. **Make sure to convert the label to float**.\n",
        "- Once the loop is over, use `scipy.sparse.block_diag()` to build the block diagonal matrix $\\hat{A}_B$. Convert it to the COO format, and then use your answer to Question 6 to turn it into a sparse PyTorch tensor.\n",
        "- Use `torch.cat()` to concatenate the tensors in `feature_matrices` along the first dimension. Do this also for `batch_indices` and `labels`.\n",
        "- Return the 4 tensors computed in the previous two items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SsQ0-JjSqFgD"
      },
      "outputs": [],
      "source": [
        "def prepare_graphs_batch(graphs):\n",
        "    adj_matrices, feature_matrices, batch_indices, labels = [], [], [], []\n",
        "\n",
        "    batch_num = 0\n",
        "    for graph in graphs:\n",
        "        labels.append(graph.y.float())\n",
        "        batch_indices.append(torch.tensor([batch_num for _ in range(0,graph.x.shape[0])],dtype=torch.int64))\n",
        "        feature_matrices.append(graph.x)\n",
        "\n",
        "        #do the adj_matrix\n",
        "        matrix = get_a_norm(graph)\n",
        "        adj_matrices.append(matrix)\n",
        "        batch_num += 1\n",
        "\n",
        "    labels = torch.cat(labels,dim=0)\n",
        "    feature_matrices = torch.cat(feature_matrices,dim=0)\n",
        "    batch_indices = torch.cat(batch_indices,dim=0)\n",
        "\n",
        "    # creat block diagonal adjacency matrix\n",
        "    block_adj = scipy.sparse.block_diag(adj_matrices)\n",
        "\n",
        "    coords = np.array(block_adj.coords)\n",
        "    values = block_adj.data\n",
        "    nnodes = block_adj.shape[0]\n",
        "    block_adj_torch = torch.sparse_coo_tensor(indices=coords, values=values, size=(nnodes, nnodes), dtype=torch.float)\n",
        "\n",
        "\n",
        "    return block_adj_torch, feature_matrices, batch_indices, labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i73P_EU0MSPX"
      },
      "source": [
        "Once your answer for the previous question is ready, you can run the next cell to prepare all the required information, for the train, validation, and test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Iol5FxJGMmAU"
      },
      "outputs": [],
      "source": [
        "train_a_norm, train_features, train_batch_idx, train_labels = prepare_graphs_batch(train_dataset)\n",
        "valid_a_norm, valid_features, valid_batch_idx, valid_labels = prepare_graphs_batch(valid_dataset)\n",
        "test_a_norm, test_features, test_batch_idx, test_labels = prepare_graphs_batch(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6q-JU87NClh"
      },
      "source": [
        "### GCNs for graph classification\n",
        "\n",
        "We now have all the ingredients to pass a batch of graphs to a GCN. However, for each graph in the batch, the output $Z^{(i)}$ contains one row for each node in the graph. If the goal is to do classification at the graph level, we have to *pool* these vectors to then compute the required logits for classification.\n",
        "\n",
        "This operation is similar as how pooling works in a CNN. We could consider taking the mean of the vectors, the sum, or use max-pooling. The difference with respect to CNNs is that in our case, we have a batch of graphs, each potentially with a different number of nodes.\n",
        "\n",
        "To implement this specific pooling, we can use the scatter operation in the `torch_scatter` library, which comes when installing PyG. We will use it, together with the tensor of batch indices from the previous two questions, to pool the outputs of the GCN for each graph, into a single vector:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/04-scatter.png\">\n",
        "\n",
        "You can check more details in the [documentation](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY87DX1uRhnY"
      },
      "source": [
        "#### Question 15 (1.0 pt)\n",
        "\n",
        "Implement a `GraphClassifier` module using PyTorch.\n",
        "\n",
        "- The constructor should take as arguments the number of input features, the hidden dimension, and the number of classes.\n",
        "- The model should contain a instance of the `GCN` module (as you implemented it in Question 8). Use the same value for the hidden dimension and the number of output features (recall that your `GCN` module from Question 8 has two GCN layers).\n",
        "- The model should also contain a `torch.nn.Linear` layer, with the hidden dimension as the input features, and the number of classes as the output.\n",
        "- The forward method receives the concatenated matrix of features, the sparse block diagonal adjacency matrix, and the batch indices (the latter is used when calling `scatter`).\n",
        "- Use the following architecture in the forward pass:\n",
        "  - GCN $\\to$ ReLU $\\to$ scatter (max) $\\to$ Linear.\n",
        "\n",
        "The output of the forward should be a 1D tensor (you might need to call `squeeze` to get rid of extra dimensions) containing the logits for all graphs in the batch, for the binary classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "750WraywwYDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a91e8f-58c4-46b8-818c-e0db5647edbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([890])\n"
          ]
        }
      ],
      "source": [
        "from torch_scatter import scatter\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GraphClassifer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, n_hidden, n_output):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gcn = GCN(in_features=in_features, n_hidden=n_hidden, n_output=n_hidden)\n",
        "\n",
        "        self.fc = torch.nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x, adjacency, batch_idx):\n",
        "        x = self.gcn(x, adjacency)\n",
        "        x = F.relu(x)\n",
        "        x = scatter(x, batch_idx, dim=0, reduce='max')\n",
        "        x = self.fc(x)\n",
        "        return x.squeeze()\n",
        "        # return F.log_softmax(x, dim=1).squeeze()\n",
        "\n",
        "\n",
        "\n",
        "in_features = train_features.shape[1]\n",
        "graph_classifier = GraphClassifer(in_features=in_features, n_hidden=32, n_output=1)\n",
        "output = graph_classifier(train_features, train_a_norm, train_batch_idx)\n",
        "print(output.size())\n",
        "\n",
        "assert output.size()[0] == len(train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1PHy-_vTjgh"
      },
      "source": [
        "#### Question 16 (1.5 pt)\n",
        "\n",
        "Implement a training loop for the graph classifier. Use the data from Question 14 to train and evaluate the model.\n",
        "\n",
        "We encourage you to use a GPU in this section for faster training. Note that if you change the runtime at this point, you must re-execute several of the cells above, including the ones that install PyG.\n",
        "\n",
        "- Instantiate a classifier with 32 as the hidden dimension\n",
        "- Use Adam with a learning rate of 1e-3.\n",
        "- Use `torch.nn.BCEWithLogitsLoss` as the loss function.\n",
        "- Train for 5,000 epochs. Once training is done, plot the loss curve and the accuracy in the validation set. Then report the accuracy in the test set.\n",
        "\n",
        "**Note:** the logits from the output of the classifier come from a linear layer. To compute actual predictions for the calculation of the accuracy, pass the logits through `torch.sigmoid()`, and set the predicted values to 1 whenever they are greater than 0.5, and to 0 otherwise.\n",
        "\n",
        "You should get an accuracy equal to or higher than 70% in the validation and test sets. Can you beat the [state-of-the-art](https://paperswithcode.com/sota/graph-classification-on-proteins)? Feel free to modify your architecture and experiment with it.\n",
        "\n",
        "Discuss what you observe during training and your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9DbGAs8W2Xja",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78e0b569-6e20-4d4e-b919-14cdcfa92dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Training loss: 0.425017 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4002\n",
            "-------------------------------\n",
            "Training loss: 0.425018 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4003\n",
            "-------------------------------\n",
            "Training loss: 0.424952 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4004\n",
            "-------------------------------\n",
            "Training loss: 0.424998 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4005\n",
            "-------------------------------\n",
            "Training loss: 0.424929 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4006\n",
            "-------------------------------\n",
            "Training loss: 0.424938 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4007\n",
            "-------------------------------\n",
            "Training loss: 0.424911 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4008\n",
            "-------------------------------\n",
            "Training loss: 0.424941 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4009\n",
            "-------------------------------\n",
            "Training loss: 0.424927 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4010\n",
            "-------------------------------\n",
            "Training loss: 0.424902 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4011\n",
            "-------------------------------\n",
            "Training loss: 0.424887 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4012\n",
            "-------------------------------\n",
            "Training loss: 0.424891 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4013\n",
            "-------------------------------\n",
            "Training loss: 0.424892 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4014\n",
            "-------------------------------\n",
            "Training loss: 0.424881 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4015\n",
            "-------------------------------\n",
            "Training loss: 0.424883 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4016\n",
            "-------------------------------\n",
            "Training loss: 0.424863 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4017\n",
            "-------------------------------\n",
            "Training loss: 0.424861 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4018\n",
            "-------------------------------\n",
            "Training loss: 0.424864 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4019\n",
            "-------------------------------\n",
            "Training loss: 0.424881 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4020\n",
            "-------------------------------\n",
            "Training loss: 0.424876 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4021\n",
            "-------------------------------\n",
            "Training loss: 0.424885 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4022\n",
            "-------------------------------\n",
            "Training loss: 0.424898 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4023\n",
            "-------------------------------\n",
            "Training loss: 0.424918 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4024\n",
            "-------------------------------\n",
            "Training loss: 0.424898 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4025\n",
            "-------------------------------\n",
            "Training loss: 0.424887 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4026\n",
            "-------------------------------\n",
            "Training loss: 0.424879 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4027\n",
            "-------------------------------\n",
            "Training loss: 0.424853 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4028\n",
            "-------------------------------\n",
            "Training loss: 0.424846 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4029\n",
            "-------------------------------\n",
            "Training loss: 0.424813 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4030\n",
            "-------------------------------\n",
            "Training loss: 0.424782 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4031\n",
            "-------------------------------\n",
            "Training loss: 0.424767 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4032\n",
            "-------------------------------\n",
            "Training loss: 0.424757 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4033\n",
            "-------------------------------\n",
            "Training loss: 0.424744 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4034\n",
            "-------------------------------\n",
            "Training loss: 0.424739 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4035\n",
            "-------------------------------\n",
            "Training loss: 0.424728 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4036\n",
            "-------------------------------\n",
            "Training loss: 0.424731 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4037\n",
            "-------------------------------\n",
            "Training loss: 0.424719 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4038\n",
            "-------------------------------\n",
            "Training loss: 0.424726 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4039\n",
            "-------------------------------\n",
            "Training loss: 0.424717 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4040\n",
            "-------------------------------\n",
            "Training loss: 0.424727 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4041\n",
            "-------------------------------\n",
            "Training loss: 0.424725 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4042\n",
            "-------------------------------\n",
            "Training loss: 0.424739 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4043\n",
            "-------------------------------\n",
            "Training loss: 0.424787 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4044\n",
            "-------------------------------\n",
            "Training loss: 0.424862 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4045\n",
            "-------------------------------\n",
            "Training loss: 0.424915 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4046\n",
            "-------------------------------\n",
            "Training loss: 0.424933 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4047\n",
            "-------------------------------\n",
            "Training loss: 0.424885 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4048\n",
            "-------------------------------\n",
            "Training loss: 0.424830 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4049\n",
            "-------------------------------\n",
            "Training loss: 0.424748 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4050\n",
            "-------------------------------\n",
            "Training loss: 0.424691 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4051\n",
            "-------------------------------\n",
            "Training loss: 0.424676 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4052\n",
            "-------------------------------\n",
            "Training loss: 0.424650 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4053\n",
            "-------------------------------\n",
            "Training loss: 0.424690 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4054\n",
            "-------------------------------\n",
            "Training loss: 0.424717 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4055\n",
            "-------------------------------\n",
            "Training loss: 0.424773 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4056\n",
            "-------------------------------\n",
            "Training loss: 0.424739 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4057\n",
            "-------------------------------\n",
            "Training loss: 0.424726 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4058\n",
            "-------------------------------\n",
            "Training loss: 0.424670 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4059\n",
            "-------------------------------\n",
            "Training loss: 0.424631 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4060\n",
            "-------------------------------\n",
            "Training loss: 0.424631 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4061\n",
            "-------------------------------\n",
            "Training loss: 0.424636 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4062\n",
            "-------------------------------\n",
            "Training loss: 0.424612 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4063\n",
            "-------------------------------\n",
            "Training loss: 0.424647 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4064\n",
            "-------------------------------\n",
            "Training loss: 0.424665 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4065\n",
            "-------------------------------\n",
            "Training loss: 0.424601 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4066\n",
            "-------------------------------\n",
            "Training loss: 0.424673 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4067\n",
            "-------------------------------\n",
            "Training loss: 0.424622 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4068\n",
            "-------------------------------\n",
            "Training loss: 0.424589 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4069\n",
            "-------------------------------\n",
            "Training loss: 0.424610 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4070\n",
            "-------------------------------\n",
            "Training loss: 0.424597 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4071\n",
            "-------------------------------\n",
            "Training loss: 0.424656 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4072\n",
            "-------------------------------\n",
            "Training loss: 0.424632 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4073\n",
            "-------------------------------\n",
            "Training loss: 0.424633 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4074\n",
            "-------------------------------\n",
            "Training loss: 0.424660 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4075\n",
            "-------------------------------\n",
            "Training loss: 0.424659 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4076\n",
            "-------------------------------\n",
            "Training loss: 0.424637 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4077\n",
            "-------------------------------\n",
            "Training loss: 0.424570 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4078\n",
            "-------------------------------\n",
            "Training loss: 0.424545 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4079\n",
            "-------------------------------\n",
            "Training loss: 0.424493 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4080\n",
            "-------------------------------\n",
            "Training loss: 0.424542 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4081\n",
            "-------------------------------\n",
            "Training loss: 0.424537 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4082\n",
            "-------------------------------\n",
            "Training loss: 0.424520 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4083\n",
            "-------------------------------\n",
            "Training loss: 0.424568 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4084\n",
            "-------------------------------\n",
            "Training loss: 0.424502 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4085\n",
            "-------------------------------\n",
            "Training loss: 0.424510 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4086\n",
            "-------------------------------\n",
            "Training loss: 0.424484 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4087\n",
            "-------------------------------\n",
            "Training loss: 0.424508 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4088\n",
            "-------------------------------\n",
            "Training loss: 0.424487 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4089\n",
            "-------------------------------\n",
            "Training loss: 0.424516 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4090\n",
            "-------------------------------\n",
            "Training loss: 0.424534 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4091\n",
            "-------------------------------\n",
            "Training loss: 0.424522 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4092\n",
            "-------------------------------\n",
            "Training loss: 0.424563 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4093\n",
            "-------------------------------\n",
            "Training loss: 0.424509 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4094\n",
            "-------------------------------\n",
            "Training loss: 0.424493 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4095\n",
            "-------------------------------\n",
            "Training loss: 0.424453 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4096\n",
            "-------------------------------\n",
            "Training loss: 0.424422 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4097\n",
            "-------------------------------\n",
            "Training loss: 0.424408 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4098\n",
            "-------------------------------\n",
            "Training loss: 0.424384 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4099\n",
            "-------------------------------\n",
            "Training loss: 0.424376 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4100\n",
            "-------------------------------\n",
            "Training loss: 0.424373 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4101\n",
            "-------------------------------\n",
            "Training loss: 0.424371 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4102\n",
            "-------------------------------\n",
            "Training loss: 0.424368 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4103\n",
            "-------------------------------\n",
            "Training loss: 0.424355 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4104\n",
            "-------------------------------\n",
            "Training loss: 0.424367 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4105\n",
            "-------------------------------\n",
            "Training loss: 0.424358 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4106\n",
            "-------------------------------\n",
            "Training loss: 0.424351 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4107\n",
            "-------------------------------\n",
            "Training loss: 0.424366 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4108\n",
            "-------------------------------\n",
            "Training loss: 0.424372 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4109\n",
            "-------------------------------\n",
            "Training loss: 0.424372 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4110\n",
            "-------------------------------\n",
            "Training loss: 0.424403 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4111\n",
            "-------------------------------\n",
            "Training loss: 0.424399 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4112\n",
            "-------------------------------\n",
            "Training loss: 0.424411 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4113\n",
            "-------------------------------\n",
            "Training loss: 0.424388 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4114\n",
            "-------------------------------\n",
            "Training loss: 0.424378 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4115\n",
            "-------------------------------\n",
            "Training loss: 0.424354 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4116\n",
            "-------------------------------\n",
            "Training loss: 0.424353 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4117\n",
            "-------------------------------\n",
            "Training loss: 0.424316 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4118\n",
            "-------------------------------\n",
            "Training loss: 0.424315 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4119\n",
            "-------------------------------\n",
            "Training loss: 0.424281 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4120\n",
            "-------------------------------\n",
            "Training loss: 0.424267 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4121\n",
            "-------------------------------\n",
            "Training loss: 0.424261 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4122\n",
            "-------------------------------\n",
            "Training loss: 0.424254 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4123\n",
            "-------------------------------\n",
            "Training loss: 0.424250 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4124\n",
            "-------------------------------\n",
            "Training loss: 0.424250 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4125\n",
            "-------------------------------\n",
            "Training loss: 0.424256 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4126\n",
            "-------------------------------\n",
            "Training loss: 0.424272 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4127\n",
            "-------------------------------\n",
            "Training loss: 0.424276 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4128\n",
            "-------------------------------\n",
            "Training loss: 0.424284 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4129\n",
            "-------------------------------\n",
            "Training loss: 0.424274 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4130\n",
            "-------------------------------\n",
            "Training loss: 0.424261 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4131\n",
            "-------------------------------\n",
            "Training loss: 0.424252 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4132\n",
            "-------------------------------\n",
            "Training loss: 0.424264 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4133\n",
            "-------------------------------\n",
            "Training loss: 0.424294 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4134\n",
            "-------------------------------\n",
            "Training loss: 0.424297 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4135\n",
            "-------------------------------\n",
            "Training loss: 0.424297 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4136\n",
            "-------------------------------\n",
            "Training loss: 0.424296 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4137\n",
            "-------------------------------\n",
            "Training loss: 0.424315 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4138\n",
            "-------------------------------\n",
            "Training loss: 0.424313 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4139\n",
            "-------------------------------\n",
            "Training loss: 0.424299 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4140\n",
            "-------------------------------\n",
            "Training loss: 0.424244 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4141\n",
            "-------------------------------\n",
            "Training loss: 0.424209 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4142\n",
            "-------------------------------\n",
            "Training loss: 0.424169 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4143\n",
            "-------------------------------\n",
            "Training loss: 0.424144 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4144\n",
            "-------------------------------\n",
            "Training loss: 0.424113 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4145\n",
            "-------------------------------\n",
            "Training loss: 0.424108 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4146\n",
            "-------------------------------\n",
            "Training loss: 0.424104 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4147\n",
            "-------------------------------\n",
            "Training loss: 0.424130 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4148\n",
            "-------------------------------\n",
            "Training loss: 0.424156 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4149\n",
            "-------------------------------\n",
            "Training loss: 0.424188 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4150\n",
            "-------------------------------\n",
            "Training loss: 0.424209 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4151\n",
            "-------------------------------\n",
            "Training loss: 0.424212 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4152\n",
            "-------------------------------\n",
            "Training loss: 0.424185 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4153\n",
            "-------------------------------\n",
            "Training loss: 0.424139 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4154\n",
            "-------------------------------\n",
            "Training loss: 0.424110 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4155\n",
            "-------------------------------\n",
            "Training loss: 0.424072 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4156\n",
            "-------------------------------\n",
            "Training loss: 0.424074 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4157\n",
            "-------------------------------\n",
            "Training loss: 0.424033 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4158\n",
            "-------------------------------\n",
            "Training loss: 0.424052 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4159\n",
            "-------------------------------\n",
            "Training loss: 0.424027 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4160\n",
            "-------------------------------\n",
            "Training loss: 0.424039 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4161\n",
            "-------------------------------\n",
            "Training loss: 0.424054 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4162\n",
            "-------------------------------\n",
            "Training loss: 0.424065 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4163\n",
            "-------------------------------\n",
            "Training loss: 0.424058 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4164\n",
            "-------------------------------\n",
            "Training loss: 0.424025 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4165\n",
            "-------------------------------\n",
            "Training loss: 0.424004 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4166\n",
            "-------------------------------\n",
            "Training loss: 0.423977 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4167\n",
            "-------------------------------\n",
            "Training loss: 0.423963 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4168\n",
            "-------------------------------\n",
            "Training loss: 0.423965 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4169\n",
            "-------------------------------\n",
            "Training loss: 0.423989 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4170\n",
            "-------------------------------\n",
            "Training loss: 0.423991 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4171\n",
            "-------------------------------\n",
            "Training loss: 0.424001 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4172\n",
            "-------------------------------\n",
            "Training loss: 0.423983 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4173\n",
            "-------------------------------\n",
            "Training loss: 0.423970 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4174\n",
            "-------------------------------\n",
            "Training loss: 0.423949 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4175\n",
            "-------------------------------\n",
            "Training loss: 0.423945 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4176\n",
            "-------------------------------\n",
            "Training loss: 0.423924 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4177\n",
            "-------------------------------\n",
            "Training loss: 0.423899 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4178\n",
            "-------------------------------\n",
            "Training loss: 0.423900 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4179\n",
            "-------------------------------\n",
            "Training loss: 0.423890 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4180\n",
            "-------------------------------\n",
            "Training loss: 0.423902 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4181\n",
            "-------------------------------\n",
            "Training loss: 0.423898 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4182\n",
            "-------------------------------\n",
            "Training loss: 0.423902 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4183\n",
            "-------------------------------\n",
            "Training loss: 0.423889 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4184\n",
            "-------------------------------\n",
            "Training loss: 0.423896 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4185\n",
            "-------------------------------\n",
            "Training loss: 0.423909 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4186\n",
            "-------------------------------\n",
            "Training loss: 0.423933 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4187\n",
            "-------------------------------\n",
            "Training loss: 0.423929 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4188\n",
            "-------------------------------\n",
            "Training loss: 0.423954 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4189\n",
            "-------------------------------\n",
            "Training loss: 0.423893 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4190\n",
            "-------------------------------\n",
            "Training loss: 0.423856 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4191\n",
            "-------------------------------\n",
            "Training loss: 0.423817 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4192\n",
            "-------------------------------\n",
            "Training loss: 0.423802 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4193\n",
            "-------------------------------\n",
            "Training loss: 0.423774 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4194\n",
            "-------------------------------\n",
            "Training loss: 0.423777 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4195\n",
            "-------------------------------\n",
            "Training loss: 0.423769 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4196\n",
            "-------------------------------\n",
            "Training loss: 0.423760 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4197\n",
            "-------------------------------\n",
            "Training loss: 0.423779 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4198\n",
            "-------------------------------\n",
            "Training loss: 0.423777 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4199\n",
            "-------------------------------\n",
            "Training loss: 0.423802 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4200\n",
            "-------------------------------\n",
            "Training loss: 0.423823 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4201\n",
            "-------------------------------\n",
            "Training loss: 0.423879 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4202\n",
            "-------------------------------\n",
            "Training loss: 0.423925 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4203\n",
            "-------------------------------\n",
            "Training loss: 0.424022 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4204\n",
            "-------------------------------\n",
            "Training loss: 0.424025 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4205\n",
            "-------------------------------\n",
            "Training loss: 0.423957 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4206\n",
            "-------------------------------\n",
            "Training loss: 0.423844 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4207\n",
            "-------------------------------\n",
            "Training loss: 0.423742 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4208\n",
            "-------------------------------\n",
            "Training loss: 0.423687 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4209\n",
            "-------------------------------\n",
            "Training loss: 0.423683 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4210\n",
            "-------------------------------\n",
            "Training loss: 0.423740 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4211\n",
            "-------------------------------\n",
            "Training loss: 0.423790 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4212\n",
            "-------------------------------\n",
            "Training loss: 0.423833 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4213\n",
            "-------------------------------\n",
            "Training loss: 0.423838 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4214\n",
            "-------------------------------\n",
            "Training loss: 0.423802 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4215\n",
            "-------------------------------\n",
            "Training loss: 0.423715 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4216\n",
            "-------------------------------\n",
            "Training loss: 0.423645 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4217\n",
            "-------------------------------\n",
            "Training loss: 0.423605 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4218\n",
            "-------------------------------\n",
            "Training loss: 0.423600 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4219\n",
            "-------------------------------\n",
            "Training loss: 0.423599 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4220\n",
            "-------------------------------\n",
            "Training loss: 0.423604 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4221\n",
            "-------------------------------\n",
            "Training loss: 0.423612 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4222\n",
            "-------------------------------\n",
            "Training loss: 0.423622 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4223\n",
            "-------------------------------\n",
            "Training loss: 0.423596 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4224\n",
            "-------------------------------\n",
            "Training loss: 0.423564 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4225\n",
            "-------------------------------\n",
            "Training loss: 0.423546 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4226\n",
            "-------------------------------\n",
            "Training loss: 0.423543 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4227\n",
            "-------------------------------\n",
            "Training loss: 0.423546 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4228\n",
            "-------------------------------\n",
            "Training loss: 0.423578 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4229\n",
            "-------------------------------\n",
            "Training loss: 0.423582 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4230\n",
            "-------------------------------\n",
            "Training loss: 0.423597 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4231\n",
            "-------------------------------\n",
            "Training loss: 0.423564 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4232\n",
            "-------------------------------\n",
            "Training loss: 0.423551 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4233\n",
            "-------------------------------\n",
            "Training loss: 0.423530 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4234\n",
            "-------------------------------\n",
            "Training loss: 0.423516 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4235\n",
            "-------------------------------\n",
            "Training loss: 0.423494 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4236\n",
            "-------------------------------\n",
            "Training loss: 0.423497 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4237\n",
            "-------------------------------\n",
            "Training loss: 0.423468 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4238\n",
            "-------------------------------\n",
            "Training loss: 0.423459 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4239\n",
            "-------------------------------\n",
            "Training loss: 0.423451 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4240\n",
            "-------------------------------\n",
            "Training loss: 0.423440 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4241\n",
            "-------------------------------\n",
            "Training loss: 0.423423 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4242\n",
            "-------------------------------\n",
            "Training loss: 0.423412 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4243\n",
            "-------------------------------\n",
            "Training loss: 0.423412 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4244\n",
            "-------------------------------\n",
            "Training loss: 0.423412 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4245\n",
            "-------------------------------\n",
            "Training loss: 0.423439 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4246\n",
            "-------------------------------\n",
            "Training loss: 0.423454 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4247\n",
            "-------------------------------\n",
            "Training loss: 0.423493 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4248\n",
            "-------------------------------\n",
            "Training loss: 0.423498 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4249\n",
            "-------------------------------\n",
            "Training loss: 0.423509 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4250\n",
            "-------------------------------\n",
            "Training loss: 0.423465 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4251\n",
            "-------------------------------\n",
            "Training loss: 0.423433 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4252\n",
            "-------------------------------\n",
            "Training loss: 0.423390 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4253\n",
            "-------------------------------\n",
            "Training loss: 0.423382 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4254\n",
            "-------------------------------\n",
            "Training loss: 0.423326 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4255\n",
            "-------------------------------\n",
            "Training loss: 0.423349 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4256\n",
            "-------------------------------\n",
            "Training loss: 0.423321 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4257\n",
            "-------------------------------\n",
            "Training loss: 0.423366 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4258\n",
            "-------------------------------\n",
            "Training loss: 0.423360 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4259\n",
            "-------------------------------\n",
            "Training loss: 0.423392 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4260\n",
            "-------------------------------\n",
            "Training loss: 0.423410 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4261\n",
            "-------------------------------\n",
            "Training loss: 0.423385 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4262\n",
            "-------------------------------\n",
            "Training loss: 0.423407 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4263\n",
            "-------------------------------\n",
            "Training loss: 0.423328 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4264\n",
            "-------------------------------\n",
            "Training loss: 0.423324 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4265\n",
            "-------------------------------\n",
            "Training loss: 0.423274 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4266\n",
            "-------------------------------\n",
            "Training loss: 0.423263 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4267\n",
            "-------------------------------\n",
            "Training loss: 0.423286 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4268\n",
            "-------------------------------\n",
            "Training loss: 0.423283 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4269\n",
            "-------------------------------\n",
            "Training loss: 0.423328 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4270\n",
            "-------------------------------\n",
            "Training loss: 0.423304 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4271\n",
            "-------------------------------\n",
            "Training loss: 0.423315 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4272\n",
            "-------------------------------\n",
            "Training loss: 0.423305 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4273\n",
            "-------------------------------\n",
            "Training loss: 0.423232 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4274\n",
            "-------------------------------\n",
            "Training loss: 0.423274 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4275\n",
            "-------------------------------\n",
            "Training loss: 0.423226 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4276\n",
            "-------------------------------\n",
            "Training loss: 0.423194 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4277\n",
            "-------------------------------\n",
            "Training loss: 0.423233 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4278\n",
            "-------------------------------\n",
            "Training loss: 0.423182 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4279\n",
            "-------------------------------\n",
            "Training loss: 0.423246 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4280\n",
            "-------------------------------\n",
            "Training loss: 0.423217 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4281\n",
            "-------------------------------\n",
            "Training loss: 0.423228 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4282\n",
            "-------------------------------\n",
            "Training loss: 0.423225 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4283\n",
            "-------------------------------\n",
            "Training loss: 0.423192 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4284\n",
            "-------------------------------\n",
            "Training loss: 0.423208 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4285\n",
            "-------------------------------\n",
            "Training loss: 0.423164 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4286\n",
            "-------------------------------\n",
            "Training loss: 0.423117 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4287\n",
            "-------------------------------\n",
            "Training loss: 0.423112 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4288\n",
            "-------------------------------\n",
            "Training loss: 0.423090 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4289\n",
            "-------------------------------\n",
            "Training loss: 0.423108 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4290\n",
            "-------------------------------\n",
            "Training loss: 0.423143 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4291\n",
            "-------------------------------\n",
            "Training loss: 0.423191 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4292\n",
            "-------------------------------\n",
            "Training loss: 0.423198 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4293\n",
            "-------------------------------\n",
            "Training loss: 0.423175 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4294\n",
            "-------------------------------\n",
            "Training loss: 0.423132 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4295\n",
            "-------------------------------\n",
            "Training loss: 0.423089 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4296\n",
            "-------------------------------\n",
            "Training loss: 0.423040 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4297\n",
            "-------------------------------\n",
            "Training loss: 0.423025 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4298\n",
            "-------------------------------\n",
            "Training loss: 0.423029 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4299\n",
            "-------------------------------\n",
            "Training loss: 0.423067 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4300\n",
            "-------------------------------\n",
            "Training loss: 0.423045 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4301\n",
            "-------------------------------\n",
            "Training loss: 0.423107 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4302\n",
            "-------------------------------\n",
            "Training loss: 0.423081 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4303\n",
            "-------------------------------\n",
            "Training loss: 0.423086 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4304\n",
            "-------------------------------\n",
            "Training loss: 0.423087 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4305\n",
            "-------------------------------\n",
            "Training loss: 0.423050 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4306\n",
            "-------------------------------\n",
            "Training loss: 0.423089 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4307\n",
            "-------------------------------\n",
            "Training loss: 0.423057 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4308\n",
            "-------------------------------\n",
            "Training loss: 0.423012 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4309\n",
            "-------------------------------\n",
            "Training loss: 0.423007 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4310\n",
            "-------------------------------\n",
            "Training loss: 0.422998 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4311\n",
            "-------------------------------\n",
            "Training loss: 0.423011 \n",
            " Training accuracy: 0.798876\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4312\n",
            "-------------------------------\n",
            "Training loss: 0.422963 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4313\n",
            "-------------------------------\n",
            "Training loss: 0.422936 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4314\n",
            "-------------------------------\n",
            "Training loss: 0.422952 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4315\n",
            "-------------------------------\n",
            "Training loss: 0.422901 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4316\n",
            "-------------------------------\n",
            "Training loss: 0.422995 \n",
            " Training accuracy: 0.798876\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4317\n",
            "-------------------------------\n",
            "Training loss: 0.422933 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4318\n",
            "-------------------------------\n",
            "Training loss: 0.422941 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4319\n",
            "-------------------------------\n",
            "Training loss: 0.422982 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4320\n",
            "-------------------------------\n",
            "Training loss: 0.422967 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4321\n",
            "-------------------------------\n",
            "Training loss: 0.422987 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4322\n",
            "-------------------------------\n",
            "Training loss: 0.423039 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4323\n",
            "-------------------------------\n",
            "Training loss: 0.422984 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4324\n",
            "-------------------------------\n",
            "Training loss: 0.422941 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4325\n",
            "-------------------------------\n",
            "Training loss: 0.422880 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4326\n",
            "-------------------------------\n",
            "Training loss: 0.422842 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4327\n",
            "-------------------------------\n",
            "Training loss: 0.422816 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4328\n",
            "-------------------------------\n",
            "Training loss: 0.422792 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4329\n",
            "-------------------------------\n",
            "Training loss: 0.422853 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4330\n",
            "-------------------------------\n",
            "Training loss: 0.422800 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4331\n",
            "-------------------------------\n",
            "Training loss: 0.422834 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4332\n",
            "-------------------------------\n",
            "Training loss: 0.422903 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4333\n",
            "-------------------------------\n",
            "Training loss: 0.422839 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4334\n",
            "-------------------------------\n",
            "Training loss: 0.422890 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4335\n",
            "-------------------------------\n",
            "Training loss: 0.422834 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4336\n",
            "-------------------------------\n",
            "Training loss: 0.422805 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4337\n",
            "-------------------------------\n",
            "Training loss: 0.422766 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4338\n",
            "-------------------------------\n",
            "Training loss: 0.422699 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4339\n",
            "-------------------------------\n",
            "Training loss: 0.422750 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4340\n",
            "-------------------------------\n",
            "Training loss: 0.422715 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4341\n",
            "-------------------------------\n",
            "Training loss: 0.422677 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4342\n",
            "-------------------------------\n",
            "Training loss: 0.422731 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4343\n",
            "-------------------------------\n",
            "Training loss: 0.422670 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4344\n",
            "-------------------------------\n",
            "Training loss: 0.422670 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4345\n",
            "-------------------------------\n",
            "Training loss: 0.422678 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4346\n",
            "-------------------------------\n",
            "Training loss: 0.422656 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4347\n",
            "-------------------------------\n",
            "Training loss: 0.422668 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4348\n",
            "-------------------------------\n",
            "Training loss: 0.422647 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4349\n",
            "-------------------------------\n",
            "Training loss: 0.422676 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4350\n",
            "-------------------------------\n",
            "Training loss: 0.422625 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4351\n",
            "-------------------------------\n",
            "Training loss: 0.422594 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4352\n",
            "-------------------------------\n",
            "Training loss: 0.422608 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4353\n",
            "-------------------------------\n",
            "Training loss: 0.422608 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4354\n",
            "-------------------------------\n",
            "Training loss: 0.422569 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4355\n",
            "-------------------------------\n",
            "Training loss: 0.422595 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4356\n",
            "-------------------------------\n",
            "Training loss: 0.422581 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4357\n",
            "-------------------------------\n",
            "Training loss: 0.422591 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4358\n",
            "-------------------------------\n",
            "Training loss: 0.422598 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4359\n",
            "-------------------------------\n",
            "Training loss: 0.422537 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4360\n",
            "-------------------------------\n",
            "Training loss: 0.422626 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4361\n",
            "-------------------------------\n",
            "Training loss: 0.422539 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4362\n",
            "-------------------------------\n",
            "Training loss: 0.422600 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4363\n",
            "-------------------------------\n",
            "Training loss: 0.422592 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4364\n",
            "-------------------------------\n",
            "Training loss: 0.422546 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4365\n",
            "-------------------------------\n",
            "Training loss: 0.422599 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4366\n",
            "-------------------------------\n",
            "Training loss: 0.422535 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4367\n",
            "-------------------------------\n",
            "Training loss: 0.422562 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4368\n",
            "-------------------------------\n",
            "Training loss: 0.422556 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4369\n",
            "-------------------------------\n",
            "Training loss: 0.422510 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4370\n",
            "-------------------------------\n",
            "Training loss: 0.422533 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4371\n",
            "-------------------------------\n",
            "Training loss: 0.422509 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4372\n",
            "-------------------------------\n",
            "Training loss: 0.422487 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4373\n",
            "-------------------------------\n",
            "Training loss: 0.422522 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4374\n",
            "-------------------------------\n",
            "Training loss: 0.422434 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4375\n",
            "-------------------------------\n",
            "Training loss: 0.422525 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4376\n",
            "-------------------------------\n",
            "Training loss: 0.422535 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4377\n",
            "-------------------------------\n",
            "Training loss: 0.422430 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4378\n",
            "-------------------------------\n",
            "Training loss: 0.422466 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4379\n",
            "-------------------------------\n",
            "Training loss: 0.422408 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4380\n",
            "-------------------------------\n",
            "Training loss: 0.422480 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4381\n",
            "-------------------------------\n",
            "Training loss: 0.422492 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4382\n",
            "-------------------------------\n",
            "Training loss: 0.422531 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4383\n",
            "-------------------------------\n",
            "Training loss: 0.422627 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4384\n",
            "-------------------------------\n",
            "Training loss: 0.422646 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4385\n",
            "-------------------------------\n",
            "Training loss: 0.422651 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4386\n",
            "-------------------------------\n",
            "Training loss: 0.422606 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4387\n",
            "-------------------------------\n",
            "Training loss: 0.422509 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4388\n",
            "-------------------------------\n",
            "Training loss: 0.422394 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4389\n",
            "-------------------------------\n",
            "Training loss: 0.422333 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4390\n",
            "-------------------------------\n",
            "Training loss: 0.422312 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4391\n",
            "-------------------------------\n",
            "Training loss: 0.422313 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4392\n",
            "-------------------------------\n",
            "Training loss: 0.422298 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4393\n",
            "-------------------------------\n",
            "Training loss: 0.422299 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4394\n",
            "-------------------------------\n",
            "Training loss: 0.422288 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4395\n",
            "-------------------------------\n",
            "Training loss: 0.422276 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4396\n",
            "-------------------------------\n",
            "Training loss: 0.422304 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4397\n",
            "-------------------------------\n",
            "Training loss: 0.422274 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4398\n",
            "-------------------------------\n",
            "Training loss: 0.422286 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4399\n",
            "-------------------------------\n",
            "Training loss: 0.422277 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4400\n",
            "-------------------------------\n",
            "Training loss: 0.422248 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4401\n",
            "-------------------------------\n",
            "Training loss: 0.422247 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4402\n",
            "-------------------------------\n",
            "Training loss: 0.422243 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4403\n",
            "-------------------------------\n",
            "Training loss: 0.422228 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4404\n",
            "-------------------------------\n",
            "Training loss: 0.422209 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4405\n",
            "-------------------------------\n",
            "Training loss: 0.422229 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4406\n",
            "-------------------------------\n",
            "Training loss: 0.422203 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4407\n",
            "-------------------------------\n",
            "Training loss: 0.422230 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4408\n",
            "-------------------------------\n",
            "Training loss: 0.422223 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4409\n",
            "-------------------------------\n",
            "Training loss: 0.422211 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4410\n",
            "-------------------------------\n",
            "Training loss: 0.422220 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4411\n",
            "-------------------------------\n",
            "Training loss: 0.422207 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4412\n",
            "-------------------------------\n",
            "Training loss: 0.422205 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4413\n",
            "-------------------------------\n",
            "Training loss: 0.422172 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4414\n",
            "-------------------------------\n",
            "Training loss: 0.422201 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4415\n",
            "-------------------------------\n",
            "Training loss: 0.422171 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4416\n",
            "-------------------------------\n",
            "Training loss: 0.422157 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4417\n",
            "-------------------------------\n",
            "Training loss: 0.422183 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4418\n",
            "-------------------------------\n",
            "Training loss: 0.422162 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4419\n",
            "-------------------------------\n",
            "Training loss: 0.422183 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4420\n",
            "-------------------------------\n",
            "Training loss: 0.422183 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4421\n",
            "-------------------------------\n",
            "Training loss: 0.422202 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4422\n",
            "-------------------------------\n",
            "Training loss: 0.422190 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4423\n",
            "-------------------------------\n",
            "Training loss: 0.422218 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4424\n",
            "-------------------------------\n",
            "Training loss: 0.422172 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4425\n",
            "-------------------------------\n",
            "Training loss: 0.422154 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4426\n",
            "-------------------------------\n",
            "Training loss: 0.422145 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4427\n",
            "-------------------------------\n",
            "Training loss: 0.422128 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4428\n",
            "-------------------------------\n",
            "Training loss: 0.422102 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4429\n",
            "-------------------------------\n",
            "Training loss: 0.422048 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4430\n",
            "-------------------------------\n",
            "Training loss: 0.422015 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4431\n",
            "-------------------------------\n",
            "Training loss: 0.422011 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4432\n",
            "-------------------------------\n",
            "Training loss: 0.422091 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4433\n",
            "-------------------------------\n",
            "Training loss: 0.422178 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4434\n",
            "-------------------------------\n",
            "Training loss: 0.422253 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4435\n",
            "-------------------------------\n",
            "Training loss: 0.422294 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4436\n",
            "-------------------------------\n",
            "Training loss: 0.422333 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4437\n",
            "-------------------------------\n",
            "Training loss: 0.422238 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4438\n",
            "-------------------------------\n",
            "Training loss: 0.422077 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4439\n",
            "-------------------------------\n",
            "Training loss: 0.421958 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4440\n",
            "-------------------------------\n",
            "Training loss: 0.421941 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4441\n",
            "-------------------------------\n",
            "Training loss: 0.421943 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4442\n",
            "-------------------------------\n",
            "Training loss: 0.421949 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4443\n",
            "-------------------------------\n",
            "Training loss: 0.421921 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4444\n",
            "-------------------------------\n",
            "Training loss: 0.421915 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4445\n",
            "-------------------------------\n",
            "Training loss: 0.421870 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4446\n",
            "-------------------------------\n",
            "Training loss: 0.421842 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4447\n",
            "-------------------------------\n",
            "Training loss: 0.421833 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4448\n",
            "-------------------------------\n",
            "Training loss: 0.421818 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4449\n",
            "-------------------------------\n",
            "Training loss: 0.421821 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4450\n",
            "-------------------------------\n",
            "Training loss: 0.421805 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4451\n",
            "-------------------------------\n",
            "Training loss: 0.421817 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4452\n",
            "-------------------------------\n",
            "Training loss: 0.421849 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4453\n",
            "-------------------------------\n",
            "Training loss: 0.421927 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4454\n",
            "-------------------------------\n",
            "Training loss: 0.421981 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4455\n",
            "-------------------------------\n",
            "Training loss: 0.421937 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4456\n",
            "-------------------------------\n",
            "Training loss: 0.421955 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4457\n",
            "-------------------------------\n",
            "Training loss: 0.421829 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4458\n",
            "-------------------------------\n",
            "Training loss: 0.421772 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4459\n",
            "-------------------------------\n",
            "Training loss: 0.421762 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4460\n",
            "-------------------------------\n",
            "Training loss: 0.421828 \n",
            " Training accuracy: 0.800000\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4461\n",
            "-------------------------------\n",
            "Training loss: 0.421915 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4462\n",
            "-------------------------------\n",
            "Training loss: 0.421988 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4463\n",
            "-------------------------------\n",
            "Training loss: 0.421993 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4464\n",
            "-------------------------------\n",
            "Training loss: 0.421888 \n",
            " Training accuracy: 0.801124\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4465\n",
            "-------------------------------\n",
            "Training loss: 0.421818 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4466\n",
            "-------------------------------\n",
            "Training loss: 0.421716 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4467\n",
            "-------------------------------\n",
            "Training loss: 0.421689 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4468\n",
            "-------------------------------\n",
            "Training loss: 0.421729 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4469\n",
            "-------------------------------\n",
            "Training loss: 0.421757 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4470\n",
            "-------------------------------\n",
            "Training loss: 0.421757 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4471\n",
            "-------------------------------\n",
            "Training loss: 0.421758 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4472\n",
            "-------------------------------\n",
            "Training loss: 0.421755 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4473\n",
            "-------------------------------\n",
            "Training loss: 0.421699 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4474\n",
            "-------------------------------\n",
            "Training loss: 0.421672 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4475\n",
            "-------------------------------\n",
            "Training loss: 0.421643 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4476\n",
            "-------------------------------\n",
            "Training loss: 0.421649 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4477\n",
            "-------------------------------\n",
            "Training loss: 0.421665 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4478\n",
            "-------------------------------\n",
            "Training loss: 0.421625 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4479\n",
            "-------------------------------\n",
            "Training loss: 0.421644 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4480\n",
            "-------------------------------\n",
            "Training loss: 0.421577 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4481\n",
            "-------------------------------\n",
            "Training loss: 0.421613 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4482\n",
            "-------------------------------\n",
            "Training loss: 0.421584 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4483\n",
            "-------------------------------\n",
            "Training loss: 0.421576 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4484\n",
            "-------------------------------\n",
            "Training loss: 0.421591 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4485\n",
            "-------------------------------\n",
            "Training loss: 0.421560 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4486\n",
            "-------------------------------\n",
            "Training loss: 0.421568 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4487\n",
            "-------------------------------\n",
            "Training loss: 0.421554 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4488\n",
            "-------------------------------\n",
            "Training loss: 0.421564 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4489\n",
            "-------------------------------\n",
            "Training loss: 0.421571 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4490\n",
            "-------------------------------\n",
            "Training loss: 0.421598 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4491\n",
            "-------------------------------\n",
            "Training loss: 0.421602 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4492\n",
            "-------------------------------\n",
            "Training loss: 0.421621 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4493\n",
            "-------------------------------\n",
            "Training loss: 0.421627 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4494\n",
            "-------------------------------\n",
            "Training loss: 0.421657 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4495\n",
            "-------------------------------\n",
            "Training loss: 0.421660 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4496\n",
            "-------------------------------\n",
            "Training loss: 0.421596 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4497\n",
            "-------------------------------\n",
            "Training loss: 0.421564 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4498\n",
            "-------------------------------\n",
            "Training loss: 0.421523 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4499\n",
            "-------------------------------\n",
            "Training loss: 0.421460 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4500\n",
            "-------------------------------\n",
            "Training loss: 0.421450 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4501\n",
            "-------------------------------\n",
            "Training loss: 0.421420 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4502\n",
            "-------------------------------\n",
            "Training loss: 0.421445 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4503\n",
            "-------------------------------\n",
            "Training loss: 0.421455 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4504\n",
            "-------------------------------\n",
            "Training loss: 0.421442 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4505\n",
            "-------------------------------\n",
            "Training loss: 0.421496 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4506\n",
            "-------------------------------\n",
            "Training loss: 0.421544 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4507\n",
            "-------------------------------\n",
            "Training loss: 0.421553 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4508\n",
            "-------------------------------\n",
            "Training loss: 0.421530 \n",
            " Training accuracy: 0.802247\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4509\n",
            "-------------------------------\n",
            "Training loss: 0.421485 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4510\n",
            "-------------------------------\n",
            "Training loss: 0.421515 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4511\n",
            "-------------------------------\n",
            "Training loss: 0.421510 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4512\n",
            "-------------------------------\n",
            "Training loss: 0.421509 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4513\n",
            "-------------------------------\n",
            "Training loss: 0.421571 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4514\n",
            "-------------------------------\n",
            "Training loss: 0.421500 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4515\n",
            "-------------------------------\n",
            "Training loss: 0.421501 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4516\n",
            "-------------------------------\n",
            "Training loss: 0.421401 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4517\n",
            "-------------------------------\n",
            "Training loss: 0.421386 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4518\n",
            "-------------------------------\n",
            "Training loss: 0.421333 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4519\n",
            "-------------------------------\n",
            "Training loss: 0.421334 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4520\n",
            "-------------------------------\n",
            "Training loss: 0.421303 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4521\n",
            "-------------------------------\n",
            "Training loss: 0.421294 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4522\n",
            "-------------------------------\n",
            "Training loss: 0.421330 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4523\n",
            "-------------------------------\n",
            "Training loss: 0.421318 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4524\n",
            "-------------------------------\n",
            "Training loss: 0.421362 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4525\n",
            "-------------------------------\n",
            "Training loss: 0.421409 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4526\n",
            "-------------------------------\n",
            "Training loss: 0.421459 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4527\n",
            "-------------------------------\n",
            "Training loss: 0.421511 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4528\n",
            "-------------------------------\n",
            "Training loss: 0.421443 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4529\n",
            "-------------------------------\n",
            "Training loss: 0.421437 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4530\n",
            "-------------------------------\n",
            "Training loss: 0.421372 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4531\n",
            "-------------------------------\n",
            "Training loss: 0.421275 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4532\n",
            "-------------------------------\n",
            "Training loss: 0.421251 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4533\n",
            "-------------------------------\n",
            "Training loss: 0.421217 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4534\n",
            "-------------------------------\n",
            "Training loss: 0.421235 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4535\n",
            "-------------------------------\n",
            "Training loss: 0.421270 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4536\n",
            "-------------------------------\n",
            "Training loss: 0.421261 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4537\n",
            "-------------------------------\n",
            "Training loss: 0.421300 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4538\n",
            "-------------------------------\n",
            "Training loss: 0.421312 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4539\n",
            "-------------------------------\n",
            "Training loss: 0.421368 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4540\n",
            "-------------------------------\n",
            "Training loss: 0.421372 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4541\n",
            "-------------------------------\n",
            "Training loss: 0.421347 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4542\n",
            "-------------------------------\n",
            "Training loss: 0.421307 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4543\n",
            "-------------------------------\n",
            "Training loss: 0.421225 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4544\n",
            "-------------------------------\n",
            "Training loss: 0.421164 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4545\n",
            "-------------------------------\n",
            "Training loss: 0.421142 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4546\n",
            "-------------------------------\n",
            "Training loss: 0.421141 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4547\n",
            "-------------------------------\n",
            "Training loss: 0.421209 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4548\n",
            "-------------------------------\n",
            "Training loss: 0.421268 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4549\n",
            "-------------------------------\n",
            "Training loss: 0.421286 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4550\n",
            "-------------------------------\n",
            "Training loss: 0.421317 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4551\n",
            "-------------------------------\n",
            "Training loss: 0.421247 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4552\n",
            "-------------------------------\n",
            "Training loss: 0.421166 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4553\n",
            "-------------------------------\n",
            "Training loss: 0.421069 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4554\n",
            "-------------------------------\n",
            "Training loss: 0.421080 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4555\n",
            "-------------------------------\n",
            "Training loss: 0.421073 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4556\n",
            "-------------------------------\n",
            "Training loss: 0.421113 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4557\n",
            "-------------------------------\n",
            "Training loss: 0.421175 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4558\n",
            "-------------------------------\n",
            "Training loss: 0.421174 \n",
            " Training accuracy: 0.803371\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4559\n",
            "-------------------------------\n",
            "Training loss: 0.421181 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4560\n",
            "-------------------------------\n",
            "Training loss: 0.421123 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4561\n",
            "-------------------------------\n",
            "Training loss: 0.421077 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4562\n",
            "-------------------------------\n",
            "Training loss: 0.421018 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4563\n",
            "-------------------------------\n",
            "Training loss: 0.421001 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4564\n",
            "-------------------------------\n",
            "Training loss: 0.420969 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4565\n",
            "-------------------------------\n",
            "Training loss: 0.420985 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4566\n",
            "-------------------------------\n",
            "Training loss: 0.421004 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4567\n",
            "-------------------------------\n",
            "Training loss: 0.421031 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4568\n",
            "-------------------------------\n",
            "Training loss: 0.421039 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4569\n",
            "-------------------------------\n",
            "Training loss: 0.421089 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4570\n",
            "-------------------------------\n",
            "Training loss: 0.421083 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4571\n",
            "-------------------------------\n",
            "Training loss: 0.421124 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4572\n",
            "-------------------------------\n",
            "Training loss: 0.421078 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4573\n",
            "-------------------------------\n",
            "Training loss: 0.421013 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4574\n",
            "-------------------------------\n",
            "Training loss: 0.420976 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4575\n",
            "-------------------------------\n",
            "Training loss: 0.420916 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4576\n",
            "-------------------------------\n",
            "Training loss: 0.420886 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4577\n",
            "-------------------------------\n",
            "Training loss: 0.420873 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4578\n",
            "-------------------------------\n",
            "Training loss: 0.420856 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4579\n",
            "-------------------------------\n",
            "Training loss: 0.420881 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4580\n",
            "-------------------------------\n",
            "Training loss: 0.420905 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4581\n",
            "-------------------------------\n",
            "Training loss: 0.420952 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4582\n",
            "-------------------------------\n",
            "Training loss: 0.420947 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4583\n",
            "-------------------------------\n",
            "Training loss: 0.420928 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4584\n",
            "-------------------------------\n",
            "Training loss: 0.420902 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4585\n",
            "-------------------------------\n",
            "Training loss: 0.420861 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4586\n",
            "-------------------------------\n",
            "Training loss: 0.420841 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4587\n",
            "-------------------------------\n",
            "Training loss: 0.420809 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4588\n",
            "-------------------------------\n",
            "Training loss: 0.420778 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4589\n",
            "-------------------------------\n",
            "Training loss: 0.420784 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4590\n",
            "-------------------------------\n",
            "Training loss: 0.420772 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4591\n",
            "-------------------------------\n",
            "Training loss: 0.420759 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4592\n",
            "-------------------------------\n",
            "Training loss: 0.420741 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4593\n",
            "-------------------------------\n",
            "Training loss: 0.420747 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4594\n",
            "-------------------------------\n",
            "Training loss: 0.420721 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4595\n",
            "-------------------------------\n",
            "Training loss: 0.420765 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4596\n",
            "-------------------------------\n",
            "Training loss: 0.420717 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4597\n",
            "-------------------------------\n",
            "Training loss: 0.420749 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4598\n",
            "-------------------------------\n",
            "Training loss: 0.420761 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4599\n",
            "-------------------------------\n",
            "Training loss: 0.420711 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4600\n",
            "-------------------------------\n",
            "Training loss: 0.420794 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4601\n",
            "-------------------------------\n",
            "Training loss: 0.420787 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4602\n",
            "-------------------------------\n",
            "Training loss: 0.420805 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4603\n",
            "-------------------------------\n",
            "Training loss: 0.420806 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4604\n",
            "-------------------------------\n",
            "Training loss: 0.420741 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4605\n",
            "-------------------------------\n",
            "Training loss: 0.420708 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4606\n",
            "-------------------------------\n",
            "Training loss: 0.420649 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4607\n",
            "-------------------------------\n",
            "Training loss: 0.420650 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4608\n",
            "-------------------------------\n",
            "Training loss: 0.420652 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4609\n",
            "-------------------------------\n",
            "Training loss: 0.420637 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4610\n",
            "-------------------------------\n",
            "Training loss: 0.420668 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4611\n",
            "-------------------------------\n",
            "Training loss: 0.420598 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4612\n",
            "-------------------------------\n",
            "Training loss: 0.420621 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4613\n",
            "-------------------------------\n",
            "Training loss: 0.420619 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4614\n",
            "-------------------------------\n",
            "Training loss: 0.420576 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4615\n",
            "-------------------------------\n",
            "Training loss: 0.420591 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4616\n",
            "-------------------------------\n",
            "Training loss: 0.420555 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4617\n",
            "-------------------------------\n",
            "Training loss: 0.420571 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4618\n",
            "-------------------------------\n",
            "Training loss: 0.420579 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4619\n",
            "-------------------------------\n",
            "Training loss: 0.420536 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4620\n",
            "-------------------------------\n",
            "Training loss: 0.420558 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4621\n",
            "-------------------------------\n",
            "Training loss: 0.420526 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4622\n",
            "-------------------------------\n",
            "Training loss: 0.420543 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4623\n",
            "-------------------------------\n",
            "Training loss: 0.420567 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4624\n",
            "-------------------------------\n",
            "Training loss: 0.420514 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4625\n",
            "-------------------------------\n",
            "Training loss: 0.420510 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4626\n",
            "-------------------------------\n",
            "Training loss: 0.420507 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4627\n",
            "-------------------------------\n",
            "Training loss: 0.420524 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4628\n",
            "-------------------------------\n",
            "Training loss: 0.420520 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4629\n",
            "-------------------------------\n",
            "Training loss: 0.420510 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4630\n",
            "-------------------------------\n",
            "Training loss: 0.420509 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4631\n",
            "-------------------------------\n",
            "Training loss: 0.420525 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4632\n",
            "-------------------------------\n",
            "Training loss: 0.420513 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4633\n",
            "-------------------------------\n",
            "Training loss: 0.420513 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4634\n",
            "-------------------------------\n",
            "Training loss: 0.420455 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4635\n",
            "-------------------------------\n",
            "Training loss: 0.420439 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4636\n",
            "-------------------------------\n",
            "Training loss: 0.420415 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4637\n",
            "-------------------------------\n",
            "Training loss: 0.420405 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4638\n",
            "-------------------------------\n",
            "Training loss: 0.420399 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4639\n",
            "-------------------------------\n",
            "Training loss: 0.420403 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4640\n",
            "-------------------------------\n",
            "Training loss: 0.420352 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4641\n",
            "-------------------------------\n",
            "Training loss: 0.420349 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4642\n",
            "-------------------------------\n",
            "Training loss: 0.420346 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4643\n",
            "-------------------------------\n",
            "Training loss: 0.420337 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4644\n",
            "-------------------------------\n",
            "Training loss: 0.420334 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4645\n",
            "-------------------------------\n",
            "Training loss: 0.420330 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4646\n",
            "-------------------------------\n",
            "Training loss: 0.420318 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4647\n",
            "-------------------------------\n",
            "Training loss: 0.420348 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4648\n",
            "-------------------------------\n",
            "Training loss: 0.420383 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4649\n",
            "-------------------------------\n",
            "Training loss: 0.420366 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4650\n",
            "-------------------------------\n",
            "Training loss: 0.420356 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4651\n",
            "-------------------------------\n",
            "Training loss: 0.420381 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4652\n",
            "-------------------------------\n",
            "Training loss: 0.420330 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4653\n",
            "-------------------------------\n",
            "Training loss: 0.420323 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4654\n",
            "-------------------------------\n",
            "Training loss: 0.420301 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4655\n",
            "-------------------------------\n",
            "Training loss: 0.420286 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4656\n",
            "-------------------------------\n",
            "Training loss: 0.420314 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4657\n",
            "-------------------------------\n",
            "Training loss: 0.420315 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4658\n",
            "-------------------------------\n",
            "Training loss: 0.420302 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4659\n",
            "-------------------------------\n",
            "Training loss: 0.420259 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4660\n",
            "-------------------------------\n",
            "Training loss: 0.420217 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4661\n",
            "-------------------------------\n",
            "Training loss: 0.420181 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4662\n",
            "-------------------------------\n",
            "Training loss: 0.420164 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4663\n",
            "-------------------------------\n",
            "Training loss: 0.420173 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4664\n",
            "-------------------------------\n",
            "Training loss: 0.420169 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4665\n",
            "-------------------------------\n",
            "Training loss: 0.420201 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4666\n",
            "-------------------------------\n",
            "Training loss: 0.420187 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4667\n",
            "-------------------------------\n",
            "Training loss: 0.420226 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4668\n",
            "-------------------------------\n",
            "Training loss: 0.420257 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4669\n",
            "-------------------------------\n",
            "Training loss: 0.420235 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4670\n",
            "-------------------------------\n",
            "Training loss: 0.420238 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4671\n",
            "-------------------------------\n",
            "Training loss: 0.420225 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4672\n",
            "-------------------------------\n",
            "Training loss: 0.420209 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4673\n",
            "-------------------------------\n",
            "Training loss: 0.420180 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4674\n",
            "-------------------------------\n",
            "Training loss: 0.420191 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4675\n",
            "-------------------------------\n",
            "Training loss: 0.420127 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4676\n",
            "-------------------------------\n",
            "Training loss: 0.420117 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4677\n",
            "-------------------------------\n",
            "Training loss: 0.420081 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4678\n",
            "-------------------------------\n",
            "Training loss: 0.420097 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4679\n",
            "-------------------------------\n",
            "Training loss: 0.420067 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4680\n",
            "-------------------------------\n",
            "Training loss: 0.420055 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4681\n",
            "-------------------------------\n",
            "Training loss: 0.420029 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4682\n",
            "-------------------------------\n",
            "Training loss: 0.420023 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4683\n",
            "-------------------------------\n",
            "Training loss: 0.420012 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4684\n",
            "-------------------------------\n",
            "Training loss: 0.420021 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4685\n",
            "-------------------------------\n",
            "Training loss: 0.420075 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4686\n",
            "-------------------------------\n",
            "Training loss: 0.420079 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4687\n",
            "-------------------------------\n",
            "Training loss: 0.420140 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4688\n",
            "-------------------------------\n",
            "Training loss: 0.420086 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4689\n",
            "-------------------------------\n",
            "Training loss: 0.420113 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4690\n",
            "-------------------------------\n",
            "Training loss: 0.420082 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4691\n",
            "-------------------------------\n",
            "Training loss: 0.420004 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4692\n",
            "-------------------------------\n",
            "Training loss: 0.419995 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4693\n",
            "-------------------------------\n",
            "Training loss: 0.419933 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4694\n",
            "-------------------------------\n",
            "Training loss: 0.419924 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4695\n",
            "-------------------------------\n",
            "Training loss: 0.419927 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4696\n",
            "-------------------------------\n",
            "Training loss: 0.419906 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4697\n",
            "-------------------------------\n",
            "Training loss: 0.419924 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4698\n",
            "-------------------------------\n",
            "Training loss: 0.419888 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4699\n",
            "-------------------------------\n",
            "Training loss: 0.419907 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4700\n",
            "-------------------------------\n",
            "Training loss: 0.419892 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4701\n",
            "-------------------------------\n",
            "Training loss: 0.419875 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4702\n",
            "-------------------------------\n",
            "Training loss: 0.419913 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4703\n",
            "-------------------------------\n",
            "Training loss: 0.419905 \n",
            " Training accuracy: 0.804494\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4704\n",
            "-------------------------------\n",
            "Training loss: 0.419946 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4705\n",
            "-------------------------------\n",
            "Training loss: 0.419967 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4706\n",
            "-------------------------------\n",
            "Training loss: 0.419960 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4707\n",
            "-------------------------------\n",
            "Training loss: 0.419981 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4708\n",
            "-------------------------------\n",
            "Training loss: 0.419969 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4709\n",
            "-------------------------------\n",
            "Training loss: 0.419995 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4710\n",
            "-------------------------------\n",
            "Training loss: 0.419952 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4711\n",
            "-------------------------------\n",
            "Training loss: 0.419938 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4712\n",
            "-------------------------------\n",
            "Training loss: 0.419875 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4713\n",
            "-------------------------------\n",
            "Training loss: 0.419847 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4714\n",
            "-------------------------------\n",
            "Training loss: 0.419804 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4715\n",
            "-------------------------------\n",
            "Training loss: 0.419785 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4716\n",
            "-------------------------------\n",
            "Training loss: 0.419774 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4717\n",
            "-------------------------------\n",
            "Training loss: 0.419765 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4718\n",
            "-------------------------------\n",
            "Training loss: 0.419751 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4719\n",
            "-------------------------------\n",
            "Training loss: 0.419763 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4720\n",
            "-------------------------------\n",
            "Training loss: 0.419762 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4721\n",
            "-------------------------------\n",
            "Training loss: 0.419788 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4722\n",
            "-------------------------------\n",
            "Training loss: 0.419811 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4723\n",
            "-------------------------------\n",
            "Training loss: 0.419829 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4724\n",
            "-------------------------------\n",
            "Training loss: 0.419874 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4725\n",
            "-------------------------------\n",
            "Training loss: 0.419870 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4726\n",
            "-------------------------------\n",
            "Training loss: 0.419871 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4727\n",
            "-------------------------------\n",
            "Training loss: 0.419800 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4728\n",
            "-------------------------------\n",
            "Training loss: 0.419728 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4729\n",
            "-------------------------------\n",
            "Training loss: 0.419681 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4730\n",
            "-------------------------------\n",
            "Training loss: 0.419670 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4731\n",
            "-------------------------------\n",
            "Training loss: 0.419672 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4732\n",
            "-------------------------------\n",
            "Training loss: 0.419667 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4733\n",
            "-------------------------------\n",
            "Training loss: 0.419662 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4734\n",
            "-------------------------------\n",
            "Training loss: 0.419670 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4735\n",
            "-------------------------------\n",
            "Training loss: 0.419668 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4736\n",
            "-------------------------------\n",
            "Training loss: 0.419680 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4737\n",
            "-------------------------------\n",
            "Training loss: 0.419668 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4738\n",
            "-------------------------------\n",
            "Training loss: 0.419674 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4739\n",
            "-------------------------------\n",
            "Training loss: 0.419687 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4740\n",
            "-------------------------------\n",
            "Training loss: 0.419698 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4741\n",
            "-------------------------------\n",
            "Training loss: 0.419724 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4742\n",
            "-------------------------------\n",
            "Training loss: 0.419747 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4743\n",
            "-------------------------------\n",
            "Training loss: 0.419766 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4744\n",
            "-------------------------------\n",
            "Training loss: 0.419749 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4745\n",
            "-------------------------------\n",
            "Training loss: 0.419750 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4746\n",
            "-------------------------------\n",
            "Training loss: 0.419718 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4747\n",
            "-------------------------------\n",
            "Training loss: 0.419698 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4748\n",
            "-------------------------------\n",
            "Training loss: 0.419683 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4749\n",
            "-------------------------------\n",
            "Training loss: 0.419643 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4750\n",
            "-------------------------------\n",
            "Training loss: 0.419590 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4751\n",
            "-------------------------------\n",
            "Training loss: 0.419585 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4752\n",
            "-------------------------------\n",
            "Training loss: 0.419533 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4753\n",
            "-------------------------------\n",
            "Training loss: 0.419502 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4754\n",
            "-------------------------------\n",
            "Training loss: 0.419519 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4755\n",
            "-------------------------------\n",
            "Training loss: 0.419505 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4756\n",
            "-------------------------------\n",
            "Training loss: 0.419524 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4757\n",
            "-------------------------------\n",
            "Training loss: 0.419504 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4758\n",
            "-------------------------------\n",
            "Training loss: 0.419491 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4759\n",
            "-------------------------------\n",
            "Training loss: 0.419508 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4760\n",
            "-------------------------------\n",
            "Training loss: 0.419488 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4761\n",
            "-------------------------------\n",
            "Training loss: 0.419499 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4762\n",
            "-------------------------------\n",
            "Training loss: 0.419462 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4763\n",
            "-------------------------------\n",
            "Training loss: 0.419462 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4764\n",
            "-------------------------------\n",
            "Training loss: 0.419439 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4765\n",
            "-------------------------------\n",
            "Training loss: 0.419432 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4766\n",
            "-------------------------------\n",
            "Training loss: 0.419417 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4767\n",
            "-------------------------------\n",
            "Training loss: 0.419403 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4768\n",
            "-------------------------------\n",
            "Training loss: 0.419399 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4769\n",
            "-------------------------------\n",
            "Training loss: 0.419409 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4770\n",
            "-------------------------------\n",
            "Training loss: 0.419411 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4771\n",
            "-------------------------------\n",
            "Training loss: 0.419414 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4772\n",
            "-------------------------------\n",
            "Training loss: 0.419406 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4773\n",
            "-------------------------------\n",
            "Training loss: 0.419450 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4774\n",
            "-------------------------------\n",
            "Training loss: 0.419427 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4775\n",
            "-------------------------------\n",
            "Training loss: 0.419452 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4776\n",
            "-------------------------------\n",
            "Training loss: 0.419463 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4777\n",
            "-------------------------------\n",
            "Training loss: 0.419489 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4778\n",
            "-------------------------------\n",
            "Training loss: 0.419495 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4779\n",
            "-------------------------------\n",
            "Training loss: 0.419453 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4780\n",
            "-------------------------------\n",
            "Training loss: 0.419433 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4781\n",
            "-------------------------------\n",
            "Training loss: 0.419391 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4782\n",
            "-------------------------------\n",
            "Training loss: 0.419322 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4783\n",
            "-------------------------------\n",
            "Training loss: 0.419308 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4784\n",
            "-------------------------------\n",
            "Training loss: 0.419313 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4785\n",
            "-------------------------------\n",
            "Training loss: 0.419290 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4786\n",
            "-------------------------------\n",
            "Training loss: 0.419328 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4787\n",
            "-------------------------------\n",
            "Training loss: 0.419314 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4788\n",
            "-------------------------------\n",
            "Training loss: 0.419286 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4789\n",
            "-------------------------------\n",
            "Training loss: 0.419310 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4790\n",
            "-------------------------------\n",
            "Training loss: 0.419260 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4791\n",
            "-------------------------------\n",
            "Training loss: 0.419261 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4792\n",
            "-------------------------------\n",
            "Training loss: 0.419268 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4793\n",
            "-------------------------------\n",
            "Training loss: 0.419258 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4794\n",
            "-------------------------------\n",
            "Training loss: 0.419241 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4795\n",
            "-------------------------------\n",
            "Training loss: 0.419261 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4796\n",
            "-------------------------------\n",
            "Training loss: 0.419243 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4797\n",
            "-------------------------------\n",
            "Training loss: 0.419261 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4798\n",
            "-------------------------------\n",
            "Training loss: 0.419318 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4799\n",
            "-------------------------------\n",
            "Training loss: 0.419357 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4800\n",
            "-------------------------------\n",
            "Training loss: 0.419408 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4801\n",
            "-------------------------------\n",
            "Training loss: 0.419415 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4802\n",
            "-------------------------------\n",
            "Training loss: 0.419418 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4803\n",
            "-------------------------------\n",
            "Training loss: 0.419386 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4804\n",
            "-------------------------------\n",
            "Training loss: 0.419355 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4805\n",
            "-------------------------------\n",
            "Training loss: 0.419288 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4806\n",
            "-------------------------------\n",
            "Training loss: 0.419230 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4807\n",
            "-------------------------------\n",
            "Training loss: 0.419198 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4808\n",
            "-------------------------------\n",
            "Training loss: 0.419137 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4809\n",
            "-------------------------------\n",
            "Training loss: 0.419175 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4810\n",
            "-------------------------------\n",
            "Training loss: 0.419241 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4811\n",
            "-------------------------------\n",
            "Training loss: 0.419229 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4812\n",
            "-------------------------------\n",
            "Training loss: 0.419268 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4813\n",
            "-------------------------------\n",
            "Training loss: 0.419246 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4814\n",
            "-------------------------------\n",
            "Training loss: 0.419231 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4815\n",
            "-------------------------------\n",
            "Training loss: 0.419166 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4816\n",
            "-------------------------------\n",
            "Training loss: 0.419143 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4817\n",
            "-------------------------------\n",
            "Training loss: 0.419099 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4818\n",
            "-------------------------------\n",
            "Training loss: 0.419105 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4819\n",
            "-------------------------------\n",
            "Training loss: 0.419163 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4820\n",
            "-------------------------------\n",
            "Training loss: 0.419135 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4821\n",
            "-------------------------------\n",
            "Training loss: 0.419117 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4822\n",
            "-------------------------------\n",
            "Training loss: 0.419111 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4823\n",
            "-------------------------------\n",
            "Training loss: 0.419069 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4824\n",
            "-------------------------------\n",
            "Training loss: 0.419083 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4825\n",
            "-------------------------------\n",
            "Training loss: 0.419033 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4826\n",
            "-------------------------------\n",
            "Training loss: 0.419039 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4827\n",
            "-------------------------------\n",
            "Training loss: 0.419028 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4828\n",
            "-------------------------------\n",
            "Training loss: 0.419011 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4829\n",
            "-------------------------------\n",
            "Training loss: 0.419058 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4830\n",
            "-------------------------------\n",
            "Training loss: 0.419045 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4831\n",
            "-------------------------------\n",
            "Training loss: 0.419048 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4832\n",
            "-------------------------------\n",
            "Training loss: 0.419041 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4833\n",
            "-------------------------------\n",
            "Training loss: 0.418995 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4834\n",
            "-------------------------------\n",
            "Training loss: 0.418976 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4835\n",
            "-------------------------------\n",
            "Training loss: 0.418959 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4836\n",
            "-------------------------------\n",
            "Training loss: 0.418919 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4837\n",
            "-------------------------------\n",
            "Training loss: 0.418914 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4838\n",
            "-------------------------------\n",
            "Training loss: 0.418900 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4839\n",
            "-------------------------------\n",
            "Training loss: 0.418892 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4840\n",
            "-------------------------------\n",
            "Training loss: 0.418884 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4841\n",
            "-------------------------------\n",
            "Training loss: 0.418879 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4842\n",
            "-------------------------------\n",
            "Training loss: 0.418867 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4843\n",
            "-------------------------------\n",
            "Training loss: 0.418867 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4844\n",
            "-------------------------------\n",
            "Training loss: 0.418864 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4845\n",
            "-------------------------------\n",
            "Training loss: 0.418846 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4846\n",
            "-------------------------------\n",
            "Training loss: 0.418853 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4847\n",
            "-------------------------------\n",
            "Training loss: 0.418837 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4848\n",
            "-------------------------------\n",
            "Training loss: 0.418843 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4849\n",
            "-------------------------------\n",
            "Training loss: 0.418847 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4850\n",
            "-------------------------------\n",
            "Training loss: 0.418857 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4851\n",
            "-------------------------------\n",
            "Training loss: 0.418898 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4852\n",
            "-------------------------------\n",
            "Training loss: 0.418960 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4853\n",
            "-------------------------------\n",
            "Training loss: 0.419056 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4854\n",
            "-------------------------------\n",
            "Training loss: 0.419150 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4855\n",
            "-------------------------------\n",
            "Training loss: 0.419276 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4856\n",
            "-------------------------------\n",
            "Training loss: 0.419262 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4857\n",
            "-------------------------------\n",
            "Training loss: 0.419196 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4858\n",
            "-------------------------------\n",
            "Training loss: 0.418984 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4859\n",
            "-------------------------------\n",
            "Training loss: 0.418832 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4860\n",
            "-------------------------------\n",
            "Training loss: 0.418769 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4861\n",
            "-------------------------------\n",
            "Training loss: 0.418746 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4862\n",
            "-------------------------------\n",
            "Training loss: 0.418739 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4863\n",
            "-------------------------------\n",
            "Training loss: 0.418757 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4864\n",
            "-------------------------------\n",
            "Training loss: 0.418770 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4865\n",
            "-------------------------------\n",
            "Training loss: 0.418762 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4866\n",
            "-------------------------------\n",
            "Training loss: 0.418759 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4867\n",
            "-------------------------------\n",
            "Training loss: 0.418756 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4868\n",
            "-------------------------------\n",
            "Training loss: 0.418743 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4869\n",
            "-------------------------------\n",
            "Training loss: 0.418759 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4870\n",
            "-------------------------------\n",
            "Training loss: 0.418764 \n",
            " Training accuracy: 0.805618\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4871\n",
            "-------------------------------\n",
            "Training loss: 0.418810 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4872\n",
            "-------------------------------\n",
            "Training loss: 0.418844 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4873\n",
            "-------------------------------\n",
            "Training loss: 0.418879 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4874\n",
            "-------------------------------\n",
            "Training loss: 0.418906 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4875\n",
            "-------------------------------\n",
            "Training loss: 0.418861 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4876\n",
            "-------------------------------\n",
            "Training loss: 0.418815 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4877\n",
            "-------------------------------\n",
            "Training loss: 0.418769 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4878\n",
            "-------------------------------\n",
            "Training loss: 0.418708 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4879\n",
            "-------------------------------\n",
            "Training loss: 0.418680 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4880\n",
            "-------------------------------\n",
            "Training loss: 0.418677 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4881\n",
            "-------------------------------\n",
            "Training loss: 0.418654 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4882\n",
            "-------------------------------\n",
            "Training loss: 0.418636 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4883\n",
            "-------------------------------\n",
            "Training loss: 0.418611 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4884\n",
            "-------------------------------\n",
            "Training loss: 0.418599 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4885\n",
            "-------------------------------\n",
            "Training loss: 0.418597 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4886\n",
            "-------------------------------\n",
            "Training loss: 0.418595 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4887\n",
            "-------------------------------\n",
            "Training loss: 0.418594 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8378\n",
            "Epoch 4888\n",
            "-------------------------------\n",
            "Training loss: 0.418597 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4889\n",
            "-------------------------------\n",
            "Training loss: 0.418617 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4890\n",
            "-------------------------------\n",
            "Training loss: 0.418625 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4891\n",
            "-------------------------------\n",
            "Training loss: 0.418650 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4892\n",
            "-------------------------------\n",
            "Training loss: 0.418662 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4893\n",
            "-------------------------------\n",
            "Training loss: 0.418673 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4894\n",
            "-------------------------------\n",
            "Training loss: 0.418691 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4895\n",
            "-------------------------------\n",
            "Training loss: 0.418718 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4896\n",
            "-------------------------------\n",
            "Training loss: 0.418697 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4897\n",
            "-------------------------------\n",
            "Training loss: 0.418649 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4898\n",
            "-------------------------------\n",
            "Training loss: 0.418592 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4899\n",
            "-------------------------------\n",
            "Training loss: 0.418551 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4900\n",
            "-------------------------------\n",
            "Training loss: 0.418515 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4901\n",
            "-------------------------------\n",
            "Training loss: 0.418499 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4902\n",
            "-------------------------------\n",
            "Training loss: 0.418484 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4903\n",
            "-------------------------------\n",
            "Training loss: 0.418475 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4904\n",
            "-------------------------------\n",
            "Training loss: 0.418474 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4905\n",
            "-------------------------------\n",
            "Training loss: 0.418462 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4906\n",
            "-------------------------------\n",
            "Training loss: 0.418464 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4907\n",
            "-------------------------------\n",
            "Training loss: 0.418451 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4908\n",
            "-------------------------------\n",
            "Training loss: 0.418454 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4909\n",
            "-------------------------------\n",
            "Training loss: 0.418451 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4910\n",
            "-------------------------------\n",
            "Training loss: 0.418476 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4911\n",
            "-------------------------------\n",
            "Training loss: 0.418464 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4912\n",
            "-------------------------------\n",
            "Training loss: 0.418476 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4913\n",
            "-------------------------------\n",
            "Training loss: 0.418506 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4914\n",
            "-------------------------------\n",
            "Training loss: 0.418555 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4915\n",
            "-------------------------------\n",
            "Training loss: 0.418591 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4916\n",
            "-------------------------------\n",
            "Training loss: 0.418646 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4917\n",
            "-------------------------------\n",
            "Training loss: 0.418670 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4918\n",
            "-------------------------------\n",
            "Training loss: 0.418684 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4919\n",
            "-------------------------------\n",
            "Training loss: 0.418625 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4920\n",
            "-------------------------------\n",
            "Training loss: 0.418562 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4921\n",
            "-------------------------------\n",
            "Training loss: 0.418475 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4922\n",
            "-------------------------------\n",
            "Training loss: 0.418430 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4923\n",
            "-------------------------------\n",
            "Training loss: 0.418401 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4924\n",
            "-------------------------------\n",
            "Training loss: 0.418374 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4925\n",
            "-------------------------------\n",
            "Training loss: 0.418343 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4926\n",
            "-------------------------------\n",
            "Training loss: 0.418354 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4927\n",
            "-------------------------------\n",
            "Training loss: 0.418342 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4928\n",
            "-------------------------------\n",
            "Training loss: 0.418322 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4929\n",
            "-------------------------------\n",
            "Training loss: 0.418333 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4930\n",
            "-------------------------------\n",
            "Training loss: 0.418305 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4931\n",
            "-------------------------------\n",
            "Training loss: 0.418320 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4932\n",
            "-------------------------------\n",
            "Training loss: 0.418297 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4933\n",
            "-------------------------------\n",
            "Training loss: 0.418302 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4934\n",
            "-------------------------------\n",
            "Training loss: 0.418285 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4935\n",
            "-------------------------------\n",
            "Training loss: 0.418283 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4936\n",
            "-------------------------------\n",
            "Training loss: 0.418269 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4937\n",
            "-------------------------------\n",
            "Training loss: 0.418262 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4938\n",
            "-------------------------------\n",
            "Training loss: 0.418257 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4939\n",
            "-------------------------------\n",
            "Training loss: 0.418247 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4940\n",
            "-------------------------------\n",
            "Training loss: 0.418250 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4941\n",
            "-------------------------------\n",
            "Training loss: 0.418233 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4942\n",
            "-------------------------------\n",
            "Training loss: 0.418232 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4943\n",
            "-------------------------------\n",
            "Training loss: 0.418227 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4944\n",
            "-------------------------------\n",
            "Training loss: 0.418223 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4945\n",
            "-------------------------------\n",
            "Training loss: 0.418211 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4946\n",
            "-------------------------------\n",
            "Training loss: 0.418203 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4947\n",
            "-------------------------------\n",
            "Training loss: 0.418196 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4948\n",
            "-------------------------------\n",
            "Training loss: 0.418206 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4949\n",
            "-------------------------------\n",
            "Training loss: 0.418199 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4950\n",
            "-------------------------------\n",
            "Training loss: 0.418205 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4951\n",
            "-------------------------------\n",
            "Training loss: 0.418202 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4952\n",
            "-------------------------------\n",
            "Training loss: 0.418250 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4953\n",
            "-------------------------------\n",
            "Training loss: 0.418237 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4954\n",
            "-------------------------------\n",
            "Training loss: 0.418295 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4955\n",
            "-------------------------------\n",
            "Training loss: 0.418291 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4956\n",
            "-------------------------------\n",
            "Training loss: 0.418297 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4957\n",
            "-------------------------------\n",
            "Training loss: 0.418339 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4958\n",
            "-------------------------------\n",
            "Training loss: 0.418328 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4959\n",
            "-------------------------------\n",
            "Training loss: 0.418377 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4960\n",
            "-------------------------------\n",
            "Training loss: 0.418358 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4961\n",
            "-------------------------------\n",
            "Training loss: 0.418322 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4962\n",
            "-------------------------------\n",
            "Training loss: 0.418276 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4963\n",
            "-------------------------------\n",
            "Training loss: 0.418206 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4964\n",
            "-------------------------------\n",
            "Training loss: 0.418172 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4965\n",
            "-------------------------------\n",
            "Training loss: 0.418138 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4966\n",
            "-------------------------------\n",
            "Training loss: 0.418137 \n",
            " Training accuracy: 0.810112\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4967\n",
            "-------------------------------\n",
            "Training loss: 0.418094 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4968\n",
            "-------------------------------\n",
            "Training loss: 0.418089 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4969\n",
            "-------------------------------\n",
            "Training loss: 0.418058 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4970\n",
            "-------------------------------\n",
            "Training loss: 0.418062 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4971\n",
            "-------------------------------\n",
            "Training loss: 0.418043 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4972\n",
            "-------------------------------\n",
            "Training loss: 0.418037 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4973\n",
            "-------------------------------\n",
            "Training loss: 0.418045 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4974\n",
            "-------------------------------\n",
            "Training loss: 0.418031 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4975\n",
            "-------------------------------\n",
            "Training loss: 0.418070 \n",
            " Training accuracy: 0.806742\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4976\n",
            "-------------------------------\n",
            "Training loss: 0.418090 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4977\n",
            "-------------------------------\n",
            "Training loss: 0.418135 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4978\n",
            "-------------------------------\n",
            "Training loss: 0.418186 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4979\n",
            "-------------------------------\n",
            "Training loss: 0.418227 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4980\n",
            "-------------------------------\n",
            "Training loss: 0.418253 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4981\n",
            "-------------------------------\n",
            "Training loss: 0.418260 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4982\n",
            "-------------------------------\n",
            "Training loss: 0.418244 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4983\n",
            "-------------------------------\n",
            "Training loss: 0.418229 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4984\n",
            "-------------------------------\n",
            "Training loss: 0.418173 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4985\n",
            "-------------------------------\n",
            "Training loss: 0.418129 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4986\n",
            "-------------------------------\n",
            "Training loss: 0.418039 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4987\n",
            "-------------------------------\n",
            "Training loss: 0.417996 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4988\n",
            "-------------------------------\n",
            "Training loss: 0.417937 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4989\n",
            "-------------------------------\n",
            "Training loss: 0.417924 \n",
            " Training accuracy: 0.812360\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4990\n",
            "-------------------------------\n",
            "Training loss: 0.417921 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8559\n",
            "Epoch 4991\n",
            "-------------------------------\n",
            "Training loss: 0.417937 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4992\n",
            "-------------------------------\n",
            "Training loss: 0.417988 \n",
            " Training accuracy: 0.807865\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4993\n",
            "-------------------------------\n",
            "Training loss: 0.418027 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4994\n",
            "-------------------------------\n",
            "Training loss: 0.418099 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4995\n",
            "-------------------------------\n",
            "Training loss: 0.418167 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4996\n",
            "-------------------------------\n",
            "Training loss: 0.418201 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4997\n",
            "-------------------------------\n",
            "Training loss: 0.418145 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4998\n",
            "-------------------------------\n",
            "Training loss: 0.418066 \n",
            " Training accuracy: 0.811236\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 4999\n",
            "-------------------------------\n",
            "Training loss: 0.417939 \n",
            " Training accuracy: 0.813483\n",
            " Validation accuracy: 0.8468\n",
            "Epoch 5000\n",
            "-------------------------------\n",
            "Training loss: 0.417872 \n",
            " Training accuracy: 0.808989\n",
            " Validation accuracy: 0.8468\n",
            "Finished Training\n",
            "Test Accuracy: 0.7321\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# If your runtime is GPU-enabled, use .to(device) to move the model\n",
        "# and all the relevant tensors to the GPU. You have to move tensors back to CPU\n",
        "# when computing metrics like accuracy, using .cpu().\n",
        "\n",
        "# Your answer here\n",
        "\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "\n",
        "def train_graph_classifier(train_a_norm, train_features, train_batch_idx, train_labels,\n",
        "                           valid_a_norm, valid_features, valid_batch_idx, valid_labels,\n",
        "                           test_a_norm, test_features, test_batch_idx, test_labels,\n",
        "                           epochs=5000, learning_rate=1e-3, hidden_dim=32):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Model, Optimizer, Loss\n",
        "    in_features = train_features.shape[1]\n",
        "    model = GraphClassifer(in_features=in_features, n_hidden=hidden_dim, n_output=1).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Move data to device\n",
        "    train_a_norm = train_a_norm.to(device)\n",
        "    train_features = train_features.to(device)\n",
        "    train_batch_idx = train_batch_idx.to(device)\n",
        "    train_labels = train_labels.to(device)\n",
        "\n",
        "    valid_a_norm = valid_a_norm.to(device)\n",
        "    valid_features = valid_features.to(device)\n",
        "    valid_batch_idx = valid_batch_idx.to(device)\n",
        "    valid_labels = valid_labels.to(device)\n",
        "\n",
        "    test_a_norm = test_a_norm.to(device)\n",
        "    test_features = test_features.to(device)\n",
        "    test_batch_idx = test_batch_idx.to(device)\n",
        "    test_labels = test_labels.to(device)\n",
        "\n",
        "    # Store data\n",
        "    loss_train = torch.zeros(epochs)\n",
        "    loss_val = torch.zeros_like(loss_train)\n",
        "    acc_train = torch.zeros_like(loss_train)\n",
        "    acc_val = torch.zeros_like(loss_train)\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(train_features, train_a_norm, train_batch_idx)\n",
        "\n",
        "        # Loss Computation\n",
        "        loss = criterion(output, train_labels)\n",
        "\n",
        "        # Backprop\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics\n",
        "        loss_train[epoch] = loss.item()\n",
        "        predicted_train = torch.sigmoid(output)\n",
        "        acc_train[epoch] = ((predicted_train>0.5)==train_labels).float().mean()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output_val = model(valid_features, valid_a_norm, valid_batch_idx)\n",
        "            loss_val[epoch] = criterion(output_val, valid_labels).item()\n",
        "            predicted_val = torch.sigmoid(output_val)\n",
        "            acc_val[epoch] = ((predicted_val>0.5)==valid_labels).float().mean()\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        print(f\"Training loss: {loss_train[epoch]:>5f} \\n Training accuracy: {acc_train[epoch]:>5f}\\n Validation accuracy: {acc_val[epoch]:.4f}\")\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_output = model(test_features, test_a_norm, test_batch_idx)\n",
        "        test_predicted = torch.sigmoid(test_output)\n",
        "        test_acc = ((test_predicted>0.5)==test_labels).float().mean()\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    ax[0].plot(range(epochs), loss_train, label=\"Training loss\")\n",
        "    ax[0].plot(range(epochs), loss_val, label=\"Validation loss\")\n",
        "    ax[0].set_title(\"Loss per epoch\")\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Loss\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(range(epochs), acc_train, label=\"Acc. train\")\n",
        "    ax[1].plot(range(epochs), acc_val, label=\"Acc. val\")\n",
        "    ax[1].set_title(\"Accuracy per epoch\")\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Accuracy\")\n",
        "    ax[1].legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "train_graph_classifier(train_a_norm, train_features, train_batch_idx, train_labels,\n",
        "                           valid_a_norm, valid_features, valid_batch_idx, valid_labels,\n",
        "                           test_a_norm, test_features, test_batch_idx, test_labels,\n",
        "                           epochs=5000, learning_rate=1e-3, hidden_dim=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvmAfDNMcnKD"
      },
      "source": [
        "## The end\n",
        "\n",
        "If you have made it all the way here successfully, congratulations! 🎉\n",
        "\n",
        "You have implemented your own GCN and tested it on a node classification task, and a more challenging classification task over multiple graphs.\n",
        "\n",
        "We hope you can use this knowledge to apply GCNs not only to the tasks described here, but other applications where data can be modeled as a graph.\n",
        "\n",
        "If you are interested in applying graph neural networks to larger graphs, or try newer architectures, you can dive deeper into [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/), a library with fast implementations for a wide range of architectures. It also comes with custom code that takes care of aspects that you dealt with manually for this assignment, like a more efficient implementation of the adjacency matrix multiplication via message-passing methods, and Data Loaders that relieve you from having to build block diagonal sparse matrices.\n",
        "\n",
        "You can also check the [Deep Graph Library](https://docs.dgl.ai/) another powerful library for deep learning on graphs which also integrates with other backends like TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7QhyAMms8-L"
      },
      "source": [
        "# Grading (10pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juIdxXhos-mV"
      },
      "source": [
        "- Question 1: 0.25pt\n",
        "- Question 2: 0.25pt\n",
        "- Question 3: 0.5pt\n",
        "- Question 4: 0.25pt\n",
        "- Question 5: 0.5pt\n",
        "- Question 6: 0.5pt\n",
        "- Question 7: 0.5pt\n",
        "- Question 8: 0.5pt\n",
        "- Question 9: 1.5pt\n",
        "- Question 10: 0.5pt\n",
        "- Question 11: 0.25pt\n",
        "- Question 12: 0.5pt\n",
        "- Question 13: 0.5pt\n",
        "- Question 14: 1pt\n",
        "- Question 15: 1pt\n",
        "- Question 16: 1.5pt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}