{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ_pmgxvGur9"
   },
   "source": [
    "# Assignment 4b - Graph Convolutional Networks\n",
    "## Deep Learning Course - Vrije Universiteit Amsterdam, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEneMITS2agU"
   },
   "source": [
    "#### Instructions on how to use this notebook:\n",
    "\n",
    "This notebook is hosted on Google Colab. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
    "\n",
    "You can also avoid using Colab entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
    "\n",
    "The advantage of using Colab is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is very valuable because for many interesting models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
    "\n",
    "The default Colab runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
    "\n",
    "```sh\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "Note that despite the name, Google Colab does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
    "\n",
    "**Submission:** Upload your notebook in .ipynb format to Canvas. The code and answers to the questions in the notebook are sufficient, no separate report is expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBgoJIpdLI2Y",
    "outputId": "2f4f1d93-2f77-4ed1-ab7d-ab814daab38e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 21 13:20:00 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 33%   28C    P8             12W /  200W |     243MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1345      G   /usr/lib/xorg/Xorg                            153MiB |\n",
      "|    0   N/A  N/A      1513      G   /usr/bin/gnome-shell                           49MiB |\n",
      "|    0   N/A  N/A      2586      G   ...ures=SpareRendererForSitePerProcess         19MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsdc7fDp40rQ"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Graphs are very useful data structures that allow us to represent sets of entities and the way they are related among each other. In a graph, entities are also known as *nodes*, and any link between entities is also called an *edge*.\n",
    "\n",
    "Examples of real world objects that can be modeled as graphs are social networks, where entities are people and relations denote friendship; and molecules, where entities are atoms and relations indicate a bond between them.\n",
    "\n",
    "There has been increased interest in the recent years in the application of deep learning architectures to graph-structured data, for tasks like predicting missing relations between entities, classifying entities, and classifying graphs. This interest has been spurred by the introduction of Graph Convolutional Networks (GCNs).\n",
    "\n",
    "In this assignment, you will implement and experiment with one of the first versions of the GCN, proposed by Thomas Kipf and Max Welling in their 2017 paper, [Semi-supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). In particular, the goals of this assignment are to\n",
    "\n",
    "- Understand how GCNs are formulated\n",
    "- Implement the GCN using PyTorch\n",
    "- Train and evaluate a model for semi-supervised node classification in citation networks\n",
    "- Train and evaluate a model for binary classification of molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvsuVNczG6pP"
   },
   "source": [
    "### Representing graphs\n",
    "\n",
    "Suppose we have the following graph:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/01-graph.png\" width=\"200\">\n",
    "\n",
    "This is an undirected graph (since the edges have no specified direction) with 4 nodes. One way to represent the connectivity structure of the graph is by means of the **adjacency matrix**. The $i$-th row of the matrix contains a 1 in the $j$-th column, if nodes $i$ and $j$ are connected. For an undirected graph like the one above, this means that the adjacency matrix\n",
    "\n",
    "- Is symmetric (e.g. an edge between 0 and 2 is equivalent as an edge between 2 and 0)\n",
    "- Is square, of size $n\\times n$ where $n$ is the number of nodes\n",
    "\n",
    "The adjacency matrix for the graph above is then the following:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1 & 0 \\\\\n",
    "1 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A second matrix of interest is the **degree matrix**. This is a diagonal matrix where the $i$-th element of the diagonal indicates the number of edges connected to node $i$. Note that these can be obtained from $A$ by summing across the columns, or the rows. For our example, the degree matrix is\n",
    "\n",
    "$$\n",
    "D = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 3 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For specific applications, each node in the graph will have an associated vector of features $x\\in\\mathbb{R}^c$. If our graph is a social network, then the vector of features can contain information like age, location, and musical tastes, in a specific numeric format. In the case of a molecule, the node could represent an atom and have features like the atomic mass, etc. We can lay out the features in a matrix $X\\in\\mathbb{R}^{n\\times c}$, so that the feature vector for node $i$ is in the $i$-th row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCEQ2ffzHCf2"
   },
   "source": [
    "### Loading a citation network\n",
    "\n",
    "To move to a real world example, we will start with the Cora dataset. This dataset represents a citation network, where nodes are scientific publications, edges denote citations between them, and features are a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) extracted from their contents.\n",
    "\n",
    "This graph contains labels for nodes, that represent a specific topic. We will use these for a node classification task.\n",
    "\n",
    "To easily load it, we will use [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) (PyG), a deep learning library for graph-structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yd2bTEBADt-a"
   },
   "outputs": [],
   "source": [
    "# Install PyTorch Geometric\n",
    "import torch\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    # Installing torch geometric packages with specific CUDA+PyTorch version.\n",
    "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details\n",
    "    TORCH = torch.__version__.split('+')[0]\n",
    "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
    "\n",
    "    %pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    %pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    %pip install torch-geometric\n",
    "    import torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0Nvh_-qEo1q"
   },
   "source": [
    "We can now use the library to download and import the dataset. Initializing the `Planetoid` class returns a `Dataset` object that can contain multiple graphs. In this task we will only use the `Cora` dataset (the citation network) and hence, we will select only the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vuOvwhsHD2YK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4WZkoiHFyZm"
   },
   "source": [
    "\n",
    "#### Question 1 (0.25 pt)\n",
    "\n",
    "The `data` object is an instance of the `Data` class in PyG. Check the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html) and report the following properties of the graph:\n",
    "\n",
    "- Number of nodes\n",
    "- Number of edges \n",
    "- The dimension $c$ of the feature vectors $x\\in\\mathbb{R}^c$\n",
    "- The number of targets for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sjVuGJhlJC_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of nodes: 2708\n",
      "Number of features: 1433\n",
      "Number of edges: 10556\n",
      "Number of targets: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nnodes = data.num_nodes\n",
    "in_features = data.num_node_features\n",
    "# Also possible\n",
    "assert (nnodes, in_features) == data.x.size()\n",
    "\n",
    "nedges = data.num_edges\n",
    "\n",
    "ntargets = len(set(map(int, data.y)))\n",
    "# Also possible\n",
    "assert ntargets == dataset.num_classes\n",
    "\n",
    "print(f\"\"\"\n",
    "Number of nodes: {nnodes}\n",
    "Number of features: {in_features}\n",
    "Number of edges: {nedges}\n",
    "Number of targets: {ntargets}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4DrGDAuJ2YO"
   },
   "source": [
    "#### Question 2 (0.25 pt)\n",
    "\n",
    "In PyG, edges are provided in a tensor of shape (2, number of edges). You can access it via `data.edge_index`. Each column in this tensor contains the IDs for two nodes that are connected in the graph.\n",
    "\n",
    "We saw that in an undirected graph, an edge between nodes $i$ and $j$ adds a value of 1 to positions $(i, j)$ and $(j, i)$ of the adjacency matrix. Is this also true for the edge index? That is, if there is an edge $(i, j)$ in `data.edge_index`, is there also an edge for $(j, i)$? This is important to know for the next steps of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jTRfNxibarRZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is: YES\n"
     ]
    }
   ],
   "source": [
    "indices = data.edge_index.t()\n",
    "is_true = all([torch.tensor([j, i]) in indices for i, j in indices])\n",
    "print(f\"The answer is: {'YES' if is_true else 'NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOpS3QTYiOqp"
   },
   "source": [
    "#### Question 3 (0.5 pt)\n",
    "\n",
    "In graphs, especially large ones, the adjacency matrix is **sparse**: most entries are zero. Sparse matrices allow for efficient storage and computation.\n",
    "\n",
    "To prepare and pre-process sparse matrices, we will use [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html). Once the matrices are ready, we will convert them to PyTorch tensors.\n",
    "\n",
    "We will use the [Sparse COO format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)). We encourage you to first get familiar with how it works after continuing with the assignment.\n",
    "\n",
    "- Use the [`scipy.sparse.coo_matrix()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) function to build the adjacency matrix. Think of what arguments are needed, and how you can obtain them from the graph data loaded above.\n",
    "- Use the `sum()` method of sparse matrices, together with `scipy.sparse.diags()`, to compute the degree matrix using the definition above.\n",
    "\n",
    "Both resulting matrices must be sparse of type `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QC01OjbJs92-"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.sparse\n",
    "\n",
    "row = data.edge_index[0, :]\n",
    "col = data.edge_index[1, :]\n",
    "assert len(row) == len(col) == nedges\n",
    "\n",
    "# We just put ones for neighbours\n",
    "values = torch.ones(nedges)\n",
    "\n",
    "shape = (nnodes, nnodes)\n",
    "\n",
    "adj_mat = scipy.sparse.coo_matrix((values, (row, col)), shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIEJyQi2TzyY"
   },
   "source": [
    "You might wonder why we suggest to use a scipy sparse matrix, while also PyTorch supports them. The reason is that in the next step, we will be multiplying two sparse matrices, an operation not supported in PyTorch. PyTorch only allows multiplying a sparse matrix with a dense one, something which we will be doing at a later stage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlmzSb0up4LB"
   },
   "source": [
    "### The Graph Convolutional Network\n",
    "\n",
    "The goal of the graph convolution is to take the feature vectors of all nodes $X\\in\\mathbb{R}^{n\\times c}$, and propagate them along the existing edges, to obtain updated representations $Z\\in\\mathbb{R}^{n\\times d}$.\n",
    "\n",
    "\n",
    "The GCN is initially motivated as performing a convolution, similarly as it is done in CNNs for images, for graph-structured data. In Kipf and Welling (2017), a theoretical derivation leads to the following formula:\n",
    "\n",
    "$$\n",
    "Z = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}XW\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $W\\in\\mathbb{R}^{c\\times d}$ is a matrix of parameters to be learned via gradient descent\n",
    "- $\\tilde{A} = A + I_n$, where $I_n$ is an $n\\times n$ identity matrix\n",
    "- $\\tilde{D}$ is the degree matrix computed with $\\tilde{A}$ as the adjacency matrix\n",
    "\n",
    "If we define $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$, the graph convolution can be written as $Z = \\hat{A}XW$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LL4b-MTvysBp"
   },
   "source": [
    "#### Question 4 (0.25 pt)\n",
    "\n",
    "Given the formula for the GCN, explain why it operates by propagating feature vectors across the graph. To answer this, it might be useful to recall the definitions of the adjacency and degree matrices, and how they are involved in the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgx2SkTTyiSN"
   },
   "source": [
    "The multiplication of the **adjacency matrix** (or some function of it) with the matrix $X$ has the effect of summing up feature vectors for all neighbouring nodes. This is effectively propagating feature vectors across the graph. The **degree matrix** provides a normalization constant for each feature vector, just to keep the scale of the features.\n",
    "\n",
    "To give a more precise intuiton, it is helpful to write the formula above in vector form ([source](https://tkipf.github.io/graph-convolutional-networks/)):\n",
    "$$\n",
    "    z_{i} = \\sum_{j \\in \\mathcal{N}_i} \\frac{1}{c_{ij}}x_{j}W\n",
    "$$\n",
    "where we identify $x_j$ as the feature vectors (rows of $X$), $\\mathcal{N}_i$ is the set of neighbours of node $i$ and $c_{ij}$ is the normalization constant. The neighbours of the node $i$ are encoded as ones in the $i$-th row of the adjacency matrix $A$. $\\tilde{A}$ encodes the same information as $A$ plus adding self-loops. $\\tilde{D}$ is the degree matrix (taking into account self-loops), constructed as:\n",
    "$$\n",
    "    \\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}\n",
    "$$\n",
    "which effectively normalizes the summed-up feature vectors according to the number of neighbors of each node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUGABEqxylsd"
   },
   "source": [
    "#### Question 5 (0.5 pt)\n",
    "\n",
    "Compute the **normalized adjacency matrix** $\\hat{A}$. The result should be a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GPZbnSaSyDzO"
   },
   "outputs": [],
   "source": [
    "# Add self-loops. This will turn A into \\tilde{A}\n",
    "adj_mat.setdiag(1)\n",
    "\n",
    "# Sum along columns to get the diagonal\n",
    "diag = adj_mat.sum(axis=1).A1\n",
    "\n",
    "# Compute degree matrix\n",
    "d = scipy.sparse.diags_array(diag, shape=(nnodes, nnodes))\n",
    "\n",
    "# Compute \\hat{A}\n",
    "adj_mat_norm = ((d ** (1/2)) @ adj_mat @ (d ** (1/2))).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLLdGdZoMEy-"
   },
   "source": [
    "#### Question 6 (0.5 pt)\n",
    "\n",
    "So far we have used scipy to build and compute sparse matrices. Since we want to train a GCN with PyTorch, we need to convert $\\hat{A}$ into a sparse PyTorch tensor. You can do this with the [`torch.sparse_coo_tensor()`](https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html) function, making sure to specify `torch.float` as the type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dgDsVHzEM32F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "assert adj_mat_norm.shape == (nnodes, nnodes)\n",
    "\n",
    "coords = np.array(adj_mat_norm.coords)\n",
    "values = adj_mat_norm.data\n",
    "\n",
    "adj_mat_norm_torch = torch.sparse_coo_tensor(indices=coords, values=values, size=(nnodes, nnodes), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAlRVT5aODkX"
   },
   "source": [
    "#### Question 7 (0.5 pt)\n",
    "\n",
    "We now have all the ingredients to build a GCN layer. Implement a class (inheriting from `torch.nn.Module`) with a learnable matrix of weights $W\\in\\mathbb{R}^{c\\times d}$. Make sure to\n",
    "\n",
    "- Call this class `GCNLayer`\n",
    "- The `__init__()` constructor should take as argument the number of input and output features.\n",
    "- Use `torch.nn.init.kaiming_uniform_` to initialize $W$.\n",
    "- Define the `forward` method, which takes as input $X$ and $\\hat{A}$ and returns $Z$. Note that multiplications involving the sparse matrix $\\hat{A}$ have to be done with `torch.spmm`. \n",
    "\n",
    "Once you have implemented the class, instantiate a layer with the correct number of input features for the Cora dataset, and a number of output features of your choice. Do a forward pass and report the shape of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFCohhhwPpTT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the output is: torch.Size([2708, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class GCNLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weights = Parameter(torch.empty(in_features, out_features))\n",
    "        self.bias = Parameter(torch.zeros(out_features))\n",
    "        torch.nn.init.kaiming_uniform_(self.weights.T, nonlinearity=\"relu\")\n",
    "    \n",
    "    def forward(self, input_m, adjacency_m):\n",
    "        assert adjacency_m.size(1) == input_m.size(0)\n",
    "        return torch.spmm(adjacency_m, F.linear(input_m, self.weights.T)) + self.bias\n",
    "\n",
    "out_features = 200\n",
    "gcn_layer = GCNLayer(in_features=in_features, out_features=out_features)\n",
    "\n",
    "# Forward pass\n",
    "output = gcn_layer(data.x, adj_mat_norm_torch)\n",
    "print(f\"The shape of the output is: {output.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ptAiizZUKaM"
   },
   "source": [
    "#### Question 8 (0.5 pt)\n",
    "\n",
    "As we have seen so far, the GCN layer implements a special type of linear transformation of the inputs. However, it is often beneficial in deep learning to stack multiple, non-linear transformations of the input features. Implement a second module class for a model with two GCN layers (use the module you implemented in the previous question).\n",
    "\n",
    "- Call this class `GCN`\n",
    "- The constructor must now take as input the number of input features, the output dimension of the first layer (this is the hidden layer), and the output dimension of the output layer.\n",
    "- In the forward pass, add a ReLU activation function after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2zhyu3S9Vj3b"
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_features, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        self.gcn_layer_1 = GCNLayer(in_features=in_features, out_features=n_hidden)\n",
    "        self.gcn_layer_2 = GCNLayer(in_features=n_hidden, out_features=n_output)\n",
    "\n",
    "    def forward(self, x, adjacency):\n",
    "        x = self.gcn_layer_1(x, adjacency)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn_layer_2(x, adjacency)\n",
    "        x = F.log_softmax(x, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NVB-3I5Wfkf"
   },
   "source": [
    "### GCNs for semi-supervised node classification\n",
    "\n",
    "Now that we have a GCN with two layers, we can test its performance in a node classification task. We will pass the input node features $X$ through the GCN layers, and the output will be of size $n\\times k$ where $k$ is the number of classes (which you found in question 1). The label denotes the topic an article in the citation network belongs to (e.g. physics, computer science, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trc4dSa7cuQj"
   },
   "source": [
    "#### Question 9 (1.5 pt)\n",
    "\n",
    "Note that the `data` object contains all labels (for all splits) in `data.y`, and binary masks for the train, validation, and test splits in `data.train_mask`, `data.val_mask`, and `data.test_mask`, respectively. These masks are the same size as `data.y`, and indicate which labels belong to which split.\n",
    "\n",
    "- Create a GCN with two layers (using the class from the previous question), with 32 as the hidden dimension, and the number of output features equal to the number of classes in the Cora dataset.\n",
    "\n",
    "- Use the Adam optimizer with a learning rate of 0.01.\n",
    "\n",
    "- Implement a training loop for the GCN. At each step, pass $X$ and $\\hat{A}$ to the GCN to obtain the logits. Compute the mean cross-entropy loss **only for the training instances**, using the binary masks.\n",
    "\n",
    "- After each training step, evaluate the accuracy for the validation instances.\n",
    "\n",
    "- Train for 100 epochs. Once training is finished, plot the training loss and validation accuracy (in a graph in function of the epoch number), and report the accuracy in the test set.\n",
    "\n",
    "You should obtain an accuracy over 75% on both the validation and test sets. You can also compare your results with the original paper, which also contains results for the Cora dataset. Give a brief discussion on the results of your experiments.\n",
    "\n",
    "Note that in contrast with other tasks, like image classification on some datasets, we don't use mini-batches here. The whole matrix of features and the adjacency is passed to the GCN in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z2OP_ZRWlmo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 1684.018677 \n",
      " Training accuracy: 0.092857\n",
      " Validation accuracy: 0.2420\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 678.573303 \n",
      " Training accuracy: 0.300000\n",
      " Validation accuracy: 0.3440\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training loss: 767.087891 \n",
      " Training accuracy: 0.307143\n",
      " Validation accuracy: 0.4580\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training loss: 694.224121 \n",
      " Training accuracy: 0.414286\n",
      " Validation accuracy: 0.4220\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training loss: 900.209473 \n",
      " Training accuracy: 0.335714\n",
      " Validation accuracy: 0.4640\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training loss: 647.288635 \n",
      " Training accuracy: 0.421429\n",
      " Validation accuracy: 0.4780\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training loss: 510.496704 \n",
      " Training accuracy: 0.464286\n",
      " Validation accuracy: 0.5100\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training loss: 354.309204 \n",
      " Training accuracy: 0.557143\n",
      " Validation accuracy: 0.5180\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training loss: 189.530930 \n",
      " Training accuracy: 0.664286\n",
      " Validation accuracy: 0.4080\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training loss: 392.932587 \n",
      " Training accuracy: 0.628571\n",
      " Validation accuracy: 0.4840\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training loss: 213.026047 \n",
      " Training accuracy: 0.728571\n",
      " Validation accuracy: 0.4660\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training loss: 292.875000 \n",
      " Training accuracy: 0.707143\n",
      " Validation accuracy: 0.4260\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training loss: 371.253632 \n",
      " Training accuracy: 0.628571\n",
      " Validation accuracy: 0.4540\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training loss: 369.961426 \n",
      " Training accuracy: 0.642857\n",
      " Validation accuracy: 0.5180\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training loss: 313.292480 \n",
      " Training accuracy: 0.707143\n",
      " Validation accuracy: 0.5980\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training loss: 247.789597 \n",
      " Training accuracy: 0.792857\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training loss: 185.786957 \n",
      " Training accuracy: 0.821429\n",
      " Validation accuracy: 0.6440\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training loss: 112.403404 \n",
      " Training accuracy: 0.842857\n",
      " Validation accuracy: 0.6380\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training loss: 44.630856 \n",
      " Training accuracy: 0.792857\n",
      " Validation accuracy: 0.5480\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training loss: 544.031860 \n",
      " Training accuracy: 0.728571\n",
      " Validation accuracy: 0.5780\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training loss: 105.681999 \n",
      " Training accuracy: 0.757143\n",
      " Validation accuracy: 0.6740\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training loss: 134.400543 \n",
      " Training accuracy: 0.871429\n",
      " Validation accuracy: 0.6800\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training loss: 244.837814 \n",
      " Training accuracy: 0.842857\n",
      " Validation accuracy: 0.6560\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training loss: 342.376984 \n",
      " Training accuracy: 0.821429\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training loss: 420.982910 \n",
      " Training accuracy: 0.807143\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training loss: 481.513733 \n",
      " Training accuracy: 0.771429\n",
      " Validation accuracy: 0.6260\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training loss: 500.152405 \n",
      " Training accuracy: 0.778571\n",
      " Validation accuracy: 0.6320\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training loss: 499.484772 \n",
      " Training accuracy: 0.778571\n",
      " Validation accuracy: 0.6540\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training loss: 482.281647 \n",
      " Training accuracy: 0.792857\n",
      " Validation accuracy: 0.6580\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training loss: 461.579865 \n",
      " Training accuracy: 0.807143\n",
      " Validation accuracy: 0.6560\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training loss: 434.766205 \n",
      " Training accuracy: 0.792857\n",
      " Validation accuracy: 0.6740\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training loss: 404.907959 \n",
      " Training accuracy: 0.792857\n",
      " Validation accuracy: 0.6840\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training loss: 371.140625 \n",
      " Training accuracy: 0.814286\n",
      " Validation accuracy: 0.6840\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training loss: 328.794403 \n",
      " Training accuracy: 0.828571\n",
      " Validation accuracy: 0.6720\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training loss: 279.815704 \n",
      " Training accuracy: 0.828571\n",
      " Validation accuracy: 0.6580\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training loss: 227.820374 \n",
      " Training accuracy: 0.835714\n",
      " Validation accuracy: 0.6520\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training loss: 177.238312 \n",
      " Training accuracy: 0.835714\n",
      " Validation accuracy: 0.6400\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training loss: 123.185432 \n",
      " Training accuracy: 0.835714\n",
      " Validation accuracy: 0.6280\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training loss: 65.440475 \n",
      " Training accuracy: 0.828571\n",
      " Validation accuracy: 0.5640\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training loss: 132.192032 \n",
      " Training accuracy: 0.778571\n",
      " Validation accuracy: 0.6180\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training loss: 38.742741 \n",
      " Training accuracy: 0.842857\n",
      " Validation accuracy: 0.6380\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training loss: 32.845802 \n",
      " Training accuracy: 0.871429\n",
      " Validation accuracy: 0.6460\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training loss: 41.262203 \n",
      " Training accuracy: 0.885714\n",
      " Validation accuracy: 0.6500\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training loss: 39.322369 \n",
      " Training accuracy: 0.885714\n",
      " Validation accuracy: 0.6500\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training loss: 28.736902 \n",
      " Training accuracy: 0.885714\n",
      " Validation accuracy: 0.6500\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training loss: 15.950820 \n",
      " Training accuracy: 0.878571\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training loss: 20.122690 \n",
      " Training accuracy: 0.850000\n",
      " Validation accuracy: 0.6400\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training loss: 9.290799 \n",
      " Training accuracy: 0.878571\n",
      " Validation accuracy: 0.6460\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training loss: 16.155113 \n",
      " Training accuracy: 0.878571\n",
      " Validation accuracy: 0.6560\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training loss: 18.872467 \n",
      " Training accuracy: 0.878571\n",
      " Validation accuracy: 0.6600\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Training loss: 15.966366 \n",
      " Training accuracy: 0.878571\n",
      " Validation accuracy: 0.6460\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Training loss: 4.822958 \n",
      " Training accuracy: 0.900000\n",
      " Validation accuracy: 0.5960\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Training loss: 34.534260 \n",
      " Training accuracy: 0.857143\n",
      " Validation accuracy: 0.6540\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Training loss: 24.607313 \n",
      " Training accuracy: 0.892857\n",
      " Validation accuracy: 0.6440\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Training loss: 48.739475 \n",
      " Training accuracy: 0.907143\n",
      " Validation accuracy: 0.6320\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Training loss: 57.729599 \n",
      " Training accuracy: 0.914286\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Training loss: 57.768360 \n",
      " Training accuracy: 0.907143\n",
      " Validation accuracy: 0.6420\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Training loss: 51.599518 \n",
      " Training accuracy: 0.914286\n",
      " Validation accuracy: 0.6380\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Training loss: 36.733826 \n",
      " Training accuracy: 0.921429\n",
      " Validation accuracy: 0.6480\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Training loss: 15.622203 \n",
      " Training accuracy: 0.914286\n",
      " Validation accuracy: 0.6000\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Training loss: 15.778876 \n",
      " Training accuracy: 0.871429\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Training loss: 5.185340 \n",
      " Training accuracy: 0.914286\n",
      " Validation accuracy: 0.6320\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Training loss: 3.850440 \n",
      " Training accuracy: 0.921429\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Training loss: 5.629016 \n",
      " Training accuracy: 0.928571\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Training loss: 6.910845 \n",
      " Training accuracy: 0.928571\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Training loss: 7.388268 \n",
      " Training accuracy: 0.928571\n",
      " Validation accuracy: 0.6360\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Training loss: 7.130322 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6380\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Training loss: 6.652047 \n",
      " Training accuracy: 0.928571\n",
      " Validation accuracy: 0.6400\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Training loss: 4.723514 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6380\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Training loss: 3.493928 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Training loss: 3.791705 \n",
      " Training accuracy: 0.921429\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Training loss: 5.433744 \n",
      " Training accuracy: 0.907143\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Training loss: 4.200995 \n",
      " Training accuracy: 0.928571\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Training loss: 3.291343 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Training loss: 3.212446 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Training loss: 5.447559 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6320\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Training loss: 3.710586 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Training loss: 3.920666 \n",
      " Training accuracy: 0.928571\n",
      " Validation accuracy: 0.6320\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Training loss: 3.008654 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Training loss: 2.963459 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Training loss: 2.928699 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Training loss: 3.070660 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Training loss: 2.867028 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6280\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Training loss: 2.778758 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Training loss: 2.730284 \n",
      " Training accuracy: 0.935714\n",
      " Validation accuracy: 0.6280\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Training loss: 2.647033 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6280\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Training loss: 2.758511 \n",
      " Training accuracy: 0.950000\n",
      " Validation accuracy: 0.6260\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Training loss: 2.623350 \n",
      " Training accuracy: 0.950000\n",
      " Validation accuracy: 0.6260\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Training loss: 2.528617 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6260\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Training loss: 2.501629 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Training loss: 2.473653 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Training loss: 3.490682 \n",
      " Training accuracy: 0.950000\n",
      " Validation accuracy: 0.6320\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Training loss: 2.418392 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6320\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Training loss: 2.406367 \n",
      " Training accuracy: 0.950000\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Training loss: 2.357300 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.5960\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Training loss: 5.054027 \n",
      " Training accuracy: 0.928571\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Training loss: 2.264904 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6320\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Training loss: 17.517843 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6360\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Training loss: 24.753443 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6360\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Training loss: 21.671886 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6340\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Training loss: 9.243979 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6240\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Training loss: 2.140649 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.5680\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Training loss: 42.343983 \n",
      " Training accuracy: 0.907143\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Training loss: 15.078782 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Training loss: 46.796894 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6180\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Training loss: 65.908318 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Training loss: 73.547768 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Training loss: 70.794067 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Training loss: 58.673843 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6240\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Training loss: 38.160374 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6260\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Training loss: 10.207305 \n",
      " Training accuracy: 0.950000\n",
      " Validation accuracy: 0.6180\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Training loss: 1.921935 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.5740\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Training loss: 184.831100 \n",
      " Training accuracy: 0.885714\n",
      " Validation accuracy: 0.6300\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Training loss: 1.873533 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6260\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Training loss: 30.790148 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Training loss: 58.540424 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 117\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 74.271202 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6180\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Training loss: 79.126984 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6180\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Training loss: 74.384216 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6180\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Training loss: 60.461224 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6240\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Training loss: 38.974689 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6360\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Training loss: 10.568855 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6240\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Training loss: 1.592806 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.5680\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Training loss: 83.422745 \n",
      " Training accuracy: 0.878571\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Training loss: 1.703185 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Training loss: 1.478230 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Training loss: 6.332793 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Training loss: 15.050947 \n",
      " Training accuracy: 0.950000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Training loss: 14.169289 \n",
      " Training accuracy: 0.950000\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Training loss: 4.482857 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Training loss: 1.174328 \n",
      " Training accuracy: 0.950000\n",
      " Validation accuracy: 0.6040\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Training loss: 1.160739 \n",
      " Training accuracy: 0.907143\n",
      " Validation accuracy: 0.5980\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Training loss: 1.873014 \n",
      " Training accuracy: 0.885714\n",
      " Validation accuracy: 0.6040\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Training loss: 2.680684 \n",
      " Training accuracy: 0.900000\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Training loss: 0.890061 \n",
      " Training accuracy: 0.942857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Training loss: 0.773721 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Training loss: 0.690015 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Training loss: 0.609066 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Training loss: 0.527033 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Training loss: 1.611985 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Training loss: 0.358289 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6240\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Training loss: 0.271122 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Training loss: 0.211644 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Training loss: 0.232405 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6240\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Training loss: 0.289828 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Training loss: 0.410153 \n",
      " Training accuracy: 0.957143\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Training loss: 0.336662 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Training loss: 0.329272 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Training loss: 0.305085 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Training loss: 0.266549 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Training loss: 0.216496 \n",
      " Training accuracy: 0.964286\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Training loss: 0.174625 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Training loss: 0.152945 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Training loss: 0.142277 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Training loss: 0.135047 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Training loss: 0.128033 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Training loss: 0.120964 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Training loss: 0.114229 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Training loss: 0.110518 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Training loss: 0.110221 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Training loss: 0.105322 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Training loss: 0.093898 \n",
      " Training accuracy: 0.971429\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Training loss: 0.080010 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Training loss: 0.069604 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Training loss: 0.061460 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Training loss: 0.053842 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Training loss: 0.046606 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Training loss: 0.040199 \n",
      " Training accuracy: 0.978571\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Training loss: 0.035349 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Training loss: 0.032406 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Training loss: 0.030900 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Training loss: 0.030146 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Training loss: 0.029720 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Training loss: 0.029434 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Training loss: 0.029208 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Training loss: 0.029012 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Training loss: 0.028830 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Training loss: 0.028658 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Training loss: 0.028492 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Training loss: 0.028330 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Training loss: 0.028171 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Training loss: 0.028016 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Training loss: 0.027863 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Training loss: 0.027713 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Training loss: 0.027565 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Training loss: 0.027420 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Training loss: 0.027276 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Training loss: 0.027134 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Training loss: 0.026994 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Training loss: 0.026856 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Training loss: 0.026720 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Training loss: 0.026585 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Training loss: 0.026452 \n",
      " Training accuracy: 0.985714\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Training loss: 0.026321 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Training loss: 0.026191 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Training loss: 0.026063 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Training loss: 0.025936 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Training loss: 0.025811 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Training loss: 0.025687 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Training loss: 0.025564 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Training loss: 0.025442 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Training loss: 0.025322 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Training loss: 0.025203 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Training loss: 0.025084 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Training loss: 0.024968 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Training loss: 0.024852 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Training loss: 0.024737 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Training loss: 0.024623 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Training loss: 0.024511 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6060\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Training loss: 0.024399 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Training loss: 0.024288 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Training loss: 0.024179 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Training loss: 0.024070 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Training loss: 0.023962 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Training loss: 0.023855 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Training loss: 0.023749 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Training loss: 0.023644 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Training loss: 0.023540 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Training loss: 0.023436 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Training loss: 0.023334 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Training loss: 0.023232 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Training loss: 0.023131 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Training loss: 0.023031 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Training loss: 0.022932 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Training loss: 0.022833 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Training loss: 0.022735 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Training loss: 0.022638 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Training loss: 0.022542 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Training loss: 0.022446 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Training loss: 0.022351 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Training loss: 0.022257 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Training loss: 0.022164 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Training loss: 0.022071 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Training loss: 0.021979 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Training loss: 0.021887 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Training loss: 0.021796 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Training loss: 0.021706 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Training loss: 0.021617 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Training loss: 0.021528 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Training loss: 0.021439 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Training loss: 0.021352 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Training loss: 0.021265 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Training loss: 0.021178 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Training loss: 0.021092 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Training loss: 0.021007 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Training loss: 0.020922 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Training loss: 0.020838 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Training loss: 0.020754 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Training loss: 0.020671 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Training loss: 0.020589 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Training loss: 0.020505 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Training loss: 0.020421 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Training loss: 0.020338 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Training loss: 0.020255 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Training loss: 0.020172 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Training loss: 0.020089 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Training loss: 0.020007 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Training loss: 0.019926 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Training loss: 0.019844 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Training loss: 0.019763 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Training loss: 0.019683 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Training loss: 0.019603 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Training loss: 0.019523 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Training loss: 0.019444 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Training loss: 0.019365 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Training loss: 0.019287 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Training loss: 0.019210 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Training loss: 0.019133 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Training loss: 0.019056 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Training loss: 0.018979 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Training loss: 0.018904 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Training loss: 0.018828 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Training loss: 0.018753 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Training loss: 0.018678 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Training loss: 0.018604 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Training loss: 0.018531 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Training loss: 0.018457 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Training loss: 0.018384 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Training loss: 0.018312 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Training loss: 0.018240 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Training loss: 0.018168 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Training loss: 0.018097 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Training loss: 0.018028 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Training loss: 0.017968 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Training loss: 0.017908 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Training loss: 0.017849 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Training loss: 0.017790 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Training loss: 0.017732 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Training loss: 0.017673 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Training loss: 0.017616 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Training loss: 0.017558 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Training loss: 0.017501 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Training loss: 0.017444 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Training loss: 0.017388 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Training loss: 0.017332 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Training loss: 0.017276 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Training loss: 0.017221 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Training loss: 0.017165 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Training loss: 0.017111 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Training loss: 0.017056 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Training loss: 0.017002 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Training loss: 0.016948 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Training loss: 0.016894 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Training loss: 0.016841 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Training loss: 0.016788 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6080\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Training loss: 0.016735 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Training loss: 0.016682 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Training loss: 0.016630 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Training loss: 0.016578 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Training loss: 0.016526 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Training loss: 0.016475 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Training loss: 0.016424 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Training loss: 0.016373 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Training loss: 0.016322 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Training loss: 0.016272 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Training loss: 0.016222 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Training loss: 0.016172 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Training loss: 0.016122 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Training loss: 0.016073 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Training loss: 0.016023 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Training loss: 0.015974 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Training loss: 0.015926 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Training loss: 0.015877 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Training loss: 0.015829 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Training loss: 0.015781 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Training loss: 0.015733 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Training loss: 0.015685 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Training loss: 0.015638 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Training loss: 0.015591 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Training loss: 0.015544 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Training loss: 0.015497 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Training loss: 0.015451 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Training loss: 0.015404 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Training loss: 0.015358 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Training loss: 0.015312 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Training loss: 0.015267 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Training loss: 0.015221 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Training loss: 0.015176 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Training loss: 0.015131 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Training loss: 0.015086 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Training loss: 0.015041 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Training loss: 0.014997 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Training loss: 0.014952 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Training loss: 0.014908 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Training loss: 0.014864 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Training loss: 0.014821 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Training loss: 0.014777 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Training loss: 0.014734 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Training loss: 0.014690 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Training loss: 0.014647 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Training loss: 0.014604 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Training loss: 0.014562 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Training loss: 0.014519 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Training loss: 0.014477 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Training loss: 0.014435 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6100\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Training loss: 0.014393 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Training loss: 0.014351 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Training loss: 0.014309 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Training loss: 0.014268 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Training loss: 0.014226 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Training loss: 0.014185 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Training loss: 0.014144 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Training loss: 0.014103 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Training loss: 0.014062 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Training loss: 0.014022 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Training loss: 0.013981 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Training loss: 0.013941 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Training loss: 0.013901 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Training loss: 0.013861 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Training loss: 0.013821 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Training loss: 0.013781 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Training loss: 0.013742 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Training loss: 0.013702 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Training loss: 0.013663 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Training loss: 0.013624 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Training loss: 0.013585 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Training loss: 0.013546 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Training loss: 0.013508 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Training loss: 0.013469 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Training loss: 0.013431 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Training loss: 0.013392 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Training loss: 0.013354 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Training loss: 0.013316 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Training loss: 0.013278 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Training loss: 0.013241 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Training loss: 0.013203 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Training loss: 0.013166 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Training loss: 0.013128 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Training loss: 0.013091 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Training loss: 0.013054 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Training loss: 0.013017 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Training loss: 0.012980 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Training loss: 0.012943 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Training loss: 0.012907 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Training loss: 0.012870 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Training loss: 0.012834 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Training loss: 0.012798 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Training loss: 0.012762 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Training loss: 0.012726 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Training loss: 0.012690 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Training loss: 0.012654 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Training loss: 0.012619 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Training loss: 0.012583 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Training loss: 0.012548 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Training loss: 0.012513 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Training loss: 0.012477 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Training loss: 0.012442 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Training loss: 0.012408 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Training loss: 0.012373 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Training loss: 0.012338 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Training loss: 0.012304 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Training loss: 0.012269 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Training loss: 0.012235 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Training loss: 0.012201 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Training loss: 0.012166 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Training loss: 0.012132 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Training loss: 0.012099 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Training loss: 0.012065 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Training loss: 0.012031 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Training loss: 0.011998 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Training loss: 0.011964 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Training loss: 0.011931 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Training loss: 0.011898 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Training loss: 0.011864 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Training loss: 0.011831 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Training loss: 0.011799 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Training loss: 0.011766 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Training loss: 0.011733 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Training loss: 0.011700 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Training loss: 0.011668 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Training loss: 0.011636 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Training loss: 0.011603 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Training loss: 0.011571 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Training loss: 0.011539 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Training loss: 0.011507 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Training loss: 0.011475 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Training loss: 0.011443 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Training loss: 0.011412 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Training loss: 0.011380 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Training loss: 0.011349 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Training loss: 0.011317 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Training loss: 0.011286 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Training loss: 0.011255 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Training loss: 0.011224 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Training loss: 0.011193 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Training loss: 0.011162 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Training loss: 0.011131 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Training loss: 0.011101 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Training loss: 0.011070 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Training loss: 0.011040 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Training loss: 0.011009 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Training loss: 0.010979 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Training loss: 0.010949 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Training loss: 0.010919 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Training loss: 0.010889 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Training loss: 0.010859 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Training loss: 0.010829 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Training loss: 0.010799 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Training loss: 0.010770 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Training loss: 0.010740 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Training loss: 0.010711 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Training loss: 0.010682 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Training loss: 0.010652 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Training loss: 0.010623 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Training loss: 0.010594 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Training loss: 0.010565 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Training loss: 0.010536 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Training loss: 0.010508 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Training loss: 0.010479 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Training loss: 0.010450 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Training loss: 0.010422 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Training loss: 0.010394 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Training loss: 0.010365 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Training loss: 0.010337 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Training loss: 0.010309 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Training loss: 0.010281 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6120\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Training loss: 0.010253 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Training loss: 0.010225 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Training loss: 0.010197 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Training loss: 0.010170 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Training loss: 0.010142 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Training loss: 0.010115 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Training loss: 0.010087 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Training loss: 0.010060 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Training loss: 0.010033 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Training loss: 0.010006 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Training loss: 0.009979 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Training loss: 0.009952 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Training loss: 0.009925 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Training loss: 0.009898 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Training loss: 0.009871 \n",
      " Training accuracy: 0.992857\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Training loss: 0.009845 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Training loss: 0.009818 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Training loss: 0.009792 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Training loss: 0.009765 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Training loss: 0.009739 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Training loss: 0.009713 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Training loss: 0.009687 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Training loss: 0.009661 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Training loss: 0.009635 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Training loss: 0.009609 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Training loss: 0.009583 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Training loss: 0.009558 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Training loss: 0.009532 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Training loss: 0.009507 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Training loss: 0.009481 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Training loss: 0.009456 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Training loss: 0.009431 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Training loss: 0.009406 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Training loss: 0.009381 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Training loss: 0.009356 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Training loss: 0.009331 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Training loss: 0.009306 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Training loss: 0.009281 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Training loss: 0.009257 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Training loss: 0.009232 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Training loss: 0.009207 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Training loss: 0.009183 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Training loss: 0.009159 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6140\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Training loss: 0.009134 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Training loss: 0.009110 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Training loss: 0.009086 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Training loss: 0.009062 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Training loss: 0.009038 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Training loss: 0.009014 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Training loss: 0.008991 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Training loss: 0.008967 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Training loss: 0.008943 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Training loss: 0.008920 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Training loss: 0.008896 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Training loss: 0.008873 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Training loss: 0.008850 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Training loss: 0.008827 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Training loss: 0.008804 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Training loss: 0.008780 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Training loss: 0.008757 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Training loss: 0.008735 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Training loss: 0.008712 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Training loss: 0.008689 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Training loss: 0.008666 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Training loss: 0.008644 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Training loss: 0.008621 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Training loss: 0.008599 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Training loss: 0.008577 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Training loss: 0.008554 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Training loss: 0.008532 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Training loss: 0.008510 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Training loss: 0.008488 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Training loss: 0.008466 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Training loss: 0.008444 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "Training loss: 0.008422 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "Training loss: 0.008400 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "Training loss: 0.008379 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "Training loss: 0.008357 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "Training loss: 0.008336 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "Training loss: 0.008314 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "Training loss: 0.008293 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "Training loss: 0.008272 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "Training loss: 0.008250 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6160\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "Training loss: 0.008229 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "Training loss: 0.008208 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "Training loss: 0.008187 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "Training loss: 0.008166 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "Training loss: 0.008145 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "Training loss: 0.008125 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "Training loss: 0.008104 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "Training loss: 0.008083 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "Training loss: 0.008063 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "Training loss: 0.008042 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "Training loss: 0.008022 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "Training loss: 0.008001 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "Training loss: 0.007981 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "Training loss: 0.007961 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "Training loss: 0.007941 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "Training loss: 0.007921 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "Training loss: 0.007901 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "Training loss: 0.007881 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "Training loss: 0.007861 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "Training loss: 0.007841 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "Training loss: 0.007821 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "Training loss: 0.007802 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "Training loss: 0.007782 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "Training loss: 0.007764 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "Training loss: 0.007745 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "Training loss: 0.007727 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "Training loss: 0.007709 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "Training loss: 0.007691 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "Training loss: 0.007673 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "Training loss: 0.007655 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "Training loss: 0.007637 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "Training loss: 0.007619 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "Training loss: 0.007602 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "Training loss: 0.007584 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "Training loss: 0.007566 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "Training loss: 0.007548 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "Training loss: 0.007531 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "Training loss: 0.007513 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "Training loss: 0.007496 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "Training loss: 0.007478 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Training loss: 0.007461 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Training loss: 0.007444 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "Training loss: 0.007426 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "Training loss: 0.007409 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "Training loss: 0.007392 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "Training loss: 0.007375 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "Training loss: 0.007358 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "Training loss: 0.007341 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "Training loss: 0.007324 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "Training loss: 0.007307 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "Training loss: 0.007290 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "Training loss: 0.007274 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "Training loss: 0.007257 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "Training loss: 0.007240 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "Training loss: 0.007224 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "Training loss: 0.007207 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "Training loss: 0.007191 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "Training loss: 0.007174 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "Training loss: 0.007158 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "Training loss: 0.007142 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "Training loss: 0.007126 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "Training loss: 0.007109 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "Training loss: 0.007093 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "Training loss: 0.007077 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "Training loss: 0.007061 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "Training loss: 0.007045 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "Training loss: 0.007029 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "Training loss: 0.007013 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "Training loss: 0.006998 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "Training loss: 0.006982 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "Training loss: 0.006966 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "Training loss: 0.006950 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "Training loss: 0.006935 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "Training loss: 0.006919 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "Training loss: 0.006904 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "Training loss: 0.006888 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "Training loss: 0.006873 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "Training loss: 0.006858 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "Training loss: 0.006842 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "Training loss: 0.006827 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "Training loss: 0.006812 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "Training loss: 0.006797 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "Training loss: 0.006782 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "Training loss: 0.006767 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "Training loss: 0.006752 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "Training loss: 0.006737 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "Training loss: 0.006722 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "Training loss: 0.006707 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "Training loss: 0.006692 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "Training loss: 0.006678 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Training loss: 0.006663 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "Training loss: 0.006648 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "Training loss: 0.006634 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "Training loss: 0.006619 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "Training loss: 0.006605 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "Training loss: 0.006590 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "Training loss: 0.006576 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "Training loss: 0.006562 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "Training loss: 0.006547 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "Training loss: 0.006533 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "Training loss: 0.006519 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "Training loss: 0.006505 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "Training loss: 0.006491 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "Training loss: 0.006477 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "Training loss: 0.006463 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "Training loss: 0.006449 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "Training loss: 0.006435 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "Training loss: 0.006421 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "Training loss: 0.006408 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "Training loss: 0.006394 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "Training loss: 0.006380 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "Training loss: 0.006366 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "Training loss: 0.006353 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "Training loss: 0.006339 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "Training loss: 0.006326 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "Training loss: 0.006312 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "Training loss: 0.006299 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "Training loss: 0.006286 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "Training loss: 0.006272 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "Training loss: 0.006259 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "Training loss: 0.006246 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "Training loss: 0.006233 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "Training loss: 0.006219 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "Training loss: 0.006206 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "Training loss: 0.006193 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "Training loss: 0.006180 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "Training loss: 0.006167 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "Training loss: 0.006154 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "Training loss: 0.006141 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "Training loss: 0.006129 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "Training loss: 0.006116 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "Training loss: 0.006103 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "Training loss: 0.006090 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "Training loss: 0.006078 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "Training loss: 0.006065 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "Training loss: 0.006052 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "Training loss: 0.006040 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "Training loss: 0.006027 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "Training loss: 0.006015 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "Training loss: 0.006002 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Training loss: 0.005990 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Training loss: 0.005978 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "Training loss: 0.005965 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "Training loss: 0.005953 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "Training loss: 0.005941 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "Training loss: 0.005929 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "Training loss: 0.005917 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "Training loss: 0.005905 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "Training loss: 0.005892 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "Training loss: 0.005880 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "Training loss: 0.005868 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "Training loss: 0.005857 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "Training loss: 0.005845 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "Training loss: 0.005833 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "Training loss: 0.005821 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "Training loss: 0.005809 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "Training loss: 0.005797 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "Training loss: 0.005786 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "Training loss: 0.005774 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "Training loss: 0.005762 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "Training loss: 0.005751 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "Training loss: 0.005739 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "Training loss: 0.005728 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "Training loss: 0.005716 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "Training loss: 0.005705 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "Training loss: 0.005694 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "Training loss: 0.005682 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "Training loss: 0.005671 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "Training loss: 0.005660 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "Training loss: 0.005648 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "Training loss: 0.005637 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "Training loss: 0.005626 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "Training loss: 0.005615 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "Training loss: 0.005604 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "Training loss: 0.005593 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "Training loss: 0.005582 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "Training loss: 0.005571 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "Training loss: 0.005560 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "Training loss: 0.005549 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "Training loss: 0.005538 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "Training loss: 0.005527 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "Training loss: 0.005516 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "Training loss: 0.005505 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "Training loss: 0.005495 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "Training loss: 0.005484 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "Training loss: 0.005473 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "Training loss: 0.005463 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "Training loss: 0.005452 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "Training loss: 0.005441 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "Training loss: 0.005431 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Training loss: 0.005420 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "Training loss: 0.005410 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "Training loss: 0.005400 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "Training loss: 0.005389 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "Training loss: 0.005379 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "Training loss: 0.005368 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "Training loss: 0.005358 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "Training loss: 0.005348 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "Training loss: 0.005338 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "Training loss: 0.005327 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "Training loss: 0.005317 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "Training loss: 0.005307 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "Training loss: 0.005297 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "Training loss: 0.005287 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "Training loss: 0.005277 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "Training loss: 0.005267 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "Training loss: 0.005257 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "Training loss: 0.005247 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "Training loss: 0.005237 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "Training loss: 0.005227 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "Training loss: 0.005217 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "Training loss: 0.005207 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "Training loss: 0.005198 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "Training loss: 0.005188 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "Training loss: 0.005178 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "Training loss: 0.005168 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "Training loss: 0.005159 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "Training loss: 0.005149 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "Training loss: 0.005140 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "Training loss: 0.005130 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "Training loss: 0.005120 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "Training loss: 0.005111 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "Training loss: 0.005101 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "Training loss: 0.005092 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "Training loss: 0.005083 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "Training loss: 0.005073 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "Training loss: 0.005064 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "Training loss: 0.005054 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "Training loss: 0.005045 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "Training loss: 0.005036 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "Training loss: 0.005027 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "Training loss: 0.005017 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "Training loss: 0.005008 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "Training loss: 0.004999 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "Training loss: 0.004990 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "Training loss: 0.004981 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "Training loss: 0.004972 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "Training loss: 0.004963 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "Training loss: 0.004954 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "Training loss: 0.004945 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Training loss: 0.004936 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Training loss: 0.004927 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "Training loss: 0.004918 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "Training loss: 0.004909 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "Training loss: 0.004900 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "Training loss: 0.004891 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "Training loss: 0.004882 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "Training loss: 0.004874 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "Training loss: 0.004865 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "Training loss: 0.004856 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "Training loss: 0.004847 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "Training loss: 0.004839 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "Training loss: 0.004830 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "Training loss: 0.004821 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "Training loss: 0.004813 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "Training loss: 0.004804 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "Training loss: 0.004796 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "Training loss: 0.004787 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "Training loss: 0.004779 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "Training loss: 0.004770 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "Training loss: 0.004762 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "Training loss: 0.004753 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "Training loss: 0.004745 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "Training loss: 0.004737 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "Training loss: 0.004728 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "Training loss: 0.004720 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "Training loss: 0.004712 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "Training loss: 0.004703 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "Training loss: 0.004695 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "Training loss: 0.004687 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "Training loss: 0.004679 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "Training loss: 0.004671 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "Training loss: 0.004663 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "Training loss: 0.004654 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "Training loss: 0.004646 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "Training loss: 0.004638 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "Training loss: 0.004630 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "Training loss: 0.004622 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "Training loss: 0.004614 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "Training loss: 0.004606 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "Training loss: 0.004598 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "Training loss: 0.004590 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "Training loss: 0.004582 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "Training loss: 0.004575 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "Training loss: 0.004567 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "Training loss: 0.004559 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "Training loss: 0.004551 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "Training loss: 0.004543 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "Training loss: 0.004536 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "Training loss: 0.004528 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Training loss: 0.004520 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "Training loss: 0.004512 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "Training loss: 0.004505 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "Training loss: 0.004497 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "Training loss: 0.004489 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "Training loss: 0.004482 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "Training loss: 0.004474 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "Training loss: 0.004467 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "Training loss: 0.004459 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "Training loss: 0.004452 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "Training loss: 0.004444 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "Training loss: 0.004437 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "Training loss: 0.004429 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "Training loss: 0.004422 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "Training loss: 0.004414 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "Training loss: 0.004407 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "Training loss: 0.004399 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "Training loss: 0.004392 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "Training loss: 0.004385 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "Training loss: 0.004378 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "Training loss: 0.004370 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "Training loss: 0.004363 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6200\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "Training loss: 0.004356 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "Training loss: 0.004348 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "Training loss: 0.004341 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "Training loss: 0.004334 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "Training loss: 0.004327 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "Training loss: 0.004320 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "Training loss: 0.004313 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "Training loss: 0.004306 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "Training loss: 0.004298 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "Training loss: 0.004291 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "Training loss: 0.004284 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "Training loss: 0.004277 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "Training loss: 0.004270 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "Training loss: 0.004263 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "Training loss: 0.004256 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "Training loss: 0.004249 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "Training loss: 0.004242 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "Training loss: 0.004236 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "Training loss: 0.004229 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "Training loss: 0.004222 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "Training loss: 0.004215 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "Training loss: 0.004208 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "Training loss: 0.004201 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "Training loss: 0.004194 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "Training loss: 0.004188 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "Training loss: 0.004181 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "Training loss: 0.004174 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "Training loss: 0.004167 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Training loss: 0.004161 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Training loss: 0.004154 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "Training loss: 0.004147 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "Training loss: 0.004141 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "Training loss: 0.004134 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "Training loss: 0.004127 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "Training loss: 0.004121 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "Training loss: 0.004114 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "Training loss: 0.004108 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "Training loss: 0.004101 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "Training loss: 0.004095 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "Training loss: 0.004088 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "Training loss: 0.004082 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "Training loss: 0.004075 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "Training loss: 0.004069 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "Training loss: 0.004062 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "Training loss: 0.004056 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "Training loss: 0.004049 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "Training loss: 0.004043 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "Training loss: 0.004037 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "Training loss: 0.004030 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "Training loss: 0.004024 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "Training loss: 0.004018 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "Training loss: 0.004011 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "Training loss: 0.004005 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "Training loss: 0.003999 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "Training loss: 0.003992 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "Training loss: 0.003986 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "Training loss: 0.003980 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "Training loss: 0.003974 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "Training loss: 0.003968 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "Training loss: 0.003961 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "Training loss: 0.003955 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "Training loss: 0.003949 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "Training loss: 0.003943 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "Training loss: 0.003937 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "Training loss: 0.003931 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "Training loss: 0.003925 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "Training loss: 0.003919 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "Training loss: 0.003913 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "Training loss: 0.003907 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "Training loss: 0.003901 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "Training loss: 0.003895 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "Training loss: 0.003889 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "Training loss: 0.003883 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "Training loss: 0.003877 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "Training loss: 0.003871 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "Training loss: 0.003865 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "Training loss: 0.003859 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "Training loss: 0.003853 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Training loss: 0.003847 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "Training loss: 0.003841 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "Training loss: 0.003835 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "Training loss: 0.003830 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "Training loss: 0.003824 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "Training loss: 0.003818 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "Training loss: 0.003812 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "Training loss: 0.003806 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "Training loss: 0.003801 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "Training loss: 0.003795 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "Training loss: 0.003789 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "Training loss: 0.003783 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "Training loss: 0.003778 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "Training loss: 0.003772 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "Training loss: 0.003766 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "Training loss: 0.003761 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "Training loss: 0.003755 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "Training loss: 0.003749 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "Training loss: 0.003744 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "Training loss: 0.003738 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "Training loss: 0.003732 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "Training loss: 0.003727 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "Training loss: 0.003721 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "Training loss: 0.003716 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "Training loss: 0.003710 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "Training loss: 0.003705 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "Training loss: 0.003699 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "Training loss: 0.003694 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "Training loss: 0.003688 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "Training loss: 0.003683 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "Training loss: 0.003677 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "Training loss: 0.003672 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "Training loss: 0.003666 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "Training loss: 0.003661 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "Training loss: 0.003656 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "Training loss: 0.003650 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "Training loss: 0.003645 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "Training loss: 0.003639 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "Training loss: 0.003634 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "Training loss: 0.003629 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "Training loss: 0.003623 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "Training loss: 0.003618 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "Training loss: 0.003613 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "Training loss: 0.003608 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "Training loss: 0.003602 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "Training loss: 0.003597 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "Training loss: 0.003592 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "Training loss: 0.003586 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "Training loss: 0.003581 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "Training loss: 0.003576 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Training loss: 0.003571 \n",
      " Training accuracy: 1.000000\n",
      " Validation accuracy: 0.6220\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# We are taking inspiration from the PyG tutorial: https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "gcn = GCN(in_features=in_features, n_hidden=32, n_output=ntargets).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01)  #, weight_decay=5e-4) (?)\n",
    "criterion = F.nll_loss  # I'm already computing the `log_softmax` in the forward\n",
    "\n",
    "# Store data\n",
    "epochs = 100\n",
    "loss_train = torch.zeros(epochs)\n",
    "loss_val = torch.zeros_like(loss_train)\n",
    "acc_train = torch.zeros_like(loss_train)\n",
    "acc_val = torch.zeros_like(loss_train)\n",
    "\n",
    "# Move data to gpu\n",
    "X, t = data.x.to(device), data.y.to(device)\n",
    "adj = adj_mat_norm_torch.to(device)\n",
    "\n",
    "size_train = int(data.train_mask.sum())\n",
    "size_val = int(data.val_mask.sum())\n",
    "\n",
    "for e in range(epochs):\n",
    "    gcn.train()\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ### Training set\n",
    "    # Forward\n",
    "    out = gcn(X, adj)[data.train_mask]\n",
    "    target = t[data.train_mask]\n",
    "\n",
    "    # Backpropagate\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Metrics\n",
    "    loss_train[e] = loss.item() # / size_train\n",
    "    ncorrect = (out.argmax(1) == target).type(torch.float).sum().item()\n",
    "    acc_train[e] = ncorrect / size_train\n",
    "\n",
    "    ### Validation set\n",
    "    gcn.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward\n",
    "        out_val = gcn(X, adj)[data.val_mask]\n",
    "        target_val = t[data.val_mask]\n",
    "\n",
    "        # Metrics\n",
    "        loss_val[e] = criterion(out_val, target_val).item()#  / size_val\n",
    "        ncorrect = (out_val.argmax(1) == target_val).type(torch.float).sum().item()\n",
    "        acc_val[e] = ncorrect / size_val\n",
    "\n",
    "    print(f\"Training loss: {loss_train[e]:>5f} \\n Training accuracy: {acc_train[e]:>5f}\\n Validation accuracy: {acc_val[e]:.4f}\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6900\n"
     ]
    }
   ],
   "source": [
    "### Training set\n",
    "with torch.no_grad():\n",
    "    test_out = gcn(X, adj)[data.test_mask]\n",
    "    test_target = t[data.test_mask]\n",
    "    test_accuracy = (test_out.argmax(1) == test_target).type(torch.float).mean().item()\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax[0].plot(range(epochs), loss_train, label=\"Training loss\")\n",
    "ax[0].plot(range(epochs), loss_val, label=\"Validation loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(range(epochs), acc_train, label=\"Acc. train\")\n",
    "ax[1].plot(range(epochs), acc_val, label=\"Acc. val\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb5YjHVClCqo"
   },
   "source": [
    "#### Question 10 (0.5 pt)\n",
    "\n",
    "The paper introduces GCNs as a way to solve a *semi-supervised* classification problem.\n",
    "\n",
    "- What makes this problem semi-supervised?\n",
    "- What is the proportion of labeled data used for training with respect to labeled data in the validation and test sets? What is difference in this context with other benchmark tasks in machine learning, like image classification with MNIST?\n",
    "- Why do you think the GCN performs well in this semi-supervised scenario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qw58r1MmCUJ"
   },
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ihrjZddvz5d"
   },
   "source": [
    "### Loading a dataset of proteins\n",
    "\n",
    "In the previous sections you learned how to pass the adjacency matrix of a graph with a couple of thousand of nodes, to classify each node with a particular label. A different and useful application of GCNs is graph classification.\n",
    "\n",
    "In contrast with the previous part, where there was a single, big graph, in graph classification we have multiple graphs, and each graph can be assigned a label. In this part of the assignment you will implement a classifier for proteins.\n",
    "\n",
    "[Proteins](https://en.wikipedia.org/wiki/Protein_(nutrient)) are parts of the buildings block of life. They consist of chains of amino acids, and can take many shapes. In the PROTEINS dataset, proteins are represented as graphs, where the nodes are amino acids, and an edge between them indicates that they are 6 [Angstroms](https://en.wikipedia.org/wiki/Angstrom) apart. All graphs have a binary label, where 1 means that the protein is not an enzyme.\n",
    "\n",
    "We will start by loading and examining this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmqweMcvnUH6",
    "outputId": "f10d56ce-4b1b-4925-bc61-58c49b22adb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "dataset = TUDataset(root='data/TU', name='PROTEINS', use_node_attr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oF1gyKPXiz-"
   },
   "source": [
    "#### Question 11 (0.25 pt)\n",
    "\n",
    "Unlike in the previous part, where we selected the first element returned by the loading function, note that here we get all the elements returned by `TUDataset()`. `dataset` is an interable object, that has some similar behaviors as a Python list: you can call `len()` on it, and you can takes slices from it.\n",
    "\n",
    "Each element in `dataset` is a `Data` object containing a graph that represents a protein. This is the same type of object that we used in the previous part to store the Cora citation network.\n",
    "\n",
    "Knowing this, answer the following:\n",
    "\n",
    "- How many proteins (graphs) are there in `dataset`?\n",
    "- Take any protein from `dataset`. How many nodes and edges does it contain? What is its label? How many features does each node have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZNPsnXXbbHHe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113\n",
      "Data(edge_index=[2, 162], x=[42, 4], y=[1])\n"
     ]
    }
   ],
   "source": [
    "# Your answer here\n",
    "print(len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHSklBZXpKpR"
   },
   "source": [
    "#### Question 12 (0.5 pt)\n",
    "\n",
    "To properly train and evaluate our model, we need training, validation, and test splits.\n",
    "\n",
    "For reproducibility purposes, we generate a random tensor of indices for you. Use it to extract the three splits from `dataset`.\n",
    "\n",
    "For training, take 80% of the indices (starting from the first element in `indices`), then the following 10% for validation, and the remaining 10% for testing. You can use the indices to index `dataset`.\n",
    "\n",
    "Call the resulting splits `train_dataset`, `valid_dataset`, and `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttY4d1GInn08"
   },
   "outputs": [],
   "source": [
    "# Don't erase the following three lines\n",
    "import torch\n",
    "torch.random.manual_seed(0)\n",
    "indices = torch.randperm(len(dataset))\n",
    "\n",
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDJbB4CQqsfp"
   },
   "source": [
    "### Working with a batch of graphs\n",
    "\n",
    "When working with the Cora dataset, you used the information in `data.edge_index` to build the sparse normalized adjacency matrix $\\hat{A}$ that is required by the GCN. We could do something similar here: for each graph, we build $\\hat{A}$, and pass it to the GCN. However, if the number of graphs is big, this can really slow down training.\n",
    "\n",
    "To avoid this, we will resort to a very useful trick that also allows us to reuse the same GCN you implemented previously. The trick makes it possible to do a forward pass through the GCN for multiple, disconnected graphs at the same time (instead of only one), much like when you train with mini-batches for other kinds of data.\n",
    "\n",
    "Let's first revisit the propagation rule of the GCN, $Z = \\hat{A}XW$, with an illustration (we have omitted the cells of $X$ and $W$ for clarity):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-forward.png\">\n",
    "\n",
    "If we have multiple graphs, we can still use the same propagation rule, if we\n",
    "\n",
    "- Set $\\hat{A}$ to be a block diagonal matrix, where the blocks are the different adjacency matrices of the graphs\n",
    "- Concatenate the feature matrices along the first dimension\n",
    "\n",
    "This is illustrated in the following figure, for a batch of 3 graphs. Note that the elements outside of the blocks are zero.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-batch-forward.png\">\n",
    "\n",
    "The resulting adjacency matrix $\\hat{A}_B$ can also be built as a sparse matrix, and once we have it together with the concatenated matrix of features, the computation of the graph convolution is exactly the same as before. Note how this trick also allows us to process graphs with different sizes and structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DLPJ62b2mQ6"
   },
   "source": [
    "#### Question 13 (0.5 pt)\n",
    "\n",
    "\n",
    "Just as the citation network, the graphs in each of the datasets you created in Question 12 also have an `edge_index` attribute, which can be used to compute the normalized adjacency matrix $\\hat{A}$, for each graph.\n",
    "\n",
    "Reusing your code for Questions 3 and 5, define a function `get_a_norm()` that takes as input an element of a dataset (e.g. `train_dataset[0]`), and returns a `scipy.sparse` matrix containing $\\hat{A}$.\n",
    "\n",
    "Note that an element of a dataset has properties like `num_edges`, `num_nodes`, etc. which you can use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nvPX2GB8oXp"
   },
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBrmYBY3AfhW"
   },
   "source": [
    "#### Question 14 (1 pt)\n",
    "\n",
    "To prepare the batch of graphs, we need to collect multiple adjacency matrices, feature matrices, and labels.\n",
    "\n",
    "When using the trick described in the last figure, we see that we have to keep track of when a graph starts and when it ends, so that we can later differentiate the outputs due to $X^{(0)}$, $X^{(1)}$, etc. To achieve this, we will additionally collect a 1D array of batch indices, one for each $X^{(i)}$.\n",
    "\n",
    "The 1D array has as many elements as rows in $X^{(i)}$, and it is filled with the value $i$ (the position of $X^{(i)}$ in the batch):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/03-batch-indices.png\">\n",
    "\n",
    "We will later concatenate all the 1D arrays along the first dimension, just as we will do with all the $X^{(i)}$.\n",
    "\n",
    "Define a function `prepare_graphs_batch()` that takes as input a dataset (e.g. `train_dataset`), and does the following\n",
    "\n",
    "- Create four empty lists:\n",
    "  - `adj_matrices`\n",
    "  - `feature_matrices`\n",
    "  - `batch_indices`\n",
    "  - `labels`\n",
    "- Iterate over the input dataset, getting one graph at a time. At each step, use your function from Question 13 to append the adjacency matrix to `adj_matrices`, append the matrix of input features to `feature_matrices`, create the array of batch indices (as explained above) and append it to `batch_indices`, and append the label of the graph to `labels`. **Make sure to convert the label to float**.\n",
    "- Once the loop is over, use `scipy.sparse.block_diag()` to build the block diagonal matrix $\\hat{A}_B$. Convert it to the COO format, and then use your answer to Question 6 to turn it into a sparse PyTorch tensor.\n",
    "- Use `torch.cat()` to concatenate the tensors in `feature_matrices` along the first dimension. Do this also for `batch_indices` and `labels`.\n",
    "- Return the 4 tensors computed in the previous two items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsQ0-JjSqFgD"
   },
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i73P_EU0MSPX"
   },
   "source": [
    "Once your answer for the previous question is ready, you can run the next cell to prepare all the required information, for the train, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iol5FxJGMmAU"
   },
   "outputs": [],
   "source": [
    "train_a_norm, train_features, train_batch_idx, train_labels = prepare_graphs_batch(train_dataset)\n",
    "valid_a_norm, valid_features, valid_batch_idx, valid_labels = prepare_graphs_batch(valid_dataset)\n",
    "test_a_norm, test_features, test_batch_idx, test_labels = prepare_graphs_batch(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6q-JU87NClh"
   },
   "source": [
    "### GCNs for graph classification\n",
    "\n",
    "We now have all the ingredients to pass a batch of graphs to a GCN. However, for each graph in the batch, the output $Z^{(i)}$ contains one row for each node in the graph. If the goal is to do classification at the graph level, we have to *pool* these vectors to then compute the required logits for classification.\n",
    "\n",
    "This operation is similar as how pooling works in a CNN. We could consider taking the mean of the vectors, the sum, or use max-pooling. The difference with respect to CNNs is that in our case, we have a batch of graphs, each potentially with a different number of nodes.\n",
    "\n",
    "To implement this specific pooling, we can use the scatter operation in the `torch_scatter` library, which comes when installing PyG. We will use it, together with the tensor of batch indices from the previous two questions, to pool the outputs of the GCN for each graph, into a single vector:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/04-scatter.png\">\n",
    "\n",
    "You can check more details in the [documentation](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY87DX1uRhnY"
   },
   "source": [
    "#### Question 15 (1.0 pt)\n",
    "\n",
    "Implement a `GraphClassifier` module using PyTorch.\n",
    "\n",
    "- The constructor should take as arguments the number of input features, the hidden dimension, and the number of classes.\n",
    "- The model should contain a instance of the `GCN` module (as you implemented it in Question 8). Use the same value for the hidden dimension and the number of output features (recall that your `GCN` module from Question 8 has two GCN layers).\n",
    "- The model should also contain a `torch.nn.Linear` layer, with the hidden dimension as the input features, and the number of classes as the output.\n",
    "- The forward method receives the concatenated matrix of features, the sparse block diagonal adjacency matrix, and the batch indices (the latter is used when calling `scatter`).\n",
    "- Use the following architecture in the forward pass:\n",
    "  - GCN $\\to$ ReLU $\\to$ scatter (max) $\\to$ Linear.\n",
    "\n",
    "The output of the forward should be a 1D tensor (you might need to call `squeeze` to get rid of extra dimensions) containing the logits for all graphs in the batch, for the binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "750WraywwYDH"
   },
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0QHnn6dV87J"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1PHy-_vTjgh"
   },
   "source": [
    "#### Question 16 (1.5 pt)\n",
    "\n",
    "Implement a training loop for the graph classifier. Use the data from Question 14 to train and evaluate the model.\n",
    "\n",
    "We encourage you to use a GPU in this section for faster training. Note that if you change the runtime at this point, you must re-execute several of the cells above, including the ones that install PyG.\n",
    "\n",
    "- Instantiate a classifier with 32 as the hidden dimension\n",
    "- Use Adam with a learning rate of 1e-3.\n",
    "- Use `torch.nn.BCEWithLogitsLoss` as the loss function.\n",
    "- Train for 5,000 epochs. Once training is done, plot the loss curve and the accuracy in the validation set. Then report the accuracy in the test set.\n",
    "\n",
    "**Note:** the logits from the output of the classifier come from a linear layer. To compute actual predictions for the calculation of the accuracy, pass the logits through `torch.sigmoid()`, and set the predicted values to 1 whenever they are greater than 0.5, and to 0 otherwise.\n",
    "\n",
    "You should get an accuracy equal to or higher than 70% in the validation and test sets. Can you beat the [state-of-the-art](https://paperswithcode.com/sota/graph-classification-on-proteins)? Feel free to modify your architecture and experiment with it.\n",
    "\n",
    "Discuss what you observe during training and your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DbGAs8W2Xja"
   },
   "outputs": [],
   "source": [
    "# If your runtime is GPU-enabled, use .to(device) to move the model\n",
    "# and all the relevant tensors to the GPU. You have to move tensors back to CPU\n",
    "# when computing metrics like accuracy, using .cpu().\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvmAfDNMcnKD"
   },
   "source": [
    "## The end\n",
    "\n",
    "If you have made it all the way here successfully, congratulations!  \n",
    "\n",
    "You have implemented your own GCN and tested it on a node classification task, and a more challenging classification task over multiple graphs.\n",
    "\n",
    "We hope you can use this knowledge to apply GCNs not only to the tasks described here, but other applications where data can be modeled as a graph.\n",
    "\n",
    "If you are interested in applying graph neural networks to larger graphs, or try newer architectures, you can dive deeper into [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/), a library with fast implementations for a wide range of architectures. It also comes with custom code that takes care of aspects that you dealt with manually for this assignment, like a more efficient implementation of the adjacency matrix multiplication via message-passing methods, and Data Loaders that relieve you from having to build block diagonal sparse matrices.\n",
    "\n",
    "You can also check the [Deep Graph Library](https://docs.dgl.ai/) another powerful library for deep learning on graphs which also integrates with other backends like TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7QhyAMms8-L"
   },
   "source": [
    "# Grading (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juIdxXhos-mV"
   },
   "source": [
    "- Question 1: 0.25pt \n",
    "- Question 2: 0.25pt \n",
    "- Question 3: 0.5pt \n",
    "- Question 4: 0.25pt \n",
    "- Question 5: 0.5pt \n",
    "- Question 6: 0.5pt \n",
    "- Question 7: 0.5pt \n",
    "- Question 8: 0.5pt \n",
    "- Question 9: 1.5pt \n",
    "- Question 10: 0.5pt \n",
    "- Question 11: 0.25pt \n",
    "- Question 12: 0.5pt \n",
    "- Question 13: 0.5pt \n",
    "- Question 14: 1pt\n",
    "- Question 15: 1pt\n",
    "- Question 16: 1.5pt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "fuckyou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
