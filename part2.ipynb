{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 and 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the mnist data with the data loader and put in the following tensors: (x_train_data, y_train_data), (x_test_data, y_test_data)\n",
    "\n",
    "def load_mnist_data():\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=len(train_data))\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "\n",
    "    x_train_data, y_train_data = next(iter(train_loader))\n",
    "    x_test_data, y_test_data = next(iter(test_loader))\n",
    "\n",
    "    return (x_train_data, y_train_data), (x_test_data, y_test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_full, y_train_full), (x_test, y_test) = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data into 50 000 training instances and 10 000 validation instances\n",
    "\n",
    "def split_data(x_train_data, y_train_data):\n",
    "\n",
    "    x_train_data, x_val_data = x_train_data[:50000], x_train_data[50000:]\n",
    "    y_train_data, y_val_data = y_train_data[:50000], y_train_data[50000:]\n",
    "\n",
    "    return (x_train_data, y_train_data), (x_val_data, y_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val) = split_data(x_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the network and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss_and_accuracy(model, x_data, y_data, criterion, batch):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(x_data), batch):\n",
    "            to = min(i + batch, len(x_data))\n",
    "            x_batch = x_data[i:to]\n",
    "            y_batch = y_data[i:to]\n",
    "            output = model(x_batch)\n",
    "            loss += criterion(output, y_batch).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y_batch.view_as(pred)).sum().item()\n",
    "    return loss / len(x_data), correct / len(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model and training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the neural network\n",
    "class MNISTConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)  # 1 input channel, 16 output channels\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # 16 input channels, 32 output channels\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 32 input channels, 64 output channels\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 10)  # Flattened to a fully connected layer with 10 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # First convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = F.relu(self.conv2(x))  # Second convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = F.relu(self.conv3(x))  # Third convolution + ReLU\n",
    "        x = F.max_pool2d(x, 2)     # Max pooling 2x2\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)            # Fully connected layer\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Training loop that computes the running loss per epoch and validation loss and accuracy per epoch\n",
    "def train(model, x_train, y_train, x_val, y_val, optimizer, criterion, epochs=10, batch_size=64):\n",
    "\n",
    "    first_epoch_running_loss = []\n",
    "\n",
    "    train_evaluations = np.zeros((epochs, 2))\n",
    "    val_evaluations = np.zeros((epochs, 2))\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i in tqdm(range(0, len(x_train), batch_size), desc=f'Batches for epoch {epoch + 1}/{epochs}'):\n",
    "\n",
    "            to = min(i + batch_size, len(x_train))\n",
    "            x_batch = x_train[i:to]\n",
    "            y_batch = y_train[i:to]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch == 0:\n",
    "                # get average loss and append to list\n",
    "                first_epoch_running_loss.append(loss.item())\n",
    "\n",
    "        \n",
    "        train_loss, train_acc = calculate_loss_and_accuracy(model, x_train, y_train, criterion, batch_size)\n",
    "        val_loss, val_acc = calculate_loss_and_accuracy(model, x_val, y_val, criterion, batch_size)\n",
    "\n",
    "        train_evaluations[epoch] = [train_loss, train_acc]\n",
    "        val_evaluations[epoch] = [val_loss, val_acc]\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return first_epoch_running_loss, train_evaluations, val_evaluations\n",
    "\n",
    "\n",
    "# Training on CPU (use \"cuda\" for GPU training if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# put the data on the device\n",
    "\n",
    "x_train, y_train, x_val, y_val = x_train.to(device), y_train.to(device), x_val.to(device), y_val.to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 782/782 [00:17<00:00, 44.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 0.0012, Train Acc: 0.9751, Val Loss: 0.0012, Val Acc: 0.9791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 782/782 [00:16<00:00, 46.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 0.0008, Train Acc: 0.9841, Val Loss: 0.0008, Val Acc: 0.9849\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model = MNISTConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "first_epoch_running_loss, train_evaluations, val_evaluations = train(model, x_train, y_train, x_val, y_val, optimizer, criterion, epochs=3, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO here we plot the results stored in *first_epoch_running_loss, train_evaluations, val_evaluations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
